{"status":"ok","feed":{"url":"https://medium.com/feed/@sahilchachra","title":"Stories by Sahil Chachra on Medium","link":"https://medium.com/@sahilchachra?source=rss-f31bf6073414------2","author":"","description":"Stories by Sahil Chachra on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/1*qeoZztJDdpLVrCSEOBy9zg.jpeg"},"items":[{"title":"Paper Summary\u200a\u2014\u200aMetaFormer is Actually What You Need for Vision","pubDate":"2022-01-07 15:34:38","link":"https://sahilchachra.medium.com/paper-summary-metaformer-is-actually-what-you-need-for-vision-b6f172482604?source=rss-f31bf6073414------2","guid":"https://medium.com/p/b6f172482604","author":"Sahil Chachra","thumbnail":"https://cdn-images-1.medium.com/max/702/1*p13M1xSf2W36K4jSm6TADA.png","description":"\n<h3>Paper Summary\u200a\u2014\u200aMetaFormer is Actually What You Need for\u00a0Vision</h3>\n<p>In recent times we have seen that Transformers (for vision) have performed very well, i.e., at par or at times surpassing the previously claimed SOTA for vision. The heart of the Transformer is the attention based token mixer which was applauded for the results the transformers gave in other fields as\u00a0well.</p>\n<p>Now, what if I tell you, you replace that entire Attention based token mixer (which takes in key, value and query and does wonders), and replace that with just a Pooling layer and the result you will get will be at par with many SOTA architectures!</p>\n<p>Can you believe? Just simple Pooling operations, (MaxPool, AvgPool, etc in vision) which has no parameters, perform at par with Attention Mechanism!? Even I did not believe until I read the paper\u200a\u2014<strong><em>\u200aMetaFormer is Actually What You Need for Vision\u00a0(</em></strong><a href=\"https://arxiv.org/abs/2111.11418\"><strong><em>link</em></strong></a><strong><em>).</em></strong></p>\n<p><strong>Note\u200a\u2014\u200aIf I quote anything from the paper it\u2019ll be italic and inside quotes. For example\u200a\u2014\u200a\u201c</strong><em>this is an\u00a0example\u201d</em></p>\n<h3>Abstract</h3>\n<p>It was believed that the attention module of the transformer contributed most to it\u2019s amazing performance but later when it was replaced by spatial MLPs, it still gave good results. So, as per the authors, it means that, apart from the attention module, the general architecture of the transformer contributes to its performance.<br>To verify this, the authors replaced the attention module with a non-parametric pooling layer (they called the model as PoolFormer) and the results were still good! This result encourages the authors to move ahead and propose the concept of \u201cMetaFormer\u201d where the general architecture of the transformer is used without any specific token\u00a0mixer.</p>\n<h3>Introduction</h3>\n<p>The attention module for mixing information among tokens in the transformer\u2019s encoder is termed as \u201ctoken mixer\u201d. Believing the attention mechanism to be the reason behind transformer\u2019s performance, many works have been proposed surrounding this\u00a0concept.</p>\n<p>But some of the recent works, replaced the attention module with a Fourier Transform and still performed very well, attaining 97% of the accuracy of Vanilla transformer. So this shows that, if we use MetaFormer (the general architecture of the transformer having any computation in the place of attention module) as the general architecture, good results are attained.</p>\n<p>PoolFormer, proposed by the authors, outperforms fine-tuned transformers and MLP like\u00a0models.</p>\n<p>Contributions of the paper are\u00a0:-</p>\n<ol>\n<li>\u201c<em>Abstract transformers into a general architecture MetaFormer\u201d</em>\n</li>\n<li>Authors evaluate PoolFormer in multiple tasks such as Classification, object detection, semantic segmentation and instance segmentation and it \u201c<em>achieves competitive performance compared with the SOTA models using sophistic design of token\u00a0mixers.\u201d</em>\n</li>\n</ol>\n<h3>Method</h3>\n<h4>MetaFormer</h4>\n<p><em>\u201cInput I is first processed by input embedding such as patch embedding in\u00a0VITs.\u201d</em></p>\n<p><em>X = </em>InputEmb(<em>I</em>)</p>\n<p>Where X is a matrix of real number of dimension NxC, N is for squence length and C is embedding dimension. \u201c<em>Then embedding tokens are fed to repeated MetaFormer blocks, each of which includes two residual sub-blocks. The first sub-block mainly contains a token mixer to communicate information among tokens\u201d and its represented as</em></p>\n<p><em>Y = </em>TokenMixer(Norm(X)) + X\u00a0, where Norm is Layer Normalization or Batch Normalization and TokenMixer is a module for mixing information like attention, Spatial MLP,\u00a0etc</p>\n<p><em>\u201cThe second sub-block consists of two-layered MLP with non-linear activation\u201d</em></p>\n<p>Z = NonLinearActivationFunction(Normalization(Y) * W1) * W2 +\u00a0Y</p>\n<p>where W1 is matrix of real numbers of dimension C x rC and W2 is matrix of real numbers of dimension rC x C. W1 &amp; W2 both are learnable parameters with MLP expansion ratio\u00a0r.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/702/1*p13M1xSf2W36K4jSm6TADA.png\"><figcaption>Figure 1\u200a\u2014\u200aSource\u00a0: From paper\u00a0itself</figcaption></figure><h4>PoolFormer</h4>\n<p>The authors use Pooling Operation as Token Mixer in the general architecture proposed above that is MetaFormer. This operation has no parameters which can be learnt and \u201c<em>it just makes each token averagely aggregate its nearby token features\u201d. </em>Spatial MLP and Self attention have quadratic computational complexity where as pooling has linear computational complexity.</p>\n<p>\u201c<em>PoolFormer has 4 stages H/4 * W/4, H/8 * W/8, H/16 * W/16 and H/32 * W/32. There are two groups having different embedding size\u00a0:-</em></p>\n<ol>\n<li><em>small-sized models with embedding dimensions of 64, 128, 320, and 512 responding to the four\u00a0stages;</em></li>\n<li><em>2) medium-sized models with embedding dimensions 96, 192, 384, and\u00a0768.</em></li>\n</ol>\n<p><em>Assuming there are L PoolFormer blocks in total, stages 1, 2, 3, and 4 will contain L/6, L/6, L/2, and L/6 PoolFormer blocks respectively. The MLP expansion ratio is set as 4. According to the above simple model scaling rule, we obtain 5 different model sizes of PoolFormer and their hyperparameters are shown</em>\u201d\u00a0below.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/754/1*ZxnVKXf19LArPOeZ_rhxXQ.png\"><figcaption>Figure 2\u200a\u2014\u200aSource\u00a0: paper\u00a0iteself</figcaption></figure><h3>Results</h3>\n<p>For ImageNet, PoolFormer-S24 has top-1 accuracy greater than 80% with only 21M parameters and 2.6G MACs whereas DeiT-s achieves 79.8 but requires 4.6G MACs. ResMLP-S24 needs 30M parameters andd 6G MACs with accuracy of\u00a079.4.</p>\n<h3>Conclusion</h3>\n<p>Recently we have seen how simple architectures or simple practices (such as MLP Mixer, ResNet Strikes back and Patches are all you need) are overcoming complex SOTA architectures. Even in this paper we saw how simple Pooling operation performs better/at-par with Attention based module in Transformer for\u00a0Vision.</p>\n<p>\u201c<em>This finding conveys that the general architecture MetaFormer is actually what we need when designing vision models. By adopting MetaFormer, it is guaranteed that the derived models would have the potential to achieve reasonable performance.</em>\u201d</p>\n<p>For experiments, detailed results and other details, I highly recommend you to read the paper as you will get in-depth insights about all these\u00a0topics.</p>\n<p>This summary was short as the idea being conveyed was simple and precise in the\u00a0paper.</p>\n<p>Thanks for reading\u00a0:). Do follow me on Medium to get paper summaries every month.<br>Connect with me on <a href=\"https://www.linkedin.com/in/sahil-chachra/\">LinkedIn</a> too\u00a0:D.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b6f172482604\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>Paper Summary\u200a\u2014\u200aMetaFormer is Actually What You Need for\u00a0Vision</h3>\n<p>In recent times we have seen that Transformers (for vision) have performed very well, i.e., at par or at times surpassing the previously claimed SOTA for vision. The heart of the Transformer is the attention based token mixer which was applauded for the results the transformers gave in other fields as\u00a0well.</p>\n<p>Now, what if I tell you, you replace that entire Attention based token mixer (which takes in key, value and query and does wonders), and replace that with just a Pooling layer and the result you will get will be at par with many SOTA architectures!</p>\n<p>Can you believe? Just simple Pooling operations, (MaxPool, AvgPool, etc in vision) which has no parameters, perform at par with Attention Mechanism!? Even I did not believe until I read the paper\u200a\u2014<strong><em>\u200aMetaFormer is Actually What You Need for Vision\u00a0(</em></strong><a href=\"https://arxiv.org/abs/2111.11418\"><strong><em>link</em></strong></a><strong><em>).</em></strong></p>\n<p><strong>Note\u200a\u2014\u200aIf I quote anything from the paper it\u2019ll be italic and inside quotes. For example\u200a\u2014\u200a\u201c</strong><em>this is an\u00a0example\u201d</em></p>\n<h3>Abstract</h3>\n<p>It was believed that the attention module of the transformer contributed most to it\u2019s amazing performance but later when it was replaced by spatial MLPs, it still gave good results. So, as per the authors, it means that, apart from the attention module, the general architecture of the transformer contributes to its performance.<br>To verify this, the authors replaced the attention module with a non-parametric pooling layer (they called the model as PoolFormer) and the results were still good! This result encourages the authors to move ahead and propose the concept of \u201cMetaFormer\u201d where the general architecture of the transformer is used without any specific token\u00a0mixer.</p>\n<h3>Introduction</h3>\n<p>The attention module for mixing information among tokens in the transformer\u2019s encoder is termed as \u201ctoken mixer\u201d. Believing the attention mechanism to be the reason behind transformer\u2019s performance, many works have been proposed surrounding this\u00a0concept.</p>\n<p>But some of the recent works, replaced the attention module with a Fourier Transform and still performed very well, attaining 97% of the accuracy of Vanilla transformer. So this shows that, if we use MetaFormer (the general architecture of the transformer having any computation in the place of attention module) as the general architecture, good results are attained.</p>\n<p>PoolFormer, proposed by the authors, outperforms fine-tuned transformers and MLP like\u00a0models.</p>\n<p>Contributions of the paper are\u00a0:-</p>\n<ol>\n<li>\u201c<em>Abstract transformers into a general architecture MetaFormer\u201d</em>\n</li>\n<li>Authors evaluate PoolFormer in multiple tasks such as Classification, object detection, semantic segmentation and instance segmentation and it \u201c<em>achieves competitive performance compared with the SOTA models using sophistic design of token\u00a0mixers.\u201d</em>\n</li>\n</ol>\n<h3>Method</h3>\n<h4>MetaFormer</h4>\n<p><em>\u201cInput I is first processed by input embedding such as patch embedding in\u00a0VITs.\u201d</em></p>\n<p><em>X = </em>InputEmb(<em>I</em>)</p>\n<p>Where X is a matrix of real number of dimension NxC, N is for squence length and C is embedding dimension. \u201c<em>Then embedding tokens are fed to repeated MetaFormer blocks, each of which includes two residual sub-blocks. The first sub-block mainly contains a token mixer to communicate information among tokens\u201d and its represented as</em></p>\n<p><em>Y = </em>TokenMixer(Norm(X)) + X\u00a0, where Norm is Layer Normalization or Batch Normalization and TokenMixer is a module for mixing information like attention, Spatial MLP,\u00a0etc</p>\n<p><em>\u201cThe second sub-block consists of two-layered MLP with non-linear activation\u201d</em></p>\n<p>Z = NonLinearActivationFunction(Normalization(Y) * W1) * W2 +\u00a0Y</p>\n<p>where W1 is matrix of real numbers of dimension C x rC and W2 is matrix of real numbers of dimension rC x C. W1 &amp; W2 both are learnable parameters with MLP expansion ratio\u00a0r.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/702/1*p13M1xSf2W36K4jSm6TADA.png\"><figcaption>Figure 1\u200a\u2014\u200aSource\u00a0: From paper\u00a0itself</figcaption></figure><h4>PoolFormer</h4>\n<p>The authors use Pooling Operation as Token Mixer in the general architecture proposed above that is MetaFormer. This operation has no parameters which can be learnt and \u201c<em>it just makes each token averagely aggregate its nearby token features\u201d. </em>Spatial MLP and Self attention have quadratic computational complexity where as pooling has linear computational complexity.</p>\n<p>\u201c<em>PoolFormer has 4 stages H/4 * W/4, H/8 * W/8, H/16 * W/16 and H/32 * W/32. There are two groups having different embedding size\u00a0:-</em></p>\n<ol>\n<li><em>small-sized models with embedding dimensions of 64, 128, 320, and 512 responding to the four\u00a0stages;</em></li>\n<li><em>2) medium-sized models with embedding dimensions 96, 192, 384, and\u00a0768.</em></li>\n</ol>\n<p><em>Assuming there are L PoolFormer blocks in total, stages 1, 2, 3, and 4 will contain L/6, L/6, L/2, and L/6 PoolFormer blocks respectively. The MLP expansion ratio is set as 4. According to the above simple model scaling rule, we obtain 5 different model sizes of PoolFormer and their hyperparameters are shown</em>\u201d\u00a0below.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/754/1*ZxnVKXf19LArPOeZ_rhxXQ.png\"><figcaption>Figure 2\u200a\u2014\u200aSource\u00a0: paper\u00a0iteself</figcaption></figure><h3>Results</h3>\n<p>For ImageNet, PoolFormer-S24 has top-1 accuracy greater than 80% with only 21M parameters and 2.6G MACs whereas DeiT-s achieves 79.8 but requires 4.6G MACs. ResMLP-S24 needs 30M parameters andd 6G MACs with accuracy of\u00a079.4.</p>\n<h3>Conclusion</h3>\n<p>Recently we have seen how simple architectures or simple practices (such as MLP Mixer, ResNet Strikes back and Patches are all you need) are overcoming complex SOTA architectures. Even in this paper we saw how simple Pooling operation performs better/at-par with Attention based module in Transformer for\u00a0Vision.</p>\n<p>\u201c<em>This finding conveys that the general architecture MetaFormer is actually what we need when designing vision models. By adopting MetaFormer, it is guaranteed that the derived models would have the potential to achieve reasonable performance.</em>\u201d</p>\n<p>For experiments, detailed results and other details, I highly recommend you to read the paper as you will get in-depth insights about all these\u00a0topics.</p>\n<p>This summary was short as the idea being conveyed was simple and precise in the\u00a0paper.</p>\n<p>Thanks for reading\u00a0:). Do follow me on Medium to get paper summaries every month.<br>Connect with me on <a href=\"https://www.linkedin.com/in/sahil-chachra/\">LinkedIn</a> too\u00a0:D.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b6f172482604\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["deep-learning","computer-vision","data-science","ai"]},{"title":"Paper Summary\u200a\u2014\u200atorch.manual_seed(3407) is all you need","pubDate":"2021-12-11 20:00:33","link":"https://sahilchachra.medium.com/paper-summary-torch-manual-seed-3407-is-all-you-need-9ef0f7aa7d78?source=rss-f31bf6073414------2","guid":"https://medium.com/p/9ef0f7aa7d78","author":"Sahil Chachra","thumbnail":"https://cdn-images-1.medium.com/max/573/1*w8sr36Sn5rqLD1G70N9KIg.png","description":"\n<h3>Paper Summary\u200a\u2014\u200atorch.manual_seed(3407) is all you\u00a0need</h3>\n<p>Whenever we train a neural network from scratch, it\u2019s weights are initialized with random values. So, if you re-run the same training job again and again, the values used to initialized the weights will keep on changing as they would be randomly generated.</p>\n<p>Now just imagine, metric of a State of the Art architecture for a given task is 80. You propose a new architecture for the same task and train your model from scratch. After you run it once (assuming all hyper-parameters were just perfect), you get 79.8 metric value. For some reason, you then just re-run the experiment keeping everything unchanged. You get 80.2 metric value and your architecture has now surpassed previous SOTA\u2019s performance. Apart from weights of the neural network being randomly initialized, every other value/hyper-parameter was left unchanged during the second trial! So what do you think the reason might\u00a0be?</p>\n<p>Let\u2019s look at the summary of the paper\u200a\u2014<strong>\u200a<em>torch.manual_seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision</em> </strong>by<strong> <em>David Picard</em>. </strong>Link of the paper\u00a0: <a href=\"https://arxiv.org/pdf/2109.08203.pdf\">Paper\u00a0Link</a>.</p>\n<p><strong>Note\u200a\u2014\u200aIf I quote anything from the paper it\u2019ll be italic and inside quotes. For example\u200a\u2014\u200a\u201c</strong><em>this is an\u00a0example\u201d</em></p>\n<h3>Abstract</h3>\n<p>The author investigates the \u201c<em>effect of random seed selection on the accuracy\u201d</em> of a deep learning architecture for perception models. He has done so by scanning up to 10\u2074 seeds on datasets such as CIFAR-10 and on the ImageNet dataset by using pre-trained models. \u201c<strong><em>The conclusions are that even if the variance is not very large, it is surprisingly easy to find an outlier that performs much better or much worse than the average.</em></strong>\u201d</p>\n<h3>Introduction</h3>\n<p>The main aim of the experiment is\u00a0:-</p>\n<ol>\n<li>\u201c<em>What is the distribution of scores with respect to the choice of seed?</em>\u00a0\u201d</li>\n<li>\u201c<em>Are there black swans, i.e., seeds that produce radically different results?</em>\u201d</li>\n<li>\u201c<em>Does pretraining on larger datasets mitigate variability induced by the choice of\u00a0seed?</em>\u201d</li>\n</ol>\n<p>These questions are important to discuss/ask because it\u2019s a common practice in domains relying on machine learning because the papers report only single run of the experiment performed. Like the table in the papers (papers say who claim the new SOTA and prove it with experimental result) just show Architecture-small = X metric value, Architecture-medium = Y metric value, etc. They don\u2019t mention Architecture-small = X metric value (1st run), Architecture-small = X metric value (2nd run), etc. As per the author this trend is there because of limited computing resources and having one at least one result is better than no result at\u00a0all!</p>\n<p>Like physical experiments have noise in the measurement the same way we have random initialization, data split in train and test, etc. The author then tells that if the effect of setting random seeds for different experiments is negligible then things are fine but if not then the publications should include \u201c<em>detailed analysis of the contribution of each random factor to the variation in observed performances\u201d</em></p>\n<h3>Experimental Setup</h3>\n<p>For CIFAR 10\u200a\u2014\u200asome 10,000 seeds were explored where each seed experiment took 30 seconds to train on evaluate. GPU used by the author was V100. The model used was custom ResNet with 9\u00a0layers.</p>\n<p>For ImageNet, since it was nearly impossible train neural networks from scratch with so many seeds, the author used pretrained models \u201c<em>where only the classification layer is initialized from scratch.</em>\u201d The author used three different models\u200a\u2014\u200aSupervised ResNet 50, SSL ResNet 50 and SSL\u00a0ViT.</p>\n<p><strong>(Please refer the paper to know more about the training\u00a0setup)</strong></p>\n<h3>Limitations</h3>\n<p>The author informs that this experiments has many limitations which affects the final conclusion of the experiment.</p>\n<p>The accuracy from the experiments the author ran is not at par with SOTA results because of budget constraints. CIFAR 10\u2019s accuracy achieved is some how comparable to ResNet\u2019s result from 2016. \u201c<strong><em>This means papers from that era may have been subject to the sensitivity to the seed\u201d </em></strong>that the author observes.</p>\n<p>For ResNet50 model trained on ImageNet dataset, in DINO paper, the authors say that they get improved accuracy by tweaking the hyperparameters. But still the author believes that experiments done with ImageNet \u201c<em>underestimate the variability because they all start from the same pretrained model, which means the effect of the seed is limited to initialization of the classification layer and the optimization process.</em></p>\n<p>The author also says that these limitations would have overcome if 10 times more computation budget was available on CIFAR dataset and 50 to 100 times more computation budget was available for ImageNet\u00a0models.</p>\n<h3>Findings</h3>\n<h4>Convergence instability</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/573/1*w8sr36Sn5rqLD1G70N9KIg.png\"><figcaption>Source\u200a\u2014\u200afrom\u00a0Paper</figcaption></figure><p>This histograms shows density plot of final validation accuracy on CIFAR10 dataset on 500 seeds. Each das line is one\u00a0run.</p>\n<p>We can see a lot of dash lines at accuracy 90.50% and 91.00%. \u201c<em>Hence a 0.5% difference in accuracy could be entirely explained by just a seed difference, without having chosen a particularly bad or good\u00a0seed.</em>\u201d</p>\n<p>\u201c<strong><em>The answer to first question is thus that the distribution of scores with respect to the seeds is fairly concentrated and sort of pointy. This is a reassuring result because it means that scores are likely to be representative of what the model and the training setup can do, expect when one is actively searching (knowingly or unknowingly) for a good/bad\u00a0seed.</em></strong>\u201d</p>\n<h4>Searching for Black\u00a0Swans</h4>\n<p>Short training setup was used to scan 10\u2074 seeds. The minimum and maximum values are 89.01% and 90.83% respectively. This difference of 1.82% is termed as significant difference in the domain! This difference means whether or not the paper on the new SOTA can be published or\u00a0not!</p>\n<p>\u201c<strong><em>The results of this test allow me to answer positively to the second question: there are indeed seeds that produce scores sufficiently good (respectively bad) to be considered as a significant improvement (respectively downgrade) by the computer vision community. This is a worrying result as the community is currently very much score driven, and yet these can just be artifacts of randomness.</em></strong>\u201d</p>\n<h4>Large Scale\u00a0datasets</h4>\n<p>Author uses pretrained models which are fine tuned and evaluated in ImageNet to see if by using larger training set with a pretrained model still shows randomness in scores with respect to seed (50 seeds were used for ImageNet experiment).</p>\n<p>As per the accuracies reported, there is standard deviation of 0.1% and the difference between minimum and maximum values is about 0.5%. This difference is less than that seen in CIFAR10 experiments\u200a\u2014\u200athis is still surprising as all the experiments were ran using same initial pretrained weights apart from last layer. Only image batches varies as per the seed set. A difference of 0.5% in ImageNet is very significant value which will determine whether to publish the work or\u00a0not.</p>\n<p><strong>The answer to the third question is mixed\u200a\u2014\u200ausing pretrained models reduce the variation induced by the seed we choose. But the variation is still has to be considered as even 0.5% is still considered as significant improvement in vision community.</strong></p>\n<p>For large dataset such as ImageNet, it will be good if more than 50 seeds is scanned to study if choice of seed affects the accuracy even for pretrained models on large scale datasets.</p>\n<h3>Discussion</h3>\n<ol>\n<li>\u201c<em>What is the distribution of scores with respect to the choice of seed?</em> \u201d\u200a\u2014\u200a\u201c<em>Once the model converged, this distribution is relatively stable which means that some seed are intrinsically better than\u00a0others.</em>\u201d</li>\n<li>\u201c<em>Are there black swans, i.e., seeds that produce radically different results?</em>\u201d\u200a\u2014\u200a\u201c<em>Yes. On a scanning of 104 seeds, we obtained a difference between the maximum and minimum accuracy close to 2% which is above the threshold commonly used by the computer vision community of what is considered significant.</em>\u201d</li>\n<li>\u201c<em>Does pretraining on larger datasets mitigate variability induced by the choice of seed?</em>\u201d\u200a\u2014\u200a\u201c<em>It certainly reduces the variations due to using different seeds, but it does not mitigate it. On Imagenet, we found a difference between the maximum and the minimum accuracy of around 0.5%, which is commonly accepted as significant by the community for this dataset.</em>\u201d</li>\n</ol>\n<p>In the end the author suggests the researchers to study the randomness of different seeds on their experiments and asks them to report average, minimum, maximum and standard deviation scores.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9ef0f7aa7d78\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>Paper Summary\u200a\u2014\u200atorch.manual_seed(3407) is all you\u00a0need</h3>\n<p>Whenever we train a neural network from scratch, it\u2019s weights are initialized with random values. So, if you re-run the same training job again and again, the values used to initialized the weights will keep on changing as they would be randomly generated.</p>\n<p>Now just imagine, metric of a State of the Art architecture for a given task is 80. You propose a new architecture for the same task and train your model from scratch. After you run it once (assuming all hyper-parameters were just perfect), you get 79.8 metric value. For some reason, you then just re-run the experiment keeping everything unchanged. You get 80.2 metric value and your architecture has now surpassed previous SOTA\u2019s performance. Apart from weights of the neural network being randomly initialized, every other value/hyper-parameter was left unchanged during the second trial! So what do you think the reason might\u00a0be?</p>\n<p>Let\u2019s look at the summary of the paper\u200a\u2014<strong>\u200a<em>torch.manual_seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision</em> </strong>by<strong> <em>David Picard</em>. </strong>Link of the paper\u00a0: <a href=\"https://arxiv.org/pdf/2109.08203.pdf\">Paper\u00a0Link</a>.</p>\n<p><strong>Note\u200a\u2014\u200aIf I quote anything from the paper it\u2019ll be italic and inside quotes. For example\u200a\u2014\u200a\u201c</strong><em>this is an\u00a0example\u201d</em></p>\n<h3>Abstract</h3>\n<p>The author investigates the \u201c<em>effect of random seed selection on the accuracy\u201d</em> of a deep learning architecture for perception models. He has done so by scanning up to 10\u2074 seeds on datasets such as CIFAR-10 and on the ImageNet dataset by using pre-trained models. \u201c<strong><em>The conclusions are that even if the variance is not very large, it is surprisingly easy to find an outlier that performs much better or much worse than the average.</em></strong>\u201d</p>\n<h3>Introduction</h3>\n<p>The main aim of the experiment is\u00a0:-</p>\n<ol>\n<li>\u201c<em>What is the distribution of scores with respect to the choice of seed?</em>\u00a0\u201d</li>\n<li>\u201c<em>Are there black swans, i.e., seeds that produce radically different results?</em>\u201d</li>\n<li>\u201c<em>Does pretraining on larger datasets mitigate variability induced by the choice of\u00a0seed?</em>\u201d</li>\n</ol>\n<p>These questions are important to discuss/ask because it\u2019s a common practice in domains relying on machine learning because the papers report only single run of the experiment performed. Like the table in the papers (papers say who claim the new SOTA and prove it with experimental result) just show Architecture-small = X metric value, Architecture-medium = Y metric value, etc. They don\u2019t mention Architecture-small = X metric value (1st run), Architecture-small = X metric value (2nd run), etc. As per the author this trend is there because of limited computing resources and having one at least one result is better than no result at\u00a0all!</p>\n<p>Like physical experiments have noise in the measurement the same way we have random initialization, data split in train and test, etc. The author then tells that if the effect of setting random seeds for different experiments is negligible then things are fine but if not then the publications should include \u201c<em>detailed analysis of the contribution of each random factor to the variation in observed performances\u201d</em></p>\n<h3>Experimental Setup</h3>\n<p>For CIFAR 10\u200a\u2014\u200asome 10,000 seeds were explored where each seed experiment took 30 seconds to train on evaluate. GPU used by the author was V100. The model used was custom ResNet with 9\u00a0layers.</p>\n<p>For ImageNet, since it was nearly impossible train neural networks from scratch with so many seeds, the author used pretrained models \u201c<em>where only the classification layer is initialized from scratch.</em>\u201d The author used three different models\u200a\u2014\u200aSupervised ResNet 50, SSL ResNet 50 and SSL\u00a0ViT.</p>\n<p><strong>(Please refer the paper to know more about the training\u00a0setup)</strong></p>\n<h3>Limitations</h3>\n<p>The author informs that this experiments has many limitations which affects the final conclusion of the experiment.</p>\n<p>The accuracy from the experiments the author ran is not at par with SOTA results because of budget constraints. CIFAR 10\u2019s accuracy achieved is some how comparable to ResNet\u2019s result from 2016. \u201c<strong><em>This means papers from that era may have been subject to the sensitivity to the seed\u201d </em></strong>that the author observes.</p>\n<p>For ResNet50 model trained on ImageNet dataset, in DINO paper, the authors say that they get improved accuracy by tweaking the hyperparameters. But still the author believes that experiments done with ImageNet \u201c<em>underestimate the variability because they all start from the same pretrained model, which means the effect of the seed is limited to initialization of the classification layer and the optimization process.</em></p>\n<p>The author also says that these limitations would have overcome if 10 times more computation budget was available on CIFAR dataset and 50 to 100 times more computation budget was available for ImageNet\u00a0models.</p>\n<h3>Findings</h3>\n<h4>Convergence instability</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/573/1*w8sr36Sn5rqLD1G70N9KIg.png\"><figcaption>Source\u200a\u2014\u200afrom\u00a0Paper</figcaption></figure><p>This histograms shows density plot of final validation accuracy on CIFAR10 dataset on 500 seeds. Each das line is one\u00a0run.</p>\n<p>We can see a lot of dash lines at accuracy 90.50% and 91.00%. \u201c<em>Hence a 0.5% difference in accuracy could be entirely explained by just a seed difference, without having chosen a particularly bad or good\u00a0seed.</em>\u201d</p>\n<p>\u201c<strong><em>The answer to first question is thus that the distribution of scores with respect to the seeds is fairly concentrated and sort of pointy. This is a reassuring result because it means that scores are likely to be representative of what the model and the training setup can do, expect when one is actively searching (knowingly or unknowingly) for a good/bad\u00a0seed.</em></strong>\u201d</p>\n<h4>Searching for Black\u00a0Swans</h4>\n<p>Short training setup was used to scan 10\u2074 seeds. The minimum and maximum values are 89.01% and 90.83% respectively. This difference of 1.82% is termed as significant difference in the domain! This difference means whether or not the paper on the new SOTA can be published or\u00a0not!</p>\n<p>\u201c<strong><em>The results of this test allow me to answer positively to the second question: there are indeed seeds that produce scores sufficiently good (respectively bad) to be considered as a significant improvement (respectively downgrade) by the computer vision community. This is a worrying result as the community is currently very much score driven, and yet these can just be artifacts of randomness.</em></strong>\u201d</p>\n<h4>Large Scale\u00a0datasets</h4>\n<p>Author uses pretrained models which are fine tuned and evaluated in ImageNet to see if by using larger training set with a pretrained model still shows randomness in scores with respect to seed (50 seeds were used for ImageNet experiment).</p>\n<p>As per the accuracies reported, there is standard deviation of 0.1% and the difference between minimum and maximum values is about 0.5%. This difference is less than that seen in CIFAR10 experiments\u200a\u2014\u200athis is still surprising as all the experiments were ran using same initial pretrained weights apart from last layer. Only image batches varies as per the seed set. A difference of 0.5% in ImageNet is very significant value which will determine whether to publish the work or\u00a0not.</p>\n<p><strong>The answer to the third question is mixed\u200a\u2014\u200ausing pretrained models reduce the variation induced by the seed we choose. But the variation is still has to be considered as even 0.5% is still considered as significant improvement in vision community.</strong></p>\n<p>For large dataset such as ImageNet, it will be good if more than 50 seeds is scanned to study if choice of seed affects the accuracy even for pretrained models on large scale datasets.</p>\n<h3>Discussion</h3>\n<ol>\n<li>\u201c<em>What is the distribution of scores with respect to the choice of seed?</em> \u201d\u200a\u2014\u200a\u201c<em>Once the model converged, this distribution is relatively stable which means that some seed are intrinsically better than\u00a0others.</em>\u201d</li>\n<li>\u201c<em>Are there black swans, i.e., seeds that produce radically different results?</em>\u201d\u200a\u2014\u200a\u201c<em>Yes. On a scanning of 104 seeds, we obtained a difference between the maximum and minimum accuracy close to 2% which is above the threshold commonly used by the computer vision community of what is considered significant.</em>\u201d</li>\n<li>\u201c<em>Does pretraining on larger datasets mitigate variability induced by the choice of seed?</em>\u201d\u200a\u2014\u200a\u201c<em>It certainly reduces the variations due to using different seeds, but it does not mitigate it. On Imagenet, we found a difference between the maximum and the minimum accuracy of around 0.5%, which is commonly accepted as significant by the community for this dataset.</em>\u201d</li>\n</ol>\n<p>In the end the author suggests the researchers to study the randomness of different seeds on their experiments and asks them to report average, minimum, maximum and standard deviation scores.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9ef0f7aa7d78\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["deep-learning","artificial-intelligence","data-science","research","experiment"]},{"title":"Paper Summary\u200a\u2014\u200aWhat is being transferred in transfer learning?","pubDate":"2021-11-21 10:29:03","link":"https://sahilchachra.medium.com/paper-summary-what-is-being-transferred-in-transfer-learning-250dc7a9d127?source=rss-f31bf6073414------2","guid":"https://medium.com/p/250dc7a9d127","author":"Sahil Chachra","thumbnail":"https://cdn-images-1.medium.com/max/768/1*zaVM0VQ0OxDeYlpd4VPHlQ@2x.jpeg","description":"\n<h3>Paper Summary\u200a\u2014\u200aWhat is being transferred in transfer learning?</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/768/1*zaVM0VQ0OxDeYlpd4VPHlQ@2x.jpeg\"></figure><p>Now-a-days we widely use the transfer learning to either achieve good results or to get save time on training the model from scratch. But we don\u2019t know what is actually being transferred during this\u00a0process.</p>\n<p>If you use a ImageNet pretrained classification model and train it on some medical domain (say chest X-ray images for some disease) you will see it\u2019s performing better than training that same model from scratch (considering we have small train set)! How even the objects present in the ImageNet dataset are similar to Human chest X-ray\u00a0images!?</p>\n<p>Few days ago, I came across this paper\u200a\u2014\u200a<strong>What is being transferred in transfer learning</strong> (<a href=\"https://arxiv.org/abs/2008.11687\">link</a>). It\u2019s a paper from <strong>Google Brain</strong>. This paper helps you understand what is happening when pre-trained models are trained for some different task. This blog is a summary of the paper.<strong> I will try to cover most of the topics but read the paper too.<em> Its worth the\u00a0time</em>!</strong></p>\n<p>Coming to the\u00a0paper\u2026</p>\n<p><strong>Note\u200a\u2014\u200aIf I quote anything from the paper it\u2019ll be italic and inside quotes. For example\u200a\u2014\u200a\u201c</strong><em>this is an example.\u201d</em></p>\n<p><em>I have not discussed about Performance barrier and loss landscape because I believe it is better to read this part from the paper directly as summarizing this would be copying stuff from the paper and there is no point just\u00a0copying.</em></p>\n<h3>Abstract</h3>\n<p>In the abstract the authors say that although we are using transfer learning so much yet we do not understand \u201c<em>which part of the neural network is responsible\u201d </em>for this. In this paper, the authors<em> \u201cprovide new tools and analyses to address these fundamental questions\u201d.</em> This paper focuses on results of several experiments which were done to understand what is being transferred in transfer learning. In short, one experiment was on \u201c<em>block-shuffled images\u201d, </em>then they separated<em> </em>the <em>\u201ceffect of feature reuse from learning low-level statistics of data</em>\u201d, they also show that when we use pretrained model weights, the <em>\u201cmodel stays in the same basin in the loss landscape and different instances of such model are similar in feature space and close in parameter space.\u201d</em></p>\n<h3>Introduction</h3>\n<p>We use transfer learning to transfer knowledge from one domain(source domain) to other domain(target domain) when we have less data or we need to produce results in lesser time. In total three datasets were used to carry out experiments\u200a\u2014\u200aImageNet, DomainNet (has images such as real images, sketches and clip arts to study transfer learning) and CheXpert (chest X-ray\u00a0images)</p>\n<h3>Problem Setup</h3>\n<p>The authors use ImageNet(Source domain) pre-trained weights as starting weights for transfer learning and consider CheXpert and datasets from DomainNet as target domain (or downstream task). In other words, they take ImageNet pretrained weights and train the model on CheXpert and DomainNet dataset to study transfer learning.</p>\n<p>The authors \u201c<em>analyze networks in four different cases\u00a0: pre-trained network, the network at random initialization, the network that is fine-tuned on target domain after pre-training on source domain and the model that is trained on target domain from random initialization.\u201d </em>All these four cases will be referred as following in the paper\u00a0: \u201c<em>RI (random initialization), P (pre trained model), RI-T (model trained on target domain from random initialization), P-T (model trained/fine-tuned on target domain starting from pre-trained weights).\u201d</em></p>\n<h3>Role of Feature\u00a0reuse</h3>\n<p>We usually use transfer learning when we have a very small dataset for target domain. But still we don\u2019t know how this works even when source domain (dataset of pre-trained weights) is visually very different from the target domain (dataset for the current\u00a0task).</p>\n<p>To study the <strong>role of feature reuse</strong>, ImageNet pre-trained model was used and target domains were CheXpert and DomainNet.</p>\n<p>When RI-T (<em>model trained on target domain from random initialization) was compared to P-T(model trained/fine-tuned on target domain starting from pre-trained weights), </em>a very huge performance boost was seen on the DomainNet\u2019s Real dataset. \u201c<em>This confirms the intuition that feature reuse plays an important role in transfer learning\u201d. </em>Now, when ImageNet pretrained weights were used to fine-tune the model on CheXpert dataset, it was observed that P-T converges faster than RI-T in all the cases. \u201c<em>This suggests additional benefits of pre-trained weights that are not directly coming from feature\u00a0reuse\u201d.</em></p>\n<p>To further verify this hypothesis, \u201c<em>a series of modified downstream task\u201d </em>were created which were very different from each other visually. To be precise, image was partitioned into equal sized blocks and those blocks were shuffled across the image randomly. By doing this, the image was changing visually but the low level statistics remained the same. (Block size 1 means\u200a\u2014\u200apixels of size 1 were taken and shuffled across the image). Now the target domain looked visually far distinct from the source\u00a0domain.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/482/1*c-_KWjUgNpyCtDSZVRQQXg.png\"><figcaption>Middle Image\u200a\u2014\u200aPixel of block size 8 shuffled. Right image\u200a\u2014\u200aPixel of block size 1 shuffled. (Source\u200a\u2014\u200aPaper\u00a0itself)</figcaption></figure><p>After performing this shuffling, it was observed that\u200a\u2014\u200a1) final performance for RI-T and P-T drops as blocks size becomes smaller in size indicating that task is becoming more difficult to learn. 2) There is a decrease in relative accuracy difference with decrease in block size on real and clipart dataset from DomainNet \u201c<em>showing consistency with the intuition that decreasing feature reuse leads to diminishing benefits\u201d. </em>3) On q1uick draw dataset from DomainNet, there not much decrease. The dataset was already very dissimilar to the ImageNet dataset, indicating that some other factors of the pre-trained weights are helping the model in the downstream task.</p>\n<p><strong>The authors conclude that\u200a\u2014\u200a\u201c</strong><em>feature reuse plays a very important role in transfer learning, especially when the downstream task shares similar visual features with the pre-training domain.\u201d </em>They also tell that there are low level statistics which don\u2019t have any visual information play a role in transfer learning as even after shuffling the image pixels completely and shuffling the input channels, they did not see significant decreasing trend of metrics for quickdraw dataset.</p>\n<h3>Opening up the\u00a0model</h3>\n<p>In the second experiment, the authors <strong>investigate the mistakes made by the model</strong>, that is, they see the \u201c<em>common mistakes\u201d </em>made by both the models if they classify the same data point incorrectly. Then they also see \u201c<em>uncommon mistakes\u201d</em> where in one model classifies a data point correctly and other one incorrectly. They first \u201c<em>compare the ratio of common and uncommon mistakes between two P-Ts, a P-T and a RI-T and two RI-Ts\u201d. T</em>hey see that good number of uncommon mistakes are between O-T and RI-T where as two P-Ts have very few uncommon mistakes. This trend is visible for CheXpert and DomainNet dataset.</p>\n<p><strong>Feature similarity</strong>\u00a0: The authors use Centered Kernel alignment technique to measure similarity between \u201c<em>two output features in a layer of network architecture given two instances of such network\u201d.</em> It was observed that \u201c<em> two instances of P-T are highly similar across different layers\u201d. </em>It was noted that the feature similarity is stronger towards the last layers of the network in case of P-T and RI-T. <strong>Important point to be noted here\u200a\u2014\u200a\u201c</strong><em>These experiments show that the initialization point, whether pre-trained or random, drastically impacts feature similarity, and although both networks are showing high accuracy, they are not that similar in the feature space. This emphasizes on role of feature reuse and that two P-T are reusing the same features.\u201d</em></p>\n<h3>Module Criticality</h3>\n<p>In this section, the authors discuss how to calculate which module of the architecture is more critical from the rest of the\u00a0modules.</p>\n<p>The most important outcome was that\u200a\u2014\u200ait was seen that lower layers of the model are responsible for more general features while the higher layers have features which are specific to the target domain/dataset the model is being trained\u00a0on.</p>\n<h3>Conclusion</h3>\n<p>Do checkout the paper once to gain more insights about the experiments performed. This paper tells us that\u00a0:-</p>\n<ol>\n<li>We can use (or at least consider trying) transfer learning even if the target domain is visually very different from the source domain (dataset used in pre-trained weights)</li>\n<li>Features in the lower layers and some low level statistics which we cannot see visually are mostly responsible for this amazing performance of transfer learning.</li>\n<li>Since upper layers are more domain specific, we should freeze lower layers and train the upper layers for better performance of the model. But from which layers to start unfreezing would require extensive experiments and resources.</li>\n</ol>\n<p>Thanks for reading the blog. Hope you find it helpful!<br>Connect with me on <a href=\"https://www.linkedin.com/in/sahil-chachra/\">LinkedIn</a>\u00a0:).</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=250dc7a9d127\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>Paper Summary\u200a\u2014\u200aWhat is being transferred in transfer learning?</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/768/1*zaVM0VQ0OxDeYlpd4VPHlQ@2x.jpeg\"></figure><p>Now-a-days we widely use the transfer learning to either achieve good results or to get save time on training the model from scratch. But we don\u2019t know what is actually being transferred during this\u00a0process.</p>\n<p>If you use a ImageNet pretrained classification model and train it on some medical domain (say chest X-ray images for some disease) you will see it\u2019s performing better than training that same model from scratch (considering we have small train set)! How even the objects present in the ImageNet dataset are similar to Human chest X-ray\u00a0images!?</p>\n<p>Few days ago, I came across this paper\u200a\u2014\u200a<strong>What is being transferred in transfer learning</strong> (<a href=\"https://arxiv.org/abs/2008.11687\">link</a>). It\u2019s a paper from <strong>Google Brain</strong>. This paper helps you understand what is happening when pre-trained models are trained for some different task. This blog is a summary of the paper.<strong> I will try to cover most of the topics but read the paper too.<em> Its worth the\u00a0time</em>!</strong></p>\n<p>Coming to the\u00a0paper\u2026</p>\n<p><strong>Note\u200a\u2014\u200aIf I quote anything from the paper it\u2019ll be italic and inside quotes. For example\u200a\u2014\u200a\u201c</strong><em>this is an example.\u201d</em></p>\n<p><em>I have not discussed about Performance barrier and loss landscape because I believe it is better to read this part from the paper directly as summarizing this would be copying stuff from the paper and there is no point just\u00a0copying.</em></p>\n<h3>Abstract</h3>\n<p>In the abstract the authors say that although we are using transfer learning so much yet we do not understand \u201c<em>which part of the neural network is responsible\u201d </em>for this. In this paper, the authors<em> \u201cprovide new tools and analyses to address these fundamental questions\u201d.</em> This paper focuses on results of several experiments which were done to understand what is being transferred in transfer learning. In short, one experiment was on \u201c<em>block-shuffled images\u201d, </em>then they separated<em> </em>the <em>\u201ceffect of feature reuse from learning low-level statistics of data</em>\u201d, they also show that when we use pretrained model weights, the <em>\u201cmodel stays in the same basin in the loss landscape and different instances of such model are similar in feature space and close in parameter space.\u201d</em></p>\n<h3>Introduction</h3>\n<p>We use transfer learning to transfer knowledge from one domain(source domain) to other domain(target domain) when we have less data or we need to produce results in lesser time. In total three datasets were used to carry out experiments\u200a\u2014\u200aImageNet, DomainNet (has images such as real images, sketches and clip arts to study transfer learning) and CheXpert (chest X-ray\u00a0images)</p>\n<h3>Problem Setup</h3>\n<p>The authors use ImageNet(Source domain) pre-trained weights as starting weights for transfer learning and consider CheXpert and datasets from DomainNet as target domain (or downstream task). In other words, they take ImageNet pretrained weights and train the model on CheXpert and DomainNet dataset to study transfer learning.</p>\n<p>The authors \u201c<em>analyze networks in four different cases\u00a0: pre-trained network, the network at random initialization, the network that is fine-tuned on target domain after pre-training on source domain and the model that is trained on target domain from random initialization.\u201d </em>All these four cases will be referred as following in the paper\u00a0: \u201c<em>RI (random initialization), P (pre trained model), RI-T (model trained on target domain from random initialization), P-T (model trained/fine-tuned on target domain starting from pre-trained weights).\u201d</em></p>\n<h3>Role of Feature\u00a0reuse</h3>\n<p>We usually use transfer learning when we have a very small dataset for target domain. But still we don\u2019t know how this works even when source domain (dataset of pre-trained weights) is visually very different from the target domain (dataset for the current\u00a0task).</p>\n<p>To study the <strong>role of feature reuse</strong>, ImageNet pre-trained model was used and target domains were CheXpert and DomainNet.</p>\n<p>When RI-T (<em>model trained on target domain from random initialization) was compared to P-T(model trained/fine-tuned on target domain starting from pre-trained weights), </em>a very huge performance boost was seen on the DomainNet\u2019s Real dataset. \u201c<em>This confirms the intuition that feature reuse plays an important role in transfer learning\u201d. </em>Now, when ImageNet pretrained weights were used to fine-tune the model on CheXpert dataset, it was observed that P-T converges faster than RI-T in all the cases. \u201c<em>This suggests additional benefits of pre-trained weights that are not directly coming from feature\u00a0reuse\u201d.</em></p>\n<p>To further verify this hypothesis, \u201c<em>a series of modified downstream task\u201d </em>were created which were very different from each other visually. To be precise, image was partitioned into equal sized blocks and those blocks were shuffled across the image randomly. By doing this, the image was changing visually but the low level statistics remained the same. (Block size 1 means\u200a\u2014\u200apixels of size 1 were taken and shuffled across the image). Now the target domain looked visually far distinct from the source\u00a0domain.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/482/1*c-_KWjUgNpyCtDSZVRQQXg.png\"><figcaption>Middle Image\u200a\u2014\u200aPixel of block size 8 shuffled. Right image\u200a\u2014\u200aPixel of block size 1 shuffled. (Source\u200a\u2014\u200aPaper\u00a0itself)</figcaption></figure><p>After performing this shuffling, it was observed that\u200a\u2014\u200a1) final performance for RI-T and P-T drops as blocks size becomes smaller in size indicating that task is becoming more difficult to learn. 2) There is a decrease in relative accuracy difference with decrease in block size on real and clipart dataset from DomainNet \u201c<em>showing consistency with the intuition that decreasing feature reuse leads to diminishing benefits\u201d. </em>3) On q1uick draw dataset from DomainNet, there not much decrease. The dataset was already very dissimilar to the ImageNet dataset, indicating that some other factors of the pre-trained weights are helping the model in the downstream task.</p>\n<p><strong>The authors conclude that\u200a\u2014\u200a\u201c</strong><em>feature reuse plays a very important role in transfer learning, especially when the downstream task shares similar visual features with the pre-training domain.\u201d </em>They also tell that there are low level statistics which don\u2019t have any visual information play a role in transfer learning as even after shuffling the image pixels completely and shuffling the input channels, they did not see significant decreasing trend of metrics for quickdraw dataset.</p>\n<h3>Opening up the\u00a0model</h3>\n<p>In the second experiment, the authors <strong>investigate the mistakes made by the model</strong>, that is, they see the \u201c<em>common mistakes\u201d </em>made by both the models if they classify the same data point incorrectly. Then they also see \u201c<em>uncommon mistakes\u201d</em> where in one model classifies a data point correctly and other one incorrectly. They first \u201c<em>compare the ratio of common and uncommon mistakes between two P-Ts, a P-T and a RI-T and two RI-Ts\u201d. T</em>hey see that good number of uncommon mistakes are between O-T and RI-T where as two P-Ts have very few uncommon mistakes. This trend is visible for CheXpert and DomainNet dataset.</p>\n<p><strong>Feature similarity</strong>\u00a0: The authors use Centered Kernel alignment technique to measure similarity between \u201c<em>two output features in a layer of network architecture given two instances of such network\u201d.</em> It was observed that \u201c<em> two instances of P-T are highly similar across different layers\u201d. </em>It was noted that the feature similarity is stronger towards the last layers of the network in case of P-T and RI-T. <strong>Important point to be noted here\u200a\u2014\u200a\u201c</strong><em>These experiments show that the initialization point, whether pre-trained or random, drastically impacts feature similarity, and although both networks are showing high accuracy, they are not that similar in the feature space. This emphasizes on role of feature reuse and that two P-T are reusing the same features.\u201d</em></p>\n<h3>Module Criticality</h3>\n<p>In this section, the authors discuss how to calculate which module of the architecture is more critical from the rest of the\u00a0modules.</p>\n<p>The most important outcome was that\u200a\u2014\u200ait was seen that lower layers of the model are responsible for more general features while the higher layers have features which are specific to the target domain/dataset the model is being trained\u00a0on.</p>\n<h3>Conclusion</h3>\n<p>Do checkout the paper once to gain more insights about the experiments performed. This paper tells us that\u00a0:-</p>\n<ol>\n<li>We can use (or at least consider trying) transfer learning even if the target domain is visually very different from the source domain (dataset used in pre-trained weights)</li>\n<li>Features in the lower layers and some low level statistics which we cannot see visually are mostly responsible for this amazing performance of transfer learning.</li>\n<li>Since upper layers are more domain specific, we should freeze lower layers and train the upper layers for better performance of the model. But from which layers to start unfreezing would require extensive experiments and resources.</li>\n</ol>\n<p>Thanks for reading the blog. Hope you find it helpful!<br>Connect with me on <a href=\"https://www.linkedin.com/in/sahil-chachra/\">LinkedIn</a>\u00a0:).</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=250dc7a9d127\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["deep-learning","transfer-learning","data-science","artificial-intelligence"]},{"title":"Paper summary\u200a\u2014\u200aDecoupled Weight Decay Regularization","pubDate":"2021-10-29 14:30:19","link":"https://sahilchachra.medium.com/paper-summary-decoupled-weight-decay-regularization-1583cbc855bd?source=rss-f31bf6073414------2","guid":"https://medium.com/p/1583cbc855bd","author":"Sahil Chachra","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*dVPcugWY3wyxmSGYdUyDYg.png","description":"\n<h3>Paper summary\u200a\u2014\u200aDecoupled Weight Decay Regularization</h3>\n<p>If you search for - what is difference between L2 Regularization and Weight decay regularization, the most frequent answer would be that both are somewhat same. Yes both take you to same result in Stochastic Gradient Descent with Momentum but not when it comes to Adaptive gradient optimizers. The concept and working of weight decay in these adaptive optimizer (ex. Adam) is different.</p>\n<p>I highly recommend you to read the paper Decoupled Weight Decay Regularization by Ilya Loshchilov &amp; Frank Hutter\u200a\u2014\u200a<a href=\"https://arxiv.org/abs/1711.05101\">link</a>.</p>\n<p>Note\u200a\u2014\u200aAll the content in this summary/blog was written by referring to the above paper. At times I would quote from the paper because not every sentence can be reframed and still retain the meaning of it. <strong>Wherever I will quote from the paper, I will make it italic and put that in\u00a0quotes.</strong></p>\n<h3>Abstract</h3>\n<p>In the abstract, the authors convey that L2 regularization and Weight decay regularization are same for standard SGD but it\u2019s not the same case with Adaptive algorithms such as Adam. So in this paper they suggest a small modification to recover the idea of weight decay by <em>\u201cdecoupling the weight decay from the optimization steps taken w.r.t the loss function\u201d. </em>Moving forward in the paper, authors have provided experiments/evidence to prove that the modifications done are effective.</p>\n<h3>Introduction</h3>\n<p>L2 regularization and weight decay are not the same for adaptive algorithms but is equivalent in the case of SGD. When using L2 regularization with Adam it is seen that the older/historic gradients are being regularized less as compared to that while using weight\u00a0decay.</p>\n<p>\u201c<em>L2 regularization is not effective in Adam\u201d mostly because of deep learning libraries implementing only L2 regularization and \u201cnot the original weight\u00a0decay\u201d.</em></p>\n<p>\u201c<em>Weight decay is equally effective in both SGD and\u00a0Adam</em>\u201d</p>\n<p>Performance of weight decay depends on the batch size. Larger the batch size, smaller it the favorable weight\u00a0decay.</p>\n<p>It is advised to use learning rate multiplier/scheduler while using\u00a0Adam.</p>\n<p><strong>\u201c<em>Main contribution of this paper is to improve regularization in Adam by decoupling the weight decay from the gradient-based update.\u201d</em></strong></p>\n<h3>Decoupling the Weight Decay from gradient based weight\u00a0update</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*dVPcugWY3wyxmSGYdUyDYg.png\"><figcaption>Source of image\u200a\u2014\u200aFrom original paper itself\u200a\u2014\u200a<a href=\"https://arxiv.org/abs/1711.05101\">Link</a></figcaption></figure><p>In the paper, the author propose to decay the weights while updating weight in current iteration for SGD (line 9). In this way, \u03bb<strong> </strong>and \u03b1 can be decoupled (independent of each other)(As before line 6 had \u03bb and line 8 had \u03b1\u200a\u2014\u200amaking each other dependent).</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Q9W_oSHOK3c8K-ZliSWYhQ.png\"><figcaption>Source of image\u200a\u2014\u200aFrom original paper itself\u200a\u2014\u200a<a href=\"https://arxiv.org/abs/1711.05101\">Link</a></figcaption></figure><p>When Adam is run with L2 regularization, it was seen that \u201c<em>weights that tend to have large gradients in f do not get regularized as much as they would with decoupled weight decay since the gradient of the regularizer gets scaled along with the gradient of f\u00a0</em>\u201d</p>\n<p><strong>How does L2 regularization and Weight decay differ in terms of Adaptive gradient algorithms?\u200a\u2014\u200a</strong>\u201c<em>In L2 regularization, the sum of the gradient of the loss function and the gradient of the regularizer are adapted, whereas with decoupled weight decay, only the gradient of the loss function are adapted. With L2, both types of gradients are normalized by their typical magnitudes and there weights x with large typical gradient magnitude s are regularized by a smaller relative amount than other\u00a0weights.</em></p>\n<p><em>In contrast, de-coupled weight decay regularizes all weights with the same rate \u03bb, effectively regularizing weights x with large s more than standard L2 regularization does.</em>\u201d</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/987/1*tX-L5ZjBlGlJyadWtXt9rA.png\"><figcaption>Figure 1\u200a\u2014\u200aSource of image\u200a\u2014\u200aFrom original paper itself\u200a\u2014\u200a<a href=\"https://arxiv.org/abs/1711.05101\">Link</a></figcaption></figure><p>From the above image we can see AdamW (Adam with Weight decay) with cosine annealing learning rate scheduler gives the best performance and hence the authors suggest to use learning rate schedulers with adaptive gradient algorithms as\u00a0well.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/923/1*2lAAgTKmMxZRn36M0PAK-w.png\"><figcaption>Figure 2\u200a\u2014\u200aSource of image\u200a\u2014\u200aFrom original paper itself\u200a\u2014\u200a<a href=\"https://arxiv.org/abs/1711.05101\">Link</a></figcaption></figure><p>From Figure 2, Top left graph we can see that in SGD, L2 regularization is not decoupled from learning rate as the best hyperparameter basin is diagonal meaning initial learning rate and L2 regularization are interdependent on each other. This means, if we change only one of them we might get worse results. So to get best results we will have to change both simultaneously\u200a\u2014\u200ainitial learning rate and L2 regularization, giving large hyperparameter space.</p>\n<p>On the other hand (referring to Figure 2\u200a\u2014\u200agraph on top right), SGD with Weight decay or SGDW shows that initial learning rate and L2 regularization are decoupled. This shows that even if learning rate is not well tuned, having it fixed at some value and only changing weight decay factor would yield good value. This is also shown by the graph not diagonal.</p>\n<p>Coming to Adam with L2 regularization (Figure 2\u200a\u2014\u200abottom left graph), we see that it performs even worse than\u00a0SGD.</p>\n<p>Adam with weight decay or AdamW(Figure 2\u200a\u2014\u200abottom right graph) shows that it largely decouples learning rate and weight decay\u200a\u2014\u200akeep one parameter constant and try to optimize the other. Performance of this was better than SGD, Adam and\u00a0SGDW!</p>\n<p>Do refer the paper for more insights on experiments, performance and mathematical proofs!</p>\n<p>Thanks for reading the summary.<br>Connect with me on <a href=\"https://www.linkedin.com/in/sahil-chachra/\">LinkedIn</a>.\u00a0:)</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=1583cbc855bd\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>Paper summary\u200a\u2014\u200aDecoupled Weight Decay Regularization</h3>\n<p>If you search for - what is difference between L2 Regularization and Weight decay regularization, the most frequent answer would be that both are somewhat same. Yes both take you to same result in Stochastic Gradient Descent with Momentum but not when it comes to Adaptive gradient optimizers. The concept and working of weight decay in these adaptive optimizer (ex. Adam) is different.</p>\n<p>I highly recommend you to read the paper Decoupled Weight Decay Regularization by Ilya Loshchilov &amp; Frank Hutter\u200a\u2014\u200a<a href=\"https://arxiv.org/abs/1711.05101\">link</a>.</p>\n<p>Note\u200a\u2014\u200aAll the content in this summary/blog was written by referring to the above paper. At times I would quote from the paper because not every sentence can be reframed and still retain the meaning of it. <strong>Wherever I will quote from the paper, I will make it italic and put that in\u00a0quotes.</strong></p>\n<h3>Abstract</h3>\n<p>In the abstract, the authors convey that L2 regularization and Weight decay regularization are same for standard SGD but it\u2019s not the same case with Adaptive algorithms such as Adam. So in this paper they suggest a small modification to recover the idea of weight decay by <em>\u201cdecoupling the weight decay from the optimization steps taken w.r.t the loss function\u201d. </em>Moving forward in the paper, authors have provided experiments/evidence to prove that the modifications done are effective.</p>\n<h3>Introduction</h3>\n<p>L2 regularization and weight decay are not the same for adaptive algorithms but is equivalent in the case of SGD. When using L2 regularization with Adam it is seen that the older/historic gradients are being regularized less as compared to that while using weight\u00a0decay.</p>\n<p>\u201c<em>L2 regularization is not effective in Adam\u201d mostly because of deep learning libraries implementing only L2 regularization and \u201cnot the original weight\u00a0decay\u201d.</em></p>\n<p>\u201c<em>Weight decay is equally effective in both SGD and\u00a0Adam</em>\u201d</p>\n<p>Performance of weight decay depends on the batch size. Larger the batch size, smaller it the favorable weight\u00a0decay.</p>\n<p>It is advised to use learning rate multiplier/scheduler while using\u00a0Adam.</p>\n<p><strong>\u201c<em>Main contribution of this paper is to improve regularization in Adam by decoupling the weight decay from the gradient-based update.\u201d</em></strong></p>\n<h3>Decoupling the Weight Decay from gradient based weight\u00a0update</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*dVPcugWY3wyxmSGYdUyDYg.png\"><figcaption>Source of image\u200a\u2014\u200aFrom original paper itself\u200a\u2014\u200a<a href=\"https://arxiv.org/abs/1711.05101\">Link</a></figcaption></figure><p>In the paper, the author propose to decay the weights while updating weight in current iteration for SGD (line 9). In this way, \u03bb<strong> </strong>and \u03b1 can be decoupled (independent of each other)(As before line 6 had \u03bb and line 8 had \u03b1\u200a\u2014\u200amaking each other dependent).</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Q9W_oSHOK3c8K-ZliSWYhQ.png\"><figcaption>Source of image\u200a\u2014\u200aFrom original paper itself\u200a\u2014\u200a<a href=\"https://arxiv.org/abs/1711.05101\">Link</a></figcaption></figure><p>When Adam is run with L2 regularization, it was seen that \u201c<em>weights that tend to have large gradients in f do not get regularized as much as they would with decoupled weight decay since the gradient of the regularizer gets scaled along with the gradient of f\u00a0</em>\u201d</p>\n<p><strong>How does L2 regularization and Weight decay differ in terms of Adaptive gradient algorithms?\u200a\u2014\u200a</strong>\u201c<em>In L2 regularization, the sum of the gradient of the loss function and the gradient of the regularizer are adapted, whereas with decoupled weight decay, only the gradient of the loss function are adapted. With L2, both types of gradients are normalized by their typical magnitudes and there weights x with large typical gradient magnitude s are regularized by a smaller relative amount than other\u00a0weights.</em></p>\n<p><em>In contrast, de-coupled weight decay regularizes all weights with the same rate \u03bb, effectively regularizing weights x with large s more than standard L2 regularization does.</em>\u201d</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/987/1*tX-L5ZjBlGlJyadWtXt9rA.png\"><figcaption>Figure 1\u200a\u2014\u200aSource of image\u200a\u2014\u200aFrom original paper itself\u200a\u2014\u200a<a href=\"https://arxiv.org/abs/1711.05101\">Link</a></figcaption></figure><p>From the above image we can see AdamW (Adam with Weight decay) with cosine annealing learning rate scheduler gives the best performance and hence the authors suggest to use learning rate schedulers with adaptive gradient algorithms as\u00a0well.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/923/1*2lAAgTKmMxZRn36M0PAK-w.png\"><figcaption>Figure 2\u200a\u2014\u200aSource of image\u200a\u2014\u200aFrom original paper itself\u200a\u2014\u200a<a href=\"https://arxiv.org/abs/1711.05101\">Link</a></figcaption></figure><p>From Figure 2, Top left graph we can see that in SGD, L2 regularization is not decoupled from learning rate as the best hyperparameter basin is diagonal meaning initial learning rate and L2 regularization are interdependent on each other. This means, if we change only one of them we might get worse results. So to get best results we will have to change both simultaneously\u200a\u2014\u200ainitial learning rate and L2 regularization, giving large hyperparameter space.</p>\n<p>On the other hand (referring to Figure 2\u200a\u2014\u200agraph on top right), SGD with Weight decay or SGDW shows that initial learning rate and L2 regularization are decoupled. This shows that even if learning rate is not well tuned, having it fixed at some value and only changing weight decay factor would yield good value. This is also shown by the graph not diagonal.</p>\n<p>Coming to Adam with L2 regularization (Figure 2\u200a\u2014\u200abottom left graph), we see that it performs even worse than\u00a0SGD.</p>\n<p>Adam with weight decay or AdamW(Figure 2\u200a\u2014\u200abottom right graph) shows that it largely decouples learning rate and weight decay\u200a\u2014\u200akeep one parameter constant and try to optimize the other. Performance of this was better than SGD, Adam and\u00a0SGDW!</p>\n<p>Do refer the paper for more insights on experiments, performance and mathematical proofs!</p>\n<p>Thanks for reading the summary.<br>Connect with me on <a href=\"https://www.linkedin.com/in/sahil-chachra/\">LinkedIn</a>.\u00a0:)</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=1583cbc855bd\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["data-science","hyperparameter-tuning","optimization","deep-learning","ai"]},{"title":"Run YoloV5s with TensorRT and DeepStream on Nvidia Jetson Nano","pubDate":"2021-09-30 12:38:56","link":"https://sahilchachra.medium.com/run-yolov5s-with-tensorrt-and-deepstream-on-nvidia-jetson-nano-8c888a2f0eae?source=rss-f31bf6073414------2","guid":"https://medium.com/p/8c888a2f0eae","author":"Sahil Chachra","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*963XnKmJ7b62z13b0Q3FKA.jpeg","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*963XnKmJ7b62z13b0Q3FKA.jpeg\"></figure><p>This article will help you to run your YoloV5s model with TensorRT and DeepStream.</p>\n<p><em>Note\u00a0: This article has GitHub repo links which will help you to run your model on TensorRT and DeepStream. I will just guide you/walk you through the steps following which you will be able to run COCO pretrained model. For using custom trained model, the GitHub repos have the steps to\u00a0follow.</em></p>\n<h3>Getting Started\u2026</h3>\n<p>In my previous article (<a href=\"https://sahilchachra.medium.com/setting-up-nvidias-jetson-nano-from-jetpack-to-yolov5-60a004bf48bc\">link</a>), I focused on how to setup your Jetson nano and run inference on Yolov5s model. For this article, I used docker image from Hello AI course by Nvidia (<a href=\"https://www.youtube.com/watch?v=QXIwdsyK7Rw\">YouTube link</a>) and ran inference on YoloV5s with TensorRT optimization. Further on, I installed DeepStream on Nano and ran inference on YoloV5s with\u00a0it.</p>\n<p>Assuming you are using <a href=\"https://github.com/ultralytics/yolov5\">official repo</a> to train/run your YoloV5s model and the folder is in the home directory,</p>\n<ol><li>Run this command to know your JetPack/L4t version</li></ol>\n<pre>$sudo apt-cache show nvidia-jetpack</pre>\n<p>2. Clone <a href=\"https://github.com/dusty-nv/jetson-inference\">this </a>repo and pull the docker image from <a href=\"https://github.com/dusty-nv/jetson-inference/blob/master/docs/aux-docker.md\">here</a> as per your Jetpack version. This is official repo for Hello AI course by Nvidia. The docker has everything pre-installed\u200a\u2014\u200aPyTorch, TensorRT, etc. (Follow the initial steps in the repo on how to clone the repo and pull the docker container)</p>\n<pre>$git clone --recursive <a href=\"https://github.com/dusty-nv/jetson-inference\">https://github.com/dusty-nv/jetson-inference</a></pre>\n<p>3. Now move into jetson-inference folder (created by cloning the repo) and run the docker which you just downloaded</p>\n<pre>$cd jetson-inference<br>$docker/run.sh</pre>\n<blockquote>If you get an error like\u00a0: Error response from daemon: unauthorized: authentication required. See \u2018docker run\u200a\u2014\u200ahelp\u2019.</blockquote>\n<blockquote>Then run\u00a0:</blockquote>\n<pre>$docker/run.sh -c dustynv/jetson-inference:r32.x.x</pre>\n<blockquote>r32.x.x -&gt; put the version number of the docker container pulled</blockquote>\n<p>4. Check if you are able to use the docker container by typing python. After confirming, exit and come back to the home directory.</p>\n<p>5. After you have trained your model (or if you want to run inference on the COCO pretrained model), convert the model from\u00a0.pt to\u00a0.wts format and build TensorRT engine. Follow this repo\u200a\u2014\u200a<a href=\"https://github.com/wang-xinyu/tensorrtx/tree/master/yolov5\">https://github.com/wang-xinyu/tensorrtx/tree/master/yolov5</a></p>\n<p>6. Once you have followed the steps in the above repo (assuming you will have a folder named tensorrtx from above repo), to run the tensorrt engine, mount the folder while starting the\u00a0docker</p>\n<pre>$docker/run.sh -c dustynv/jetson-inference:r32.5.0 --volume ~/tensorrtx/:/tensorrtx/</pre>\n<p>Now, your docker container can access the tensorrtx folder stored in home directory.</p>\n<p>7. Now run this command to test your tensorrt\u00a0engine</p>\n<pre>cd /tensorrtx/yolo<br>python yolov5_trt.py</pre>\n<p>8. Now, install DeepStream SDK in your Nano from <a href=\"https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_Quickstart.html#jetson-setup\">here</a>(Nvidia\u2019s site). Exit from your docker. The docker container we used doesn\u2019t have DeepStream installed. To download DeepStream SDK use this <a href=\"https://developer.nvidia.com/deepstream-getting-started\">link</a>(Nvidia\u2019s site)</p>\n<p>9. After setting up DeepStream, to run your YoloV5s TensorRT engine with DeepStream, follow this\u00a0<a href=\"https://github.com/marcoslucianops/DeepStream-Yolo/blob/master/YOLOv5-5.0.md\">repo</a>.</p>\n<p>10. Assuming you are in home directory after setting up DeepStream, to run your YoloV5s tensorrt engine with DeepStream\u00a0:-</p>\n<pre>$cd /opt/nvidia/deepstream/deepstream-5.1/sources/yolo<br>$deepstream-app -c deepstream_app_config.txt</pre>\n<p><em>Thanks for reading my blog. Hope this would have helped you to run YoloV5s TensortRT engine with DeepStream. If you find any issues or any better resource, do mention it in the comments.</em></p>\n<p><em>Thanks\u00a0:).</em></p>\n<p><em>Do connect with me on </em><a href=\"https://www.linkedin.com/in/sahil-chachra/\"><em>LinkedIn\u00a0</em></a><em>:)</em></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8c888a2f0eae\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*963XnKmJ7b62z13b0Q3FKA.jpeg\"></figure><p>This article will help you to run your YoloV5s model with TensorRT and DeepStream.</p>\n<p><em>Note\u00a0: This article has GitHub repo links which will help you to run your model on TensorRT and DeepStream. I will just guide you/walk you through the steps following which you will be able to run COCO pretrained model. For using custom trained model, the GitHub repos have the steps to\u00a0follow.</em></p>\n<h3>Getting Started\u2026</h3>\n<p>In my previous article (<a href=\"https://sahilchachra.medium.com/setting-up-nvidias-jetson-nano-from-jetpack-to-yolov5-60a004bf48bc\">link</a>), I focused on how to setup your Jetson nano and run inference on Yolov5s model. For this article, I used docker image from Hello AI course by Nvidia (<a href=\"https://www.youtube.com/watch?v=QXIwdsyK7Rw\">YouTube link</a>) and ran inference on YoloV5s with TensorRT optimization. Further on, I installed DeepStream on Nano and ran inference on YoloV5s with\u00a0it.</p>\n<p>Assuming you are using <a href=\"https://github.com/ultralytics/yolov5\">official repo</a> to train/run your YoloV5s model and the folder is in the home directory,</p>\n<ol><li>Run this command to know your JetPack/L4t version</li></ol>\n<pre>$sudo apt-cache show nvidia-jetpack</pre>\n<p>2. Clone <a href=\"https://github.com/dusty-nv/jetson-inference\">this </a>repo and pull the docker image from <a href=\"https://github.com/dusty-nv/jetson-inference/blob/master/docs/aux-docker.md\">here</a> as per your Jetpack version. This is official repo for Hello AI course by Nvidia. The docker has everything pre-installed\u200a\u2014\u200aPyTorch, TensorRT, etc. (Follow the initial steps in the repo on how to clone the repo and pull the docker container)</p>\n<pre>$git clone --recursive <a href=\"https://github.com/dusty-nv/jetson-inference\">https://github.com/dusty-nv/jetson-inference</a></pre>\n<p>3. Now move into jetson-inference folder (created by cloning the repo) and run the docker which you just downloaded</p>\n<pre>$cd jetson-inference<br>$docker/run.sh</pre>\n<blockquote>If you get an error like\u00a0: Error response from daemon: unauthorized: authentication required. See \u2018docker run\u200a\u2014\u200ahelp\u2019.</blockquote>\n<blockquote>Then run\u00a0:</blockquote>\n<pre>$docker/run.sh -c dustynv/jetson-inference:r32.x.x</pre>\n<blockquote>r32.x.x -&gt; put the version number of the docker container pulled</blockquote>\n<p>4. Check if you are able to use the docker container by typing python. After confirming, exit and come back to the home directory.</p>\n<p>5. After you have trained your model (or if you want to run inference on the COCO pretrained model), convert the model from\u00a0.pt to\u00a0.wts format and build TensorRT engine. Follow this repo\u200a\u2014\u200a<a href=\"https://github.com/wang-xinyu/tensorrtx/tree/master/yolov5\">https://github.com/wang-xinyu/tensorrtx/tree/master/yolov5</a></p>\n<p>6. Once you have followed the steps in the above repo (assuming you will have a folder named tensorrtx from above repo), to run the tensorrt engine, mount the folder while starting the\u00a0docker</p>\n<pre>$docker/run.sh -c dustynv/jetson-inference:r32.5.0 --volume ~/tensorrtx/:/tensorrtx/</pre>\n<p>Now, your docker container can access the tensorrtx folder stored in home directory.</p>\n<p>7. Now run this command to test your tensorrt\u00a0engine</p>\n<pre>cd /tensorrtx/yolo<br>python yolov5_trt.py</pre>\n<p>8. Now, install DeepStream SDK in your Nano from <a href=\"https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_Quickstart.html#jetson-setup\">here</a>(Nvidia\u2019s site). Exit from your docker. The docker container we used doesn\u2019t have DeepStream installed. To download DeepStream SDK use this <a href=\"https://developer.nvidia.com/deepstream-getting-started\">link</a>(Nvidia\u2019s site)</p>\n<p>9. After setting up DeepStream, to run your YoloV5s TensorRT engine with DeepStream, follow this\u00a0<a href=\"https://github.com/marcoslucianops/DeepStream-Yolo/blob/master/YOLOv5-5.0.md\">repo</a>.</p>\n<p>10. Assuming you are in home directory after setting up DeepStream, to run your YoloV5s tensorrt engine with DeepStream\u00a0:-</p>\n<pre>$cd /opt/nvidia/deepstream/deepstream-5.1/sources/yolo<br>$deepstream-app -c deepstream_app_config.txt</pre>\n<p><em>Thanks for reading my blog. Hope this would have helped you to run YoloV5s TensortRT engine with DeepStream. If you find any issues or any better resource, do mention it in the comments.</em></p>\n<p><em>Thanks\u00a0:).</em></p>\n<p><em>Do connect with me on </em><a href=\"https://www.linkedin.com/in/sahil-chachra/\"><em>LinkedIn\u00a0</em></a><em>:)</em></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8c888a2f0eae\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["deepstream","jetson-nano","tensorrt","ai","deep-learning"]},{"title":"Setting up Nvidia\u2019s Jetson Nano\u00a0: From Jetpack to YoloV5","pubDate":"2021-08-21 13:29:58","link":"https://sahilchachra.medium.com/setting-up-nvidias-jetson-nano-from-jetpack-to-yolov5-60a004bf48bc?source=rss-f31bf6073414------2","guid":"https://medium.com/p/60a004bf48bc","author":"Sahil Chachra","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*Rn1XfeaR13cbkpa-snT7lw.jpeg","description":"\n<h3>Setting up Nvidia\u2019s Jetson Nano\u00a0: From Jetpack to\u00a0YoloV5</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Rn1XfeaR13cbkpa-snT7lw.jpeg\"></figure><p>In this blog, I will take you through several resources to step up your Jetson such as to install remote desktop, Pytorch, Tensorflow, etc. Also, I will talk about how to install other required libraries to run your deep learning tasks some\u00a0tips.</p>\n<p><em>Note\u00a0: All the steps/procedures discussed/shared were used/tested by me on Nvidia Jetson Nano(B01, 4GB variant). The following packages were installed/set up on Jetpack 4.5 (R32.5.0). While writing this blog, Jetpack 4.6 has been released.</em></p>\n<h3>A brief on my Jetson\u2019s\u00a0setup</h3>\n<p>I am using Jetson Nano(B01) variant which has 4GB ram. I use 4A power adapter to power it. I have also installed a small fan on the heatsink to maintain the temperature(don\u2019t know yet how effective it is). In terms of storage, I am using 64GB SD card from SanDisk. To work on jetson, I actually plug in a monitor, mouse and keyboard and use it like a separate machine for now. I tried using it via SSH, but then while learning you cannot entirely work through terminal/cmd. To access internet, I am using a ethernet cable which connects the jetson directly to my router. To capture live feed, I am using Logitech C270(HD)\u00a0webcam.</p>\n<p>To access internet, why am I not using Intel\u2019s wifi/bluetooth adapter or USB wifi receiver? I am not using Intel\u2019s adapter because right now I don\u2019t see a need to use jetson wirelessly. I am using it with my monitor and other components plugged in. And the reason to not use USB wifi receiver is because each jetpack is based on some linux kernel version and those receivers work with few specific linux kernel\u2019s versions only. So considering it as not so future proof, I did not consider using\u00a0it.</p>\n<h3>Initial setup</h3>\n<ol>\n<li>Head over to Nvidia\u2019s official site to download Jetpack and follow the instructions to install it. You will need to download SD card formatter tool and jetpack image flashing tool. Everything will be mentioned in the steps. (Link for Jetpack 4.5 and steps\u200a\u2014\u200a<a href=\"https://developer.nvidia.com/jetpack-sdk-45-archive\">https://developer.nvidia.com/jetpack-sdk-45-archive</a>)</li>\n<li>After flashing jetpack to SD card, plug it in and also attach monitor, keyboard and\u00a0mouse.</li>\n<li>After booting it, increase the swap size to 4GB. ( If you don\u2019t see the steps in the site then refer this video\u200a\u2014\u200a<a href=\"https://www.youtube.com/watch?v=uvU8AXY1170\">https://www.youtube.com/watch?v=uvU8AXY1170</a>)</li>\n<li>Now connect your jetson to internet via ethernet cable or Intel\u2019s wifi/bluetooth module for\u00a0jetson.</li>\n<li>Now run\u00a0\u2014</li>\n</ol>\n<pre>sudo apt-get update<br>sudo apt-get upgrade</pre>\n<p>6. After this install nano text editor\u00a0\u2014</p>\n<pre>sudo apt install nano</pre>\n<p>7. If you want to access jetson remotely with UI, setup VNC server. Link\u200a\u2014\u200a<a href=\"https://medium.com/@bharathsudharsan023/jetson-nano-remote-vnc-access-d1e71c82492b\">https://medium.com/@bharathsudharsan023/jetson-nano-remote-vnc-access-d1e71c82492b</a>\u00a0. If you are on Windows, then follow the steps till installing and setting up VNC server. Then to access jetson via VNC, install VNC viewer. Open VNC viewer, go to File -&gt; New connection -&gt; in VNC server enter your jetson\u2019s IP address and in name field give any name you\u00a0want.</p>\n<p>8. If you want to access jetson remotely without UI, go to cmd/terminal and type -&gt; ssh your_jetson_username@jetson_IP_address. It will prompt for password. (You can also connect jetson to your system using mirco-usb if you don\u2019t have access to internet. You connect by going to terminal and typing -&gt; ssh your_jetson_username@192.168.0.55\u00a0. This should be the IP most probably)</p>\n<p>Congratulations, your Jetson Nano is now\u00a0ready!</p>\n<h3>Should I follow Jetson AI Fundamentals course\u00a0now?</h3>\n<p>I spent few days with the course. The course helped me to setup my jetson and get some hands on. Do spend some time with this course and get familiar to jetson\u2019s speed and UI. This course is very beginner friendly. They use NGC container to run a simple project which has required libraries pre-installed (but those libraries are accessible inside the container only). This course will teach you how to use jetson with headless mode(without monitor, mouse and keyboard).</p>\n<h3>Installing required libraries</h3>\n<p>Remember to use pip3 while installing these libraries(you have python 2 and python 3 both installed). Anaconda is not supported by Jetson as per today. If you want to create virtual environments you can use python\u2019s venv or Archiconda.</p>\n<ol><li>Install numpy version 1.19.4 only. Any version above this can cause problems while using Pytorch or Tensorflow (Faced issues myself\u200a\u2014\u200atensorflow threw core dumped error). To install numpy 1.19.4. Also install opencv\u00a0:-</li></ol>\n<pre>pip3 install numpy==1.19.4<br>pip3 install opencv-python</pre>\n<p>2. Download few dependencies before installing Pytorch:-</p>\n<pre>sudo apt-get install python3-pip libjpeg-dev libopenblas-dev libopenmpi-dev libomp-dev<br>sudo apt install libfreetype6-dev python3-dev zlib1g-dev</pre>\n<p>3. Head over to <a href=\"https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-9-0-now-available/72048\">https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-9-0-now-available/72048</a> or search \u201cdownload pytorch jetson\u201d and open Nvidia\u2019s website. Now, click on arrow and download Pytorch 1.8. It will download a\u00a0.whl file. Install it\u00a0using:-</p>\n<pre>pip3 install path_to_pytorch18.whl</pre>\n<p>4. Pytorch also requires torchvision library. To install it, (in the below command, vision and torchvision are separated by space\u200a\u2014\u200acopy the entire clone command till the word torchvision and paste it in the terminal)</p>\n<pre>git clone \u2014 branch v0.9.1 <a href=\"https://github.com/pytorch/vision\">https://github.com/pytorch/vision</a> torchvision<br>cd torchvision/<br>sudo python3 setup.py install</pre>\n<p>5. Now come out of the torchvision folder by using command <strong>cd\u00a0..</strong> and verify installation of Pytorch and torchvision by importing them. If you see this error \u201cCannot find Python.h\u201d while installing torchvision, kindly reinstall python3-dev package.</p>\n<p>6. Now install Tensorflow 2.4.1 by following this article\u200a\u2014\u200a<a href=\"https://qengineering.eu/install-tensorflow-2.4.0-on-jetson-nano.html\">https://qengineering.eu/install-tensorflow-2.4.0-on-jetson-nano.html</a>\u00a0. <em>If gdown command(used to download tf2.4.1.whl file) doesn\u2019t work, then simply copy the drive link from the steps and download the file yourself.</em></p>\n<p>7. Verify the installation of tensorflow by importing it and printing the\u00a0version.</p>\n<h3>Running Yolov5 on your\u00a0Jetson</h3>\n<ol><li>Clone the repo and move inside the cloned\u00a0folder</li></ol>\n<pre>git clone <a href=\"https://github.com/ultralytics/yolov5.git\">https://github.com/ultralytics/yolov5.git</a><br>cd yolov5<br>export OPENBLAS_CORETYPE=ARMV8</pre>\n<p>2. Now type nano requirements.txt in terminal and comment out these packages as these are already installed -&gt; numpy, tensorflow, torch, torchvision and\u00a0opencv</p>\n<p>3. Now install the remaining libraries using the following command. This process may take time as many of these libraries are downloaded and built locally on the\u00a0jetson.</p>\n<pre>pip3 install -r requirements.txt</pre>\n<p>4. After installing, download Yolov5s model from the repository, place it in the yolov5 directory and run the following. If the following commands throws error, kindly look for missing dependencies.</p>\n<pre>python3 detect.py --weights yolov5s.pt --source 0 --device 0</pre>\n<p>Source 0 means you are using Webcam. (change the source to path of image or video if you don\u2019t want to use live feed). Device 0 means that the model should be loaded in\u00a0GPU.</p>\n<p>5. Do checkout detect.py for more options while running inference on YoloV5s model (Yolov5s is a light weight\u00a0model)</p>\n<h3>Few Tips</h3>\n<ol>\n<li>Keep System monitor application on and always keep an eye on it. Jetson tends to lag when you have several tabs open on Chromium and parallelly you are running some inference.</li>\n<li>Restart the jetson if you see high ram usage even with no application open.</li>\n<li>If you are new to jetson, I highly recommend to use it with display, mouse and keyboard rather than via\u00a0SSH.</li>\n<li>You may use VNC but at times it is very slow. You may use SSH if you know you won\u2019t require UI for any\u00a0task.</li>\n<li>Try to use jetson in a cool place. High temperature may damage it and also the SD card as it just sits below the heatsink.</li>\n<li>Try to use NGC containers as they provide many options, such as\u200a\u2014\u200aproviding entire DL libraries installed in a container. Just pull and run! (I couldn\u2019t use it as the files/libraries I downloaded after pulling it were all gone when I restarted the\u00a0jetson).</li>\n<li>Every time you run yolov5\u2019s detect.py, the output is saved. Remember to delete the output if not needed else it will eat up\u00a0space.</li>\n</ol>\n<h3>Conclusion</h3>\n<p>Hope these tips and steps might have helped you in setting up your jetson nano. The main goal of this blog was to guide you through the right steps to install these libraries instead of you keep on searching for these. Also, if you come across any community where people discuss/work on jetson, do let me know in the comments section of this blog. I would love to join to learn and contribute! Thanks for reading my blog.\u00a0:-)</p>\n<p>Connect with me on <a href=\"https://www.linkedin.com/in/sahil-chachra/\">LinkedIn</a>.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=60a004bf48bc\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>Setting up Nvidia\u2019s Jetson Nano\u00a0: From Jetpack to\u00a0YoloV5</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Rn1XfeaR13cbkpa-snT7lw.jpeg\"></figure><p>In this blog, I will take you through several resources to step up your Jetson such as to install remote desktop, Pytorch, Tensorflow, etc. Also, I will talk about how to install other required libraries to run your deep learning tasks some\u00a0tips.</p>\n<p><em>Note\u00a0: All the steps/procedures discussed/shared were used/tested by me on Nvidia Jetson Nano(B01, 4GB variant). The following packages were installed/set up on Jetpack 4.5 (R32.5.0). While writing this blog, Jetpack 4.6 has been released.</em></p>\n<h3>A brief on my Jetson\u2019s\u00a0setup</h3>\n<p>I am using Jetson Nano(B01) variant which has 4GB ram. I use 4A power adapter to power it. I have also installed a small fan on the heatsink to maintain the temperature(don\u2019t know yet how effective it is). In terms of storage, I am using 64GB SD card from SanDisk. To work on jetson, I actually plug in a monitor, mouse and keyboard and use it like a separate machine for now. I tried using it via SSH, but then while learning you cannot entirely work through terminal/cmd. To access internet, I am using a ethernet cable which connects the jetson directly to my router. To capture live feed, I am using Logitech C270(HD)\u00a0webcam.</p>\n<p>To access internet, why am I not using Intel\u2019s wifi/bluetooth adapter or USB wifi receiver? I am not using Intel\u2019s adapter because right now I don\u2019t see a need to use jetson wirelessly. I am using it with my monitor and other components plugged in. And the reason to not use USB wifi receiver is because each jetpack is based on some linux kernel version and those receivers work with few specific linux kernel\u2019s versions only. So considering it as not so future proof, I did not consider using\u00a0it.</p>\n<h3>Initial setup</h3>\n<ol>\n<li>Head over to Nvidia\u2019s official site to download Jetpack and follow the instructions to install it. You will need to download SD card formatter tool and jetpack image flashing tool. Everything will be mentioned in the steps. (Link for Jetpack 4.5 and steps\u200a\u2014\u200a<a href=\"https://developer.nvidia.com/jetpack-sdk-45-archive\">https://developer.nvidia.com/jetpack-sdk-45-archive</a>)</li>\n<li>After flashing jetpack to SD card, plug it in and also attach monitor, keyboard and\u00a0mouse.</li>\n<li>After booting it, increase the swap size to 4GB. ( If you don\u2019t see the steps in the site then refer this video\u200a\u2014\u200a<a href=\"https://www.youtube.com/watch?v=uvU8AXY1170\">https://www.youtube.com/watch?v=uvU8AXY1170</a>)</li>\n<li>Now connect your jetson to internet via ethernet cable or Intel\u2019s wifi/bluetooth module for\u00a0jetson.</li>\n<li>Now run\u00a0\u2014</li>\n</ol>\n<pre>sudo apt-get update<br>sudo apt-get upgrade</pre>\n<p>6. After this install nano text editor\u00a0\u2014</p>\n<pre>sudo apt install nano</pre>\n<p>7. If you want to access jetson remotely with UI, setup VNC server. Link\u200a\u2014\u200a<a href=\"https://medium.com/@bharathsudharsan023/jetson-nano-remote-vnc-access-d1e71c82492b\">https://medium.com/@bharathsudharsan023/jetson-nano-remote-vnc-access-d1e71c82492b</a>\u00a0. If you are on Windows, then follow the steps till installing and setting up VNC server. Then to access jetson via VNC, install VNC viewer. Open VNC viewer, go to File -&gt; New connection -&gt; in VNC server enter your jetson\u2019s IP address and in name field give any name you\u00a0want.</p>\n<p>8. If you want to access jetson remotely without UI, go to cmd/terminal and type -&gt; ssh your_jetson_username@jetson_IP_address. It will prompt for password. (You can also connect jetson to your system using mirco-usb if you don\u2019t have access to internet. You connect by going to terminal and typing -&gt; ssh your_jetson_username@192.168.0.55\u00a0. This should be the IP most probably)</p>\n<p>Congratulations, your Jetson Nano is now\u00a0ready!</p>\n<h3>Should I follow Jetson AI Fundamentals course\u00a0now?</h3>\n<p>I spent few days with the course. The course helped me to setup my jetson and get some hands on. Do spend some time with this course and get familiar to jetson\u2019s speed and UI. This course is very beginner friendly. They use NGC container to run a simple project which has required libraries pre-installed (but those libraries are accessible inside the container only). This course will teach you how to use jetson with headless mode(without monitor, mouse and keyboard).</p>\n<h3>Installing required libraries</h3>\n<p>Remember to use pip3 while installing these libraries(you have python 2 and python 3 both installed). Anaconda is not supported by Jetson as per today. If you want to create virtual environments you can use python\u2019s venv or Archiconda.</p>\n<ol><li>Install numpy version 1.19.4 only. Any version above this can cause problems while using Pytorch or Tensorflow (Faced issues myself\u200a\u2014\u200atensorflow threw core dumped error). To install numpy 1.19.4. Also install opencv\u00a0:-</li></ol>\n<pre>pip3 install numpy==1.19.4<br>pip3 install opencv-python</pre>\n<p>2. Download few dependencies before installing Pytorch:-</p>\n<pre>sudo apt-get install python3-pip libjpeg-dev libopenblas-dev libopenmpi-dev libomp-dev<br>sudo apt install libfreetype6-dev python3-dev zlib1g-dev</pre>\n<p>3. Head over to <a href=\"https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-9-0-now-available/72048\">https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-9-0-now-available/72048</a> or search \u201cdownload pytorch jetson\u201d and open Nvidia\u2019s website. Now, click on arrow and download Pytorch 1.8. It will download a\u00a0.whl file. Install it\u00a0using:-</p>\n<pre>pip3 install path_to_pytorch18.whl</pre>\n<p>4. Pytorch also requires torchvision library. To install it, (in the below command, vision and torchvision are separated by space\u200a\u2014\u200acopy the entire clone command till the word torchvision and paste it in the terminal)</p>\n<pre>git clone \u2014 branch v0.9.1 <a href=\"https://github.com/pytorch/vision\">https://github.com/pytorch/vision</a> torchvision<br>cd torchvision/<br>sudo python3 setup.py install</pre>\n<p>5. Now come out of the torchvision folder by using command <strong>cd\u00a0..</strong> and verify installation of Pytorch and torchvision by importing them. If you see this error \u201cCannot find Python.h\u201d while installing torchvision, kindly reinstall python3-dev package.</p>\n<p>6. Now install Tensorflow 2.4.1 by following this article\u200a\u2014\u200a<a href=\"https://qengineering.eu/install-tensorflow-2.4.0-on-jetson-nano.html\">https://qengineering.eu/install-tensorflow-2.4.0-on-jetson-nano.html</a>\u00a0. <em>If gdown command(used to download tf2.4.1.whl file) doesn\u2019t work, then simply copy the drive link from the steps and download the file yourself.</em></p>\n<p>7. Verify the installation of tensorflow by importing it and printing the\u00a0version.</p>\n<h3>Running Yolov5 on your\u00a0Jetson</h3>\n<ol><li>Clone the repo and move inside the cloned\u00a0folder</li></ol>\n<pre>git clone <a href=\"https://github.com/ultralytics/yolov5.git\">https://github.com/ultralytics/yolov5.git</a><br>cd yolov5<br>export OPENBLAS_CORETYPE=ARMV8</pre>\n<p>2. Now type nano requirements.txt in terminal and comment out these packages as these are already installed -&gt; numpy, tensorflow, torch, torchvision and\u00a0opencv</p>\n<p>3. Now install the remaining libraries using the following command. This process may take time as many of these libraries are downloaded and built locally on the\u00a0jetson.</p>\n<pre>pip3 install -r requirements.txt</pre>\n<p>4. After installing, download Yolov5s model from the repository, place it in the yolov5 directory and run the following. If the following commands throws error, kindly look for missing dependencies.</p>\n<pre>python3 detect.py --weights yolov5s.pt --source 0 --device 0</pre>\n<p>Source 0 means you are using Webcam. (change the source to path of image or video if you don\u2019t want to use live feed). Device 0 means that the model should be loaded in\u00a0GPU.</p>\n<p>5. Do checkout detect.py for more options while running inference on YoloV5s model (Yolov5s is a light weight\u00a0model)</p>\n<h3>Few Tips</h3>\n<ol>\n<li>Keep System monitor application on and always keep an eye on it. Jetson tends to lag when you have several tabs open on Chromium and parallelly you are running some inference.</li>\n<li>Restart the jetson if you see high ram usage even with no application open.</li>\n<li>If you are new to jetson, I highly recommend to use it with display, mouse and keyboard rather than via\u00a0SSH.</li>\n<li>You may use VNC but at times it is very slow. You may use SSH if you know you won\u2019t require UI for any\u00a0task.</li>\n<li>Try to use jetson in a cool place. High temperature may damage it and also the SD card as it just sits below the heatsink.</li>\n<li>Try to use NGC containers as they provide many options, such as\u200a\u2014\u200aproviding entire DL libraries installed in a container. Just pull and run! (I couldn\u2019t use it as the files/libraries I downloaded after pulling it were all gone when I restarted the\u00a0jetson).</li>\n<li>Every time you run yolov5\u2019s detect.py, the output is saved. Remember to delete the output if not needed else it will eat up\u00a0space.</li>\n</ol>\n<h3>Conclusion</h3>\n<p>Hope these tips and steps might have helped you in setting up your jetson nano. The main goal of this blog was to guide you through the right steps to install these libraries instead of you keep on searching for these. Also, if you come across any community where people discuss/work on jetson, do let me know in the comments section of this blog. I would love to join to learn and contribute! Thanks for reading my blog.\u00a0:-)</p>\n<p>Connect with me on <a href=\"https://www.linkedin.com/in/sahil-chachra/\">LinkedIn</a>.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=60a004bf48bc\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["deep-learning","yolov5","inference","jetson-nano","jetsons"]},{"title":"Comparative study\u200a\u2014\u200aUsing EfficientNetB0 to EfficientNetB7 as encoder in UNet.","pubDate":"2021-07-20 16:57:05","link":"https://sahilchachra.medium.com/comparative-study-using-efficientnetb0-to-efficientnetb7-as-encoder-in-unet-a73ec6aeffe8?source=rss-f31bf6073414------2","guid":"https://medium.com/p/a73ec6aeffe8","author":"Sahil Chachra","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*KGxIa2tx_9iEo9JzHqnU_w.png","description":"\n<h3>Comparative study\u200a\u2014\u200aUsing EfficientNetB0 to EfficientNetB7 as encoder in UNet. (Comparison of 8 architectures)</h3>\n<p><em>In this blog I will compare performance of EfficinetNetB0 to EfficientNetB7 (all 8 architectures) as an encoder of UNet architecture. I will discuss about the data used, performance metrices and the final\u00a0result.</em></p>\n<p>Almost a month ago, I had published a blog on performance of Vanilla(basic) UNet architecture and EfficientNetB0 as encoder in UNet. Link to the blog is\u00a0<a href=\"https://sahilchachra.medium.com/vanilla-unet-vs-unet-with-efficientnetb0-as-encoder-55495edd2ceb\">here</a>.</p>\n<h3>Dataset</h3>\n<p>The dataset I used for this experiment was Carla Semantic Segmentation Dataset (1000 images). This dataset has been built using a simulator meaning it\u2019s a Synthetic dataset. This dataset belongs to group of Autonomous vehicles datasets. The number of images used for training was 800 and 200 were used for validation.</p>\n<h3>Training</h3>\n<p>Each of these models were set to train for 100 epochs with EarlyStopping enabled. So, each model\u2019s training stopped somewhere between 30 to 50 epochs. The parameter used to monitor training was validation loss.</p>\n<p>Batch size and learning rate was decided by running several experiments on one architecture and then was same kept constant for the entire experiment (that is, batch size and learning rate was same for each model that was being trained). Now batch size and learning rate can be different for each architecture, but to maintain uniformity for the experiment, each model was trained using same hyperparameters.</p>\n<p>You may now think, why the number of epochs were not same? So, by keeping the number of epochs same, we wouldn\u2019t know which model converges faster, whether lesser number of parameters means faster convergence(less epochs needed for training). So, I allowed each model to take it\u2019s own time to converge, giving better insights about the training.</p>\n<h3>EfficientNet architectures as Encoder in\u00a0UNet</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*KGxIa2tx_9iEo9JzHqnU_w.png\"><figcaption>Figure 1\u00a0: Source of image\u200a\u2014\u200a<a href=\"https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html\">link</a></figcaption></figure><p>In the above image, we can see comparison of different architectures w.r.t the number of parameters and Top 1 accuracy on Imagenet dataset. We see EfficientNetB0 having very less number of parameters and still having better accuracy than ResNet 50 which has significant amount of parameters. Which shows how promising the EfficientNet family is! Lesser parameters means faster training and lighter\u00a0model.</p>\n<p>All these above numbers are when we are using these models for normal classification problems as in when we directly use an EfficientNet architecture. What about the number of parameters when we use it as an encoder in\u00a0UNet?</p>\n<p>Keeping the encoder part frozen (encoder.trainable = False in Keras), we have the following number of parameters.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/976/1*cN5WR9gZcEnx0J1Tlx1m0w.png\"><figcaption>Figure 2\u00a0: Parameters in each\u00a0model</figcaption></figure><p>Coming to training details of each architecture (the following table shows metrices and epoch number having least validation loss)</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*I8VwKY_jou6iEQgtFAjWVw.png\"><figcaption>Figure 3\u00a0: Performance of each\u00a0model</figcaption></figure><p>The above image shows\u200a\u2014\u200afor each model, in which epoch least validation loss was achieved, total epochs the model was trained for and other relevant\u00a0metrics.</p>\n<h3>What do we understand from above tables (Figure 2 and Figure\u00a03)?</h3>\n<ol>\n<li>From figure 3, we see that from <strong>UNet_EfficientNetB0 to UNet_EfficientNetB6</strong>, <strong>validation MeanIOU remains same </strong><em>(except for UNet_EfficientNetB1 and UNet_EfficientNetB2)</em><strong>, difference between validation loss </strong>is just<strong> 0.01 or 0.02 </strong>and<strong> validation accuracy is also almost same <em>(difference of 0.01). </em></strong><em>The time for each epoch on average for these 6 architectures were 1 minute to 1.5\u00a0minute.</em>\n</li>\n<li>\n<em>Referring to figure 2, from </em><strong>UNet_EfficientNetB0 to UNet_EfficientNetB6</strong> (<em>except UNet_EfficientNetB2</em>), the <strong>number of trainable parameters are almost same</strong>. <strong>UNet_EfficientNetB2</strong> has <strong>highest number of trainable parameters</strong> (even more than UNet_EfficientNetB7, which is the longest architecture compared to other 6) and its <strong>overall</strong> <strong>performance is worst</strong> compared to other\u00a0models.</li>\n<li>Coming to<strong> UNet_EfficientNetB7</strong>, we see <strong>significant increase in Validation MeanIOU</strong> and <strong>least validation loss</strong> amongst all. Even I was surprise to see this. If you refer figure 2, UNet_EfficientNetB7 has highest number of parameters, approximately 2 times of that in UNet_EfficientNetB0. UNet_EfficientNetB7 took about 2.5\u20133 mins for each\u00a0epoch.</li>\n<li>So if you want a light model which trains very fast and performs significantly good, you can sure go with UNet_EfficientNetB0. From above tables, you can see that from UNet_EfficientNetB0 to UNet_EfficientNetB6 the performance metrices are same so why to waste resources?</li>\n<li>If training time and size of model is not a factor, then definitely go with UNet_EfficientNetB7. There is significant jump in performance as well as\u00a0output.</li>\n</ol>\n<h3>Performance Graphs and\u00a0Output</h3>\n<p><strong>Note\u00a0: The images are kept large to avoid\u00a0blur</strong></p>\n<h4>1. UNet_EfficientNetB0</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Or-5SGGrI20KAq1Kb5EXgg.png\"><figcaption><strong>UNet_EfficientNetB0\u200a\u2014\u200aEpochs vs Validation Loss</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*cIghkYkvszRFfDmD8pJ52A.png\"><figcaption><strong>UNet_EfficientNetB0\u200a\u2014\u200aEpochs vs Validation MeanIOU</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*1i7OxRmsBk4S4GIR3DVrAg.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*Twt--mCgbpIZIGOtSMoOqw.png\"><figcaption><strong>UNet_EfficientNetB0 Output</strong></figcaption></figure><h4>2. UNet_EfficientNetB1</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*QaqZywuhnce3J5oTjqs-fA.png\"><figcaption><strong>UNet_EfficientNetB1\u200a\u2014\u200aEpochs vs Validation Loss</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*L9D-_tlbjE7SxoQLCpazhw.png\"><figcaption><strong>UNet_EfficientNetB1\u200a\u2014\u200aEpochs vs Validation MeanIOU</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*tw078CP45oRroZTnk0MfFw.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*j0gnfqX3-jA9ji300CLdxQ.png\"><figcaption><strong>UNet_EfficientNetB1 Output</strong></figcaption></figure><h4>3. UNet_EfficientNetB2</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*O99lp1ZxyaawoL4L0TEx5g.png\"><figcaption><strong>UNet_EfficientNetB2\u200a\u2014\u200aEpochs vs Validation Loss</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*zYuWUgUNHh6EpC8clBVsHg.png\"><figcaption><strong>UNet_EfficientNetB2\u200a\u2014\u200aEpochs vs Validation MeanIOU</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*LoPLJ_7m1KTTh-g6rcFQzA.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*cI5LeuWmt4I2koFaMlwgoQ.png\"><figcaption><strong>UNet_EfficientNetB2 Output</strong></figcaption></figure><h4>4. UNet_EfficientNetB3</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*pwVl0_hNYuficM3a1MUykw.png\"><figcaption><strong>UNet_EfficientNetB3\u200a\u2014\u200aEpochs vs Validation Loss</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*uuBcnLP6jKDL_y2R4MzEmA.png\"><figcaption><strong>UNet_EfficientNetB3\u2014 Epochs vs Validation MeanIOU</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*7oUvWqxe7LYVDwWF2weNWg.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*6GBzP_Q5NOWJHdVJ1_w9ww.png\"><figcaption><strong>UNet_EfficientNetB3 Output</strong></figcaption></figure><h4>5. UNet_EfficientNetB4</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*EDVTMBIvWcys7e3y6JzgGg.png\"><figcaption><strong>UNet_EfficientNetB4\u200a\u2014\u200aEpochs vs Validation Loss</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*kcs4p6t2wMtR-4c1aSuZJA.png\"><figcaption><strong>UNet_EfficientNetB4\u200a\u2014\u200aEpochs vs Validation MeanIOU</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*oF62TEZosudCeFwAe-YdXA.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*AxZXoWvmgnUE0mprrNS61w.png\"><figcaption><strong>UNet_EfficientNetB4 Output</strong></figcaption></figure><h4>6. UNet_EfficientNetB5</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*mj_Tm4T6r5tTAKKD23kjRg.png\"><figcaption><strong>UNet_EfficientNetB5\u200a\u2014\u200aEpochs vs Validation Loss</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*_BfKDhOLmmTLPFJcoztiEQ.png\"><figcaption><strong>UNet_EfficientNetB5\u2014 Epochs vs Validation MeanIOU</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*zr6l4V_0ml2KmEFa8pgMrw.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*b5_wrtLsDWEAnTI08xS1jQ.png\"><figcaption><strong>UNet_EfficientNetB5 Output</strong></figcaption></figure><h4>7. UNet_EfficientNetB6</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*gIkRSx86RH5Sz0n6b1zOjw.png\"><figcaption><strong>UNet_EfficientNetB6\u200a\u2014\u200aEpochs vs Validation Loss</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*GTA9cTC6sLadsmHiPY182w.png\"><figcaption><strong>UNet_EfficientNetB6\u200a\u2014\u200aEpochs vs Validation MeanIOU</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*8DDpyFRUog4B9CSVTpLRPg.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*5YnWnbOk4cyo584HhXJz4w.png\"><figcaption><strong>UNet_EfficientNetB6 Output</strong></figcaption></figure><h4>8. UNet_EfficientNetB7</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*7f9oWkLtaOZHdkMkdwiH6g.png\"><figcaption><strong>UNet_EfficientNetB7\u2014 Epochs vs Validation Loss</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*rOxSx5bnzbyzTREZQXUtcA.png\"><figcaption><strong>UNet_EfficientNetB7\u200a\u2014\u200aEpochs vs Validation MeanIOU</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*NTwaJ8cdK-lcQZGptnVAwA.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*eRqZjGzTMGrNBUu_l_29lQ.png\"><figcaption><strong>UNet_EfficientNetB7 Output</strong></figcaption></figure><h3>Conclusion</h3>\n<p>The choice model is also not only based on performance metrices but also on the actual output of the model. In production what matters the most is output and the inference time of a\u00a0model.</p>\n<p>Thankyou\u00a0:).<br>Connect with me on <a href=\"https://www.linkedin.com/in/sahil-chachra/\">LinkedIn</a>.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a73ec6aeffe8\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>Comparative study\u200a\u2014\u200aUsing EfficientNetB0 to EfficientNetB7 as encoder in UNet. (Comparison of 8 architectures)</h3>\n<p><em>In this blog I will compare performance of EfficinetNetB0 to EfficientNetB7 (all 8 architectures) as an encoder of UNet architecture. I will discuss about the data used, performance metrices and the final\u00a0result.</em></p>\n<p>Almost a month ago, I had published a blog on performance of Vanilla(basic) UNet architecture and EfficientNetB0 as encoder in UNet. Link to the blog is\u00a0<a href=\"https://sahilchachra.medium.com/vanilla-unet-vs-unet-with-efficientnetb0-as-encoder-55495edd2ceb\">here</a>.</p>\n<h3>Dataset</h3>\n<p>The dataset I used for this experiment was Carla Semantic Segmentation Dataset (1000 images). This dataset has been built using a simulator meaning it\u2019s a Synthetic dataset. This dataset belongs to group of Autonomous vehicles datasets. The number of images used for training was 800 and 200 were used for validation.</p>\n<h3>Training</h3>\n<p>Each of these models were set to train for 100 epochs with EarlyStopping enabled. So, each model\u2019s training stopped somewhere between 30 to 50 epochs. The parameter used to monitor training was validation loss.</p>\n<p>Batch size and learning rate was decided by running several experiments on one architecture and then was same kept constant for the entire experiment (that is, batch size and learning rate was same for each model that was being trained). Now batch size and learning rate can be different for each architecture, but to maintain uniformity for the experiment, each model was trained using same hyperparameters.</p>\n<p>You may now think, why the number of epochs were not same? So, by keeping the number of epochs same, we wouldn\u2019t know which model converges faster, whether lesser number of parameters means faster convergence(less epochs needed for training). So, I allowed each model to take it\u2019s own time to converge, giving better insights about the training.</p>\n<h3>EfficientNet architectures as Encoder in\u00a0UNet</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*KGxIa2tx_9iEo9JzHqnU_w.png\"><figcaption>Figure 1\u00a0: Source of image\u200a\u2014\u200a<a href=\"https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html\">link</a></figcaption></figure><p>In the above image, we can see comparison of different architectures w.r.t the number of parameters and Top 1 accuracy on Imagenet dataset. We see EfficientNetB0 having very less number of parameters and still having better accuracy than ResNet 50 which has significant amount of parameters. Which shows how promising the EfficientNet family is! Lesser parameters means faster training and lighter\u00a0model.</p>\n<p>All these above numbers are when we are using these models for normal classification problems as in when we directly use an EfficientNet architecture. What about the number of parameters when we use it as an encoder in\u00a0UNet?</p>\n<p>Keeping the encoder part frozen (encoder.trainable = False in Keras), we have the following number of parameters.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/976/1*cN5WR9gZcEnx0J1Tlx1m0w.png\"><figcaption>Figure 2\u00a0: Parameters in each\u00a0model</figcaption></figure><p>Coming to training details of each architecture (the following table shows metrices and epoch number having least validation loss)</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*I8VwKY_jou6iEQgtFAjWVw.png\"><figcaption>Figure 3\u00a0: Performance of each\u00a0model</figcaption></figure><p>The above image shows\u200a\u2014\u200afor each model, in which epoch least validation loss was achieved, total epochs the model was trained for and other relevant\u00a0metrics.</p>\n<h3>What do we understand from above tables (Figure 2 and Figure\u00a03)?</h3>\n<ol>\n<li>From figure 3, we see that from <strong>UNet_EfficientNetB0 to UNet_EfficientNetB6</strong>, <strong>validation MeanIOU remains same </strong><em>(except for UNet_EfficientNetB1 and UNet_EfficientNetB2)</em><strong>, difference between validation loss </strong>is just<strong> 0.01 or 0.02 </strong>and<strong> validation accuracy is also almost same <em>(difference of 0.01). </em></strong><em>The time for each epoch on average for these 6 architectures were 1 minute to 1.5\u00a0minute.</em>\n</li>\n<li>\n<em>Referring to figure 2, from </em><strong>UNet_EfficientNetB0 to UNet_EfficientNetB6</strong> (<em>except UNet_EfficientNetB2</em>), the <strong>number of trainable parameters are almost same</strong>. <strong>UNet_EfficientNetB2</strong> has <strong>highest number of trainable parameters</strong> (even more than UNet_EfficientNetB7, which is the longest architecture compared to other 6) and its <strong>overall</strong> <strong>performance is worst</strong> compared to other\u00a0models.</li>\n<li>Coming to<strong> UNet_EfficientNetB7</strong>, we see <strong>significant increase in Validation MeanIOU</strong> and <strong>least validation loss</strong> amongst all. Even I was surprise to see this. If you refer figure 2, UNet_EfficientNetB7 has highest number of parameters, approximately 2 times of that in UNet_EfficientNetB0. UNet_EfficientNetB7 took about 2.5\u20133 mins for each\u00a0epoch.</li>\n<li>So if you want a light model which trains very fast and performs significantly good, you can sure go with UNet_EfficientNetB0. From above tables, you can see that from UNet_EfficientNetB0 to UNet_EfficientNetB6 the performance metrices are same so why to waste resources?</li>\n<li>If training time and size of model is not a factor, then definitely go with UNet_EfficientNetB7. There is significant jump in performance as well as\u00a0output.</li>\n</ol>\n<h3>Performance Graphs and\u00a0Output</h3>\n<p><strong>Note\u00a0: The images are kept large to avoid\u00a0blur</strong></p>\n<h4>1. UNet_EfficientNetB0</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Or-5SGGrI20KAq1Kb5EXgg.png\"><figcaption><strong>UNet_EfficientNetB0\u200a\u2014\u200aEpochs vs Validation Loss</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*cIghkYkvszRFfDmD8pJ52A.png\"><figcaption><strong>UNet_EfficientNetB0\u200a\u2014\u200aEpochs vs Validation MeanIOU</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*1i7OxRmsBk4S4GIR3DVrAg.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*Twt--mCgbpIZIGOtSMoOqw.png\"><figcaption><strong>UNet_EfficientNetB0 Output</strong></figcaption></figure><h4>2. UNet_EfficientNetB1</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*QaqZywuhnce3J5oTjqs-fA.png\"><figcaption><strong>UNet_EfficientNetB1\u200a\u2014\u200aEpochs vs Validation Loss</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*L9D-_tlbjE7SxoQLCpazhw.png\"><figcaption><strong>UNet_EfficientNetB1\u200a\u2014\u200aEpochs vs Validation MeanIOU</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*tw078CP45oRroZTnk0MfFw.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*j0gnfqX3-jA9ji300CLdxQ.png\"><figcaption><strong>UNet_EfficientNetB1 Output</strong></figcaption></figure><h4>3. UNet_EfficientNetB2</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*O99lp1ZxyaawoL4L0TEx5g.png\"><figcaption><strong>UNet_EfficientNetB2\u200a\u2014\u200aEpochs vs Validation Loss</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*zYuWUgUNHh6EpC8clBVsHg.png\"><figcaption><strong>UNet_EfficientNetB2\u200a\u2014\u200aEpochs vs Validation MeanIOU</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*LoPLJ_7m1KTTh-g6rcFQzA.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*cI5LeuWmt4I2koFaMlwgoQ.png\"><figcaption><strong>UNet_EfficientNetB2 Output</strong></figcaption></figure><h4>4. UNet_EfficientNetB3</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*pwVl0_hNYuficM3a1MUykw.png\"><figcaption><strong>UNet_EfficientNetB3\u200a\u2014\u200aEpochs vs Validation Loss</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*uuBcnLP6jKDL_y2R4MzEmA.png\"><figcaption><strong>UNet_EfficientNetB3\u2014 Epochs vs Validation MeanIOU</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*7oUvWqxe7LYVDwWF2weNWg.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*6GBzP_Q5NOWJHdVJ1_w9ww.png\"><figcaption><strong>UNet_EfficientNetB3 Output</strong></figcaption></figure><h4>5. UNet_EfficientNetB4</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*EDVTMBIvWcys7e3y6JzgGg.png\"><figcaption><strong>UNet_EfficientNetB4\u200a\u2014\u200aEpochs vs Validation Loss</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*kcs4p6t2wMtR-4c1aSuZJA.png\"><figcaption><strong>UNet_EfficientNetB4\u200a\u2014\u200aEpochs vs Validation MeanIOU</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*oF62TEZosudCeFwAe-YdXA.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*AxZXoWvmgnUE0mprrNS61w.png\"><figcaption><strong>UNet_EfficientNetB4 Output</strong></figcaption></figure><h4>6. UNet_EfficientNetB5</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*mj_Tm4T6r5tTAKKD23kjRg.png\"><figcaption><strong>UNet_EfficientNetB5\u200a\u2014\u200aEpochs vs Validation Loss</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*_BfKDhOLmmTLPFJcoztiEQ.png\"><figcaption><strong>UNet_EfficientNetB5\u2014 Epochs vs Validation MeanIOU</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*zr6l4V_0ml2KmEFa8pgMrw.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*b5_wrtLsDWEAnTI08xS1jQ.png\"><figcaption><strong>UNet_EfficientNetB5 Output</strong></figcaption></figure><h4>7. UNet_EfficientNetB6</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*gIkRSx86RH5Sz0n6b1zOjw.png\"><figcaption><strong>UNet_EfficientNetB6\u200a\u2014\u200aEpochs vs Validation Loss</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*GTA9cTC6sLadsmHiPY182w.png\"><figcaption><strong>UNet_EfficientNetB6\u200a\u2014\u200aEpochs vs Validation MeanIOU</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*8DDpyFRUog4B9CSVTpLRPg.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*5YnWnbOk4cyo584HhXJz4w.png\"><figcaption><strong>UNet_EfficientNetB6 Output</strong></figcaption></figure><h4>8. UNet_EfficientNetB7</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*7f9oWkLtaOZHdkMkdwiH6g.png\"><figcaption><strong>UNet_EfficientNetB7\u2014 Epochs vs Validation Loss</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*rOxSx5bnzbyzTREZQXUtcA.png\"><figcaption><strong>UNet_EfficientNetB7\u200a\u2014\u200aEpochs vs Validation MeanIOU</strong></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*NTwaJ8cdK-lcQZGptnVAwA.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*eRqZjGzTMGrNBUu_l_29lQ.png\"><figcaption><strong>UNet_EfficientNetB7 Output</strong></figcaption></figure><h3>Conclusion</h3>\n<p>The choice model is also not only based on performance metrices but also on the actual output of the model. In production what matters the most is output and the inference time of a\u00a0model.</p>\n<p>Thankyou\u00a0:).<br>Connect with me on <a href=\"https://www.linkedin.com/in/sahil-chachra/\">LinkedIn</a>.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a73ec6aeffe8\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["efficientnet","deep-learning","artificial-intelligence","research","semantic-segmentation"]},{"title":"Vanilla UNet vs UNet with EfficientNetB0 as Encoder","pubDate":"2021-06-23 17:12:42","link":"https://sahilchachra.medium.com/vanilla-unet-vs-unet-with-efficientnetb0-as-encoder-55495edd2ceb?source=rss-f31bf6073414------2","guid":"https://medium.com/p/55495edd2ceb","author":"Sahil Chachra","thumbnail":"https://cdn-images-1.medium.com/max/850/1*5wnQrauDShcZ7VhpVMHKog.png","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/850/1*5wnQrauDShcZ7VhpVMHKog.png\"><figcaption>This is output of UNet with EfficientNetB0 as encoder (Original image\u200a\u2014\u200a<a href=\"https://profilerehab.com/theme/facebook/iron_man_cover_1\">link</a>)</figcaption></figure><p><em>In this blog, I will discuss the performance of Vanilla(basic/simple) UNet architecture vs UNet with EfficientNetB0 as\u00a0Encoder.</em></p>\n<p>Recently, I had participated in a Kaggle competition and for that competition, a maximum number of submissions were made using EfficientNet architectures. I was surprised to see that almost any submission I viewed had an implementation of EfficientNet models. Then, later I came across EfficientDet, an object detection architecture having EfficientNet architecture in the backbone! So I thought, why not use this as a backbone architecture in UNet and try it\u00a0out?</p>\n<p>To know about EfficientNet, you can refer to this <a href=\"https://medium.com/r?url=https%3A%2F%2Famaarora.github.io%2F2020%2F08%2F13%2Fefficientnet.html\">link</a> and for EfficientDet, this\u00a0<a href=\"https://medium.com/@nainaakash012/efficientdet-scalable-and-efficient-object-detection-ea05ccd28427\">link</a>.</p>\n<p>To take a look at the UNet with EfficientNetB0 as encoder architecture, click\u00a0<a href=\"https://drive.google.com/file/d/1aF7SQAMzKb6vjfskiT9fp_y7ymSRd7HV/view?usp=sharing\">here</a>.</p>\n<h3><strong>Vanilla UNet vs UNet with EfficientNetB0 as\u00a0encoder</strong></h3>\n<p>So, coming to the experiment I performed\u200a\u2014\u200aI trained two models, one using simple UNet architecture and the other one having EfficientNetB0 as an encoder in UNet architecture. The dataset I used was <a href=\"https://www.kaggle.com/dataset/b9d4e32be2f57c2901fc9c5cd5f6633be7075f4b32d73348a6d5db245f2c1934\">Person Segmentation</a>, from\u00a0Kaggle.</p>\n<p>Both the models were trained for 20 epochs on Google Colab. Now, why 20 epochs? The answer is I was performing Transfer learning on EffficientNetB0-UNet, so it achieved quite good values (in specific performance metrics) at 20 epochs. Since this blog deals with a comparison of two architectures, both were supposed to be trained on the same number of epochs. Results are as\u00a0follows.</p>\n<h3><strong>The Loss Values (Training Loss and Validation Loss)</strong></h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/432/1*RQ8jpQbmjq6iTvudw0heHQ.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/432/1*vL9GZelplzk-ngbBSpzyvw.png\"><figcaption>Left\u200a\u2014\u200aVanilla UNet Loss and Validation loss &amp; Right\u200a\u2014\u200aUNetEfficientNetB0 Loss and Validation loss</figcaption></figure><blockquote>In the left graph(Vanilla UNet), we can see a smooth line for training loss whereas an unstable line for validation loss. In the right graph (UNet_EfficienetNetB0), we can see two smooth lines (almost) for both training loss and validation loss. The vanilla UNet model shows lower loss during training but higher loss value during validation whereas UNet_EfficientNetB0 model has almost same value loss for both training and validation.</blockquote>\n<h3>Recall values</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/432/1*jz0kQZQjJx4KcnQ0I-6r1g.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/432/1*cFi0Zz77UJ0zBRqz5FtCaQ.png\"><figcaption>Left\u200a\u2014\u200aComparison of Recall values during training &amp; Right\u200a\u2014\u200aComparison of Recall values during validation.</figcaption></figure><h3><strong>Precision Values</strong></h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/432/1*1uoSKkANUCxKHDYbUUp4MQ.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/432/1*4RCmwqEH2dIc2j2kno_XOA.png\"><figcaption>Left\u200a\u2014\u200aComparison of Precision values during training &amp; Right\u200a\u2014\u200aComparison of Precision values during validation.</figcaption></figure><blockquote>For recall and precision graphs, we can see that Vanilla UNet performs better during training but on the validation set, UNet_EfficienetNetB0 performs\u00a0better.</blockquote>\n<h3>Inference</h3>\n<p><em>After training both the models on Google Colab, I tested them on my local machine. The results are as follows\u00a0:</em></p>\n<h4>Original Images:-</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/409/1*Tha8cbaPofLwyet6Th1bdw.jpeg\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/510/1*z0hbaT8heynbwWLeeRGmhg.jpeg\"><figcaption>Left Image Source\u200a\u2014\u200a<a href=\"https://deadline.com/2021/03/florence-pugh-zach-braff-good-person-deal-mgm-morgan-freeman-1234720070/\">link</a> &amp; Right image source\u200a\u2014\u200a<a href=\"https://profilerehab.com/theme/facebook/iron_man_cover_1\">link</a></figcaption></figure><h4>Output:-</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/510/1*ehhWG56zUGL5Q2q0BDUI3Q.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/510/1*6S2lxNJWpMNwVHLCYHTUTg.png\"><figcaption>Left\u200a\u2014\u200aUNet with EfficientNetB0 as encoder output &amp; Right\u200a\u2014\u200aVanilla UNet\u00a0output</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/409/1*sKErRKewJpNyrDXvHOklew.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/409/1*8xFz-c16ZocGWJSYdBsqDA.png\"><figcaption>Left\u200a\u2014\u200aUNet with EfficientNetB0 as encoder output &amp; Right\u200a\u2014\u200aVanilla UNet\u00a0output</figcaption></figure><p>From the above inference, we can see that UNet with EfficientNetB0 as encoder gives better output compared to Vanilla UNet. I think playing around with the encoder part of UNet, as in say, using EfficientNetB1 or B2 may result in an even better\u00a0model.</p>\n<p>In my next blog, I will compare the performance of EfficientNetB0 to B7 as encoder in\u00a0UNet.</p>\n<p>Connect with me on <a href=\"https://www.linkedin.com/in/sahil-chachra/\">LinkedIn</a>\u00a0:-D. Thanks for reading my\u00a0blog!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=55495edd2ceb\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/850/1*5wnQrauDShcZ7VhpVMHKog.png\"><figcaption>This is output of UNet with EfficientNetB0 as encoder (Original image\u200a\u2014\u200a<a href=\"https://profilerehab.com/theme/facebook/iron_man_cover_1\">link</a>)</figcaption></figure><p><em>In this blog, I will discuss the performance of Vanilla(basic/simple) UNet architecture vs UNet with EfficientNetB0 as\u00a0Encoder.</em></p>\n<p>Recently, I had participated in a Kaggle competition and for that competition, a maximum number of submissions were made using EfficientNet architectures. I was surprised to see that almost any submission I viewed had an implementation of EfficientNet models. Then, later I came across EfficientDet, an object detection architecture having EfficientNet architecture in the backbone! So I thought, why not use this as a backbone architecture in UNet and try it\u00a0out?</p>\n<p>To know about EfficientNet, you can refer to this <a href=\"https://medium.com/r?url=https%3A%2F%2Famaarora.github.io%2F2020%2F08%2F13%2Fefficientnet.html\">link</a> and for EfficientDet, this\u00a0<a href=\"https://medium.com/@nainaakash012/efficientdet-scalable-and-efficient-object-detection-ea05ccd28427\">link</a>.</p>\n<p>To take a look at the UNet with EfficientNetB0 as encoder architecture, click\u00a0<a href=\"https://drive.google.com/file/d/1aF7SQAMzKb6vjfskiT9fp_y7ymSRd7HV/view?usp=sharing\">here</a>.</p>\n<h3><strong>Vanilla UNet vs UNet with EfficientNetB0 as\u00a0encoder</strong></h3>\n<p>So, coming to the experiment I performed\u200a\u2014\u200aI trained two models, one using simple UNet architecture and the other one having EfficientNetB0 as an encoder in UNet architecture. The dataset I used was <a href=\"https://www.kaggle.com/dataset/b9d4e32be2f57c2901fc9c5cd5f6633be7075f4b32d73348a6d5db245f2c1934\">Person Segmentation</a>, from\u00a0Kaggle.</p>\n<p>Both the models were trained for 20 epochs on Google Colab. Now, why 20 epochs? The answer is I was performing Transfer learning on EffficientNetB0-UNet, so it achieved quite good values (in specific performance metrics) at 20 epochs. Since this blog deals with a comparison of two architectures, both were supposed to be trained on the same number of epochs. Results are as\u00a0follows.</p>\n<h3><strong>The Loss Values (Training Loss and Validation Loss)</strong></h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/432/1*RQ8jpQbmjq6iTvudw0heHQ.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/432/1*vL9GZelplzk-ngbBSpzyvw.png\"><figcaption>Left\u200a\u2014\u200aVanilla UNet Loss and Validation loss &amp; Right\u200a\u2014\u200aUNetEfficientNetB0 Loss and Validation loss</figcaption></figure><blockquote>In the left graph(Vanilla UNet), we can see a smooth line for training loss whereas an unstable line for validation loss. In the right graph (UNet_EfficienetNetB0), we can see two smooth lines (almost) for both training loss and validation loss. The vanilla UNet model shows lower loss during training but higher loss value during validation whereas UNet_EfficientNetB0 model has almost same value loss for both training and validation.</blockquote>\n<h3>Recall values</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/432/1*jz0kQZQjJx4KcnQ0I-6r1g.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/432/1*cFi0Zz77UJ0zBRqz5FtCaQ.png\"><figcaption>Left\u200a\u2014\u200aComparison of Recall values during training &amp; Right\u200a\u2014\u200aComparison of Recall values during validation.</figcaption></figure><h3><strong>Precision Values</strong></h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/432/1*1uoSKkANUCxKHDYbUUp4MQ.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/432/1*4RCmwqEH2dIc2j2kno_XOA.png\"><figcaption>Left\u200a\u2014\u200aComparison of Precision values during training &amp; Right\u200a\u2014\u200aComparison of Precision values during validation.</figcaption></figure><blockquote>For recall and precision graphs, we can see that Vanilla UNet performs better during training but on the validation set, UNet_EfficienetNetB0 performs\u00a0better.</blockquote>\n<h3>Inference</h3>\n<p><em>After training both the models on Google Colab, I tested them on my local machine. The results are as follows\u00a0:</em></p>\n<h4>Original Images:-</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/409/1*Tha8cbaPofLwyet6Th1bdw.jpeg\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/510/1*z0hbaT8heynbwWLeeRGmhg.jpeg\"><figcaption>Left Image Source\u200a\u2014\u200a<a href=\"https://deadline.com/2021/03/florence-pugh-zach-braff-good-person-deal-mgm-morgan-freeman-1234720070/\">link</a> &amp; Right image source\u200a\u2014\u200a<a href=\"https://profilerehab.com/theme/facebook/iron_man_cover_1\">link</a></figcaption></figure><h4>Output:-</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/510/1*ehhWG56zUGL5Q2q0BDUI3Q.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/510/1*6S2lxNJWpMNwVHLCYHTUTg.png\"><figcaption>Left\u200a\u2014\u200aUNet with EfficientNetB0 as encoder output &amp; Right\u200a\u2014\u200aVanilla UNet\u00a0output</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/409/1*sKErRKewJpNyrDXvHOklew.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/409/1*8xFz-c16ZocGWJSYdBsqDA.png\"><figcaption>Left\u200a\u2014\u200aUNet with EfficientNetB0 as encoder output &amp; Right\u200a\u2014\u200aVanilla UNet\u00a0output</figcaption></figure><p>From the above inference, we can see that UNet with EfficientNetB0 as encoder gives better output compared to Vanilla UNet. I think playing around with the encoder part of UNet, as in say, using EfficientNetB1 or B2 may result in an even better\u00a0model.</p>\n<p>In my next blog, I will compare the performance of EfficientNetB0 to B7 as encoder in\u00a0UNet.</p>\n<p>Connect with me on <a href=\"https://www.linkedin.com/in/sahil-chachra/\">LinkedIn</a>\u00a0:-D. Thanks for reading my\u00a0blog!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=55495edd2ceb\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["efficientnet","unet","semantic-segmentation","deep-learning","computer-vision"]}]}