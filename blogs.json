{"status":"ok","feed":{"url":"https://medium.com/feed/@sahilchachra","title":"Stories by Sahil Chachra on Medium","link":"https://medium.com/@sahilchachra?source=rss-f31bf6073414------2","author":"","description":"Stories by Sahil Chachra on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/1*qeoZztJDdpLVrCSEOBy9zg.jpeg"},"items":[{"title":"All you want, to get started with GStreamer in Python","pubDate":"2022-05-29 10:55:02","link":"https://sahilchachra.medium.com/all-you-want-to-get-started-with-gstreamer-in-python-2276d9ed548e?source=rss-f31bf6073414------2","guid":"https://medium.com/p/2276d9ed548e","author":"Sahil Chachra","thumbnail":"https://cdn-images-1.medium.com/max/1024/0*aBffV9m1C7DK2TjK.png","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*aBffV9m1C7DK2TjK.png\"><figcaption><a href=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/db/Gstreamer-logo.svg/2560px-Gstreamer-logo.svg.png\">Source</a></figcaption></figure><p>This blog helps you get started with GStreamer. It will cover Introduction to GStreamer, Installation process (For Ubuntu based distros), Basic terms in GStreamer, Sample pipelines and What few of the components in the pipeline mean\u00a0. The blog will also help you use GStreamer in OpenCV and also show you some\u00a0demos.</p>\n<h3>Contents</h3>\n<ol>\n<li>Introduction</li>\n<li>Installation</li>\n<li>Basic Terms</li>\n<li>Example pipelines</li>\n<li>GStreamer &amp;\u00a0OpenCV</li>\n<li>References</li>\n</ol>\n<h3>Introduction</h3>\n<h4>What is GStreamer?</h4>\n<p>GStreamer is open source, cross platform, pipeline based framework for multimedia. Gstreamer is used in Media players, Video/Audio editors, Web browsers, Streaming servers, etc. They rely heavily on threads to process the\u00a0data.</p>\n<p>From official docs\u200a\u2014\u200a\u201c<em>The GStreamer core function is to provide a framework for plugins, data flow and media type handling/negotiation. It also provides an API to write applications using the various plugins.</em>\u201d</p>\n<p>GStreamer helps build pipeline workflows in which it reads a file in one format, process them (resize, rescale, add filters) and then export it into another format. Each components in Gstreamer is plug and play. For example, in your pipeline, you can clip, crop, transcode and merge audio video from different source using just Gstreamer in command\u00a0line!</p>\n<p>By using GStreamer in your Computer Vision pipeline, for example, you will be able to convert your input stream from one format to another, resize and scale your input by just one line of command in Gstreamer before you pass it to the model! You can even send your inferred output (same frames) in format required by some source by converting/encoding/resizing/rescaling it on the go in the Gstreamer pipeline\u00a0itself.</p>\n<h4>Architecture in\u00a0brief</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/590/0*oU9xjINP1H-6VHrf.png\"><figcaption>GStreamer Architecture (Source\u200a\u2014\u200a<a href=\"https://gstreamer.freedesktop.org/documentation/application-development/introduction/gstreamer.html?gi-language=c\">Docs</a>)</figcaption></figure><p><strong>GStreamer core framework</strong> is the heart of the design. It provides data transfer, primitives for negotiating data types, communication channels for the applications to talk and synchronises media.</p>\n<p>Yellow boxes on the top row are actual applications. Blue box on the top layer is GStreamer tools. The bottom layer are\u00a0plugins.</p>\n<h3>Installation (Ubuntu/PopOs/Mint)</h3>\n<p>a. Make sure you have <a href=\"https://linuxize.com/post/how-to-install-gcc-compiler-on-ubuntu-18-04/\">gcc</a> and <a href=\"https://vitux.com/how-to-install-cmake-on-ubuntu/\">cmake</a> installed.</p>\n<p>b. Install <a href=\"https://docs.anaconda.com/anaconda/install/linux/\">Anaconda</a> or <a href=\"https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/\">Python Virtual environment</a></p>\n<p>c. Follow this <a href=\"https://medium.com/@arfanmahmud47/build-opencv-4-from-source-with-gstreamer-ubuntu-zorin-peppermint-c2cff5393ef\">blog</a>(personally used this blog) to then install all the dependencies (such as codec files and other libraries) and build OpenCV with GStreamer.</p>\n<p>Note\u200a\u2014\u200aOpenCV by default won\u2019t be able to leverage GStreamer if already installed. We need to build OpenCV with GStreamer to use it in our\u00a0code.</p>\n<h3>Basic Terms</h3>\n<ol><li><strong>What is GstElement?</strong></li></ol>\n<p><em>GstElement </em>object<em> </em>is the basic building block for media pipeline. All the elements such as decoder, encoder, demux which we see as a black box, have been derived from GstElement.</p>\n<p>Elements (such as sink, source or filter) have properties which are used to modify their behaviour. They also have signals which help them execute a function call on the\u00a0element.</p>\n<p>To know about properties of elements, type gst-inspect-1.0 name_of_element. Example\u00a0:-</p>\n<pre>gst-inspect-1.0 autovideosink</pre>\n<p><strong>2. What is a pipeline in GStreamer?</strong></p>\n<p>In simple terms, we take in several components of GStreamer, such as input video source, video decoder and output source, put them together one after the other! That\u2019s it! For example\u00a0:-</p>\n<p><em>From docs</em>\u200a\u2014\u200aDisplay only the video portion of an MPEG-1 video file, outputting to an X display\u00a0window:</p>\n<pre>gst-launch-1.0 filesrc location=videofile.mpg ! dvddemux ! mpeg2dec ! xvimagesink</pre>\n<p>Each element is seperated by \u2018!\u2019 and has spaces before and after\u00a0\u2018!\u2019.</p>\n<p><strong>3. What is source and\u00a0sink?</strong></p>\n<p>Source\u200a\u2014\u200aSource elements are those which can only generate data. Such as video file or an IP\u00a0camera.</p>\n<p>Sink\u200a\u2014\u200aSink elements are end points in a pipeline. For example, Output video playback, soundcard playback, screen, disk writing are sink elements.</p>\n<p><strong>4. Filters</strong></p>\n<p>Filters and filter-like elements have both input and output. They recieve data as well as send the data after some kind of processing.</p>\n<p><strong>5. What are source and sink\u00a0pads?</strong></p>\n<p>Consider a filter, which resizes the video frame. Now, it will take video frame as input and give resized video frame as output. The point where it takes in an input is called sink pad and from the point where it send out processed data is called source\u00a0pad.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/155/0*ah6Tm4fSSJJ1MRVV.png\"><figcaption>Source\u200a\u2014\u200a<a href=\"https://gstreamer.freedesktop.org/documentation/tools/gst-launch.html?gi-language=c\">Docs</a></figcaption></figure><p>Sink pad is always on the left and Source pad is always on the right, be it any GstElement.</p>\n<p><strong>6. State of the\u00a0Elements</strong></p>\n<p>NULL\u200a\u2014\u200aDeactivated element. No resources has been allocated to the element. It is denoted by GST_STATE_NULL and it is a default state for an\u00a0element.</p>\n<p>READY\u200a\u2014\u200aAll the required resources have been allocated and is ready to process. It is denoted by GST_STATE_READY. In this state the stream is not yet\u00a0opened.</p>\n<p>PAUSED\u200a\u2014\u200aDenoted by GST_STATE_PAUSED. An element has a stream opened, but is not processing it actively. Quoting from docs\u200a\u2014\u200a\u201cElements going into the PAUSED state should prepare themselves for moving over to the PLAYING state as soon as possible.\u201d</p>\n<p>PLAYING\u200a\u2014\u200aIn this state, the element actively process the data. It is denoted by GST_STATE_PLAYING.</p>\n<p>GStreamer Core handles the changing of the states of the elements automatically.</p>\n<h3>Example pipelines</h3>\n<ol><li><strong>The first pipeline is the Hello world for GStreamer</strong></li></ol>\n<pre>gst-launch-1.0 videotestsrc ! videoconvert ! autovideosink</pre>\n<p>gst-launch-1.0 = Build and launch a\u00a0pipeline</p>\n<p>videotestsource = sample video from GStreamer examples</p>\n<p>videoconver = Converts video frames to multiple\u00a0formats</p>\n<p>autovideosink = automatically detects an appropriate video sink to\u00a0use</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*nxqB5R1pKRuyKwWgar-xmQ.gif\"></figure><p><strong>2. Adding a capability to the\u00a0pipeline</strong></p>\n<pre>gst-launch-1.0 videotestsrc ! video/x-raw, format=BGR ! autovideoconvert ! ximagesink</pre>\n<p>video/x-raw, format=BGR is a capability of videotestsrc. We are converting the input frames to BGR and then sending it to autovideoconvert.</p>\n<p>autovideoconvert automatically converts the format of the video which is supported by the next element in the pipeline.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*nMgtaTYpqpOE0XiSLpDB-Q.gif\"></figure><p><strong>3. Setting width, height and framerate</strong></p>\n<pre>gst-launch-1.0 videotestsrc ! video/x-raw, format=BGR ! autovideoconvert ! videoconvert ! video/x-raw, width=640, height=480, framerate=1/2 ! ximagesink</pre>\n<p>Here we change the height, width and framerate of the input video before sending it to the display. ximagesink is display. By giving Framerate as 1/2, it plays 1 frame every 2\u00a0second.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*4pnEbu7kkAVg3Ai3_rcR0Q.gif\"></figure><p><strong>4. Using phone\u2019s camera as IP\u00a0cam</strong></p>\n<p>Command is\u00a0:-</p>\n<pre>sudo gst-launch-1.0 rtspsrc location=rtsp://192.168.1.7:8080/h264_ulaw.sdp latency=10 ! queue ! rtph264depay ! h264parse ! avdec_h264 ! videoconvert ! videoscale ! video/x-raw,width=640,height=480 ! ximagesink</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*maUu5_KWFbgGMo_we39B9A.gif\"></figure><p><strong>Let\u2019s break down the elements and see what each of them\u00a0means.</strong></p>\n<p>rtspsrc = Input source is RTSP. Location is property of rtspsrc element. We define the path of input source which is in this case an IP address. Latency is also a property. From docs \u201c<em>For pipelines with live sources, a latency is introduced, mostly because of the way a live source works</em>\u201d. Read more about Latency in the\u00a0<a href=\"https://gstreamer.freedesktop.org/documentation/additional/design/latency.html?gi-language=c\">docs</a>.</p>\n<p>queue = queue creates a buffer such that the input data from the source is stored and the next element would pick the data from the queue and process at it\u2019s own pace. You can read more about it\u00a0<a href=\"https://gstreamer.freedesktop.org/documentation/coreelements/queue.html?gi-language=c\">here</a>.</p>\n<p>rtph264depay = H264 video from RTP\u00a0packets</p>\n<p>h264parse = Parses h264 encoded frames in a way such that avdec_h264 can understand</p>\n<p>avdec_h264 = Decodes h264 formatted data</p>\n<p>videoconvert = Ensures compatibility between previous element and next element by converting frames from one format to\u00a0other</p>\n<p>videoscale = Resized video. video/x-raw is capability of videoscale. Height and width are properties.</p>\n<p>ximagesink = Displays output on\u00a0screen</p>\n<p><strong><em>NOTE\u00a0: Properties are added with the element by giving a space after the element and Capabilities are added like a element, seperating it with a\u00a0\u2018!\u2019.</em></strong></p>\n<h3>GStreamer &amp;\u00a0OpenCV</h3>\n<p>I\u2019ll show a sample code on how to use GStreamer pipeline as input to\u00a0OpenCV</p>\n<pre>import cv2</pre>\n<pre>gstreamer_str = \"sudo gst-launch-1.0 rtspsrc location=rtsp://192.168.1.5:8080/h264_ulaw.sdp latency=100 ! queue ! rtph264depay ! h264parse ! avdec_h264 ! videoconvert ! videoscale ! video/x-raw,width=640,height=480,format=BGR ! appsink drop=1\"</pre>\n<pre>cap = cv2.VideoCapture(gstreamer_str, cv2.CAP_GSTREAMER)</pre>\n<pre>while(cap.isOpened()):</pre>\n<pre>    ret, frame = cap.read()</pre>\n<pre>    if ret:</pre>\n<pre>        cv2.imshow(\"Input via Gstreamer\", frame)</pre>\n<pre>        if cv2.waitKey(25) &amp; 0xFF == ord('q'):</pre>\n<pre>            break</pre>\n<pre>        else:</pre>\n<pre>            break</pre>\n<pre>cap.release()</pre>\n<pre>cv2.destroyAllWindows()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*3L3L26ks6yZqRKhnpLYD7w.gif\"></figure><h3>References</h3>\n<ol>\n<li>Installation Guide\u200a\u2014\u200a<a href=\"https://medium.com/@arfanmahmud47/build-opencv-4-from-source-with-gstreamer-ubuntu-zorin-peppermint-c2cff5393ef\">Medium\u00a0Article</a>\n</li>\n<li>Understanding GStreamer for Absolute Beginners\u200a\u2014\u200a<a href=\"https://www.youtube.com/watch?v=_yU1kfcC6rY\">YouTube</a>\n</li>\n<li>Introduction To GStreamer\u200a\u2014\u200a<a href=\"https://www.youtube.com/watch?v=am_0vlQpJgE\">YouTube</a>\n</li>\n<li>GStreamer documentation\u200a\u2014\u200a<a href=\"https://gstreamer.freedesktop.org/documentation/application-development/introduction/gstreamer.html?gi-language=c\">Docs</a>\n</li>\n</ol>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=2276d9ed548e\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*aBffV9m1C7DK2TjK.png\"><figcaption><a href=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/db/Gstreamer-logo.svg/2560px-Gstreamer-logo.svg.png\">Source</a></figcaption></figure><p>This blog helps you get started with GStreamer. It will cover Introduction to GStreamer, Installation process (For Ubuntu based distros), Basic terms in GStreamer, Sample pipelines and What few of the components in the pipeline mean\u00a0. The blog will also help you use GStreamer in OpenCV and also show you some\u00a0demos.</p>\n<h3>Contents</h3>\n<ol>\n<li>Introduction</li>\n<li>Installation</li>\n<li>Basic Terms</li>\n<li>Example pipelines</li>\n<li>GStreamer &amp;\u00a0OpenCV</li>\n<li>References</li>\n</ol>\n<h3>Introduction</h3>\n<h4>What is GStreamer?</h4>\n<p>GStreamer is open source, cross platform, pipeline based framework for multimedia. Gstreamer is used in Media players, Video/Audio editors, Web browsers, Streaming servers, etc. They rely heavily on threads to process the\u00a0data.</p>\n<p>From official docs\u200a\u2014\u200a\u201c<em>The GStreamer core function is to provide a framework for plugins, data flow and media type handling/negotiation. It also provides an API to write applications using the various plugins.</em>\u201d</p>\n<p>GStreamer helps build pipeline workflows in which it reads a file in one format, process them (resize, rescale, add filters) and then export it into another format. Each components in Gstreamer is plug and play. For example, in your pipeline, you can clip, crop, transcode and merge audio video from different source using just Gstreamer in command\u00a0line!</p>\n<p>By using GStreamer in your Computer Vision pipeline, for example, you will be able to convert your input stream from one format to another, resize and scale your input by just one line of command in Gstreamer before you pass it to the model! You can even send your inferred output (same frames) in format required by some source by converting/encoding/resizing/rescaling it on the go in the Gstreamer pipeline\u00a0itself.</p>\n<h4>Architecture in\u00a0brief</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/590/0*oU9xjINP1H-6VHrf.png\"><figcaption>GStreamer Architecture (Source\u200a\u2014\u200a<a href=\"https://gstreamer.freedesktop.org/documentation/application-development/introduction/gstreamer.html?gi-language=c\">Docs</a>)</figcaption></figure><p><strong>GStreamer core framework</strong> is the heart of the design. It provides data transfer, primitives for negotiating data types, communication channels for the applications to talk and synchronises media.</p>\n<p>Yellow boxes on the top row are actual applications. Blue box on the top layer is GStreamer tools. The bottom layer are\u00a0plugins.</p>\n<h3>Installation (Ubuntu/PopOs/Mint)</h3>\n<p>a. Make sure you have <a href=\"https://linuxize.com/post/how-to-install-gcc-compiler-on-ubuntu-18-04/\">gcc</a> and <a href=\"https://vitux.com/how-to-install-cmake-on-ubuntu/\">cmake</a> installed.</p>\n<p>b. Install <a href=\"https://docs.anaconda.com/anaconda/install/linux/\">Anaconda</a> or <a href=\"https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/\">Python Virtual environment</a></p>\n<p>c. Follow this <a href=\"https://medium.com/@arfanmahmud47/build-opencv-4-from-source-with-gstreamer-ubuntu-zorin-peppermint-c2cff5393ef\">blog</a>(personally used this blog) to then install all the dependencies (such as codec files and other libraries) and build OpenCV with GStreamer.</p>\n<p>Note\u200a\u2014\u200aOpenCV by default won\u2019t be able to leverage GStreamer if already installed. We need to build OpenCV with GStreamer to use it in our\u00a0code.</p>\n<h3>Basic Terms</h3>\n<ol><li><strong>What is GstElement?</strong></li></ol>\n<p><em>GstElement </em>object<em> </em>is the basic building block for media pipeline. All the elements such as decoder, encoder, demux which we see as a black box, have been derived from GstElement.</p>\n<p>Elements (such as sink, source or filter) have properties which are used to modify their behaviour. They also have signals which help them execute a function call on the\u00a0element.</p>\n<p>To know about properties of elements, type gst-inspect-1.0 name_of_element. Example\u00a0:-</p>\n<pre>gst-inspect-1.0 autovideosink</pre>\n<p><strong>2. What is a pipeline in GStreamer?</strong></p>\n<p>In simple terms, we take in several components of GStreamer, such as input video source, video decoder and output source, put them together one after the other! That\u2019s it! For example\u00a0:-</p>\n<p><em>From docs</em>\u200a\u2014\u200aDisplay only the video portion of an MPEG-1 video file, outputting to an X display\u00a0window:</p>\n<pre>gst-launch-1.0 filesrc location=videofile.mpg ! dvddemux ! mpeg2dec ! xvimagesink</pre>\n<p>Each element is seperated by \u2018!\u2019 and has spaces before and after\u00a0\u2018!\u2019.</p>\n<p><strong>3. What is source and\u00a0sink?</strong></p>\n<p>Source\u200a\u2014\u200aSource elements are those which can only generate data. Such as video file or an IP\u00a0camera.</p>\n<p>Sink\u200a\u2014\u200aSink elements are end points in a pipeline. For example, Output video playback, soundcard playback, screen, disk writing are sink elements.</p>\n<p><strong>4. Filters</strong></p>\n<p>Filters and filter-like elements have both input and output. They recieve data as well as send the data after some kind of processing.</p>\n<p><strong>5. What are source and sink\u00a0pads?</strong></p>\n<p>Consider a filter, which resizes the video frame. Now, it will take video frame as input and give resized video frame as output. The point where it takes in an input is called sink pad and from the point where it send out processed data is called source\u00a0pad.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/155/0*ah6Tm4fSSJJ1MRVV.png\"><figcaption>Source\u200a\u2014\u200a<a href=\"https://gstreamer.freedesktop.org/documentation/tools/gst-launch.html?gi-language=c\">Docs</a></figcaption></figure><p>Sink pad is always on the left and Source pad is always on the right, be it any GstElement.</p>\n<p><strong>6. State of the\u00a0Elements</strong></p>\n<p>NULL\u200a\u2014\u200aDeactivated element. No resources has been allocated to the element. It is denoted by GST_STATE_NULL and it is a default state for an\u00a0element.</p>\n<p>READY\u200a\u2014\u200aAll the required resources have been allocated and is ready to process. It is denoted by GST_STATE_READY. In this state the stream is not yet\u00a0opened.</p>\n<p>PAUSED\u200a\u2014\u200aDenoted by GST_STATE_PAUSED. An element has a stream opened, but is not processing it actively. Quoting from docs\u200a\u2014\u200a\u201cElements going into the PAUSED state should prepare themselves for moving over to the PLAYING state as soon as possible.\u201d</p>\n<p>PLAYING\u200a\u2014\u200aIn this state, the element actively process the data. It is denoted by GST_STATE_PLAYING.</p>\n<p>GStreamer Core handles the changing of the states of the elements automatically.</p>\n<h3>Example pipelines</h3>\n<ol><li><strong>The first pipeline is the Hello world for GStreamer</strong></li></ol>\n<pre>gst-launch-1.0 videotestsrc ! videoconvert ! autovideosink</pre>\n<p>gst-launch-1.0 = Build and launch a\u00a0pipeline</p>\n<p>videotestsource = sample video from GStreamer examples</p>\n<p>videoconver = Converts video frames to multiple\u00a0formats</p>\n<p>autovideosink = automatically detects an appropriate video sink to\u00a0use</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*nxqB5R1pKRuyKwWgar-xmQ.gif\"></figure><p><strong>2. Adding a capability to the\u00a0pipeline</strong></p>\n<pre>gst-launch-1.0 videotestsrc ! video/x-raw, format=BGR ! autovideoconvert ! ximagesink</pre>\n<p>video/x-raw, format=BGR is a capability of videotestsrc. We are converting the input frames to BGR and then sending it to autovideoconvert.</p>\n<p>autovideoconvert automatically converts the format of the video which is supported by the next element in the pipeline.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*nMgtaTYpqpOE0XiSLpDB-Q.gif\"></figure><p><strong>3. Setting width, height and framerate</strong></p>\n<pre>gst-launch-1.0 videotestsrc ! video/x-raw, format=BGR ! autovideoconvert ! videoconvert ! video/x-raw, width=640, height=480, framerate=1/2 ! ximagesink</pre>\n<p>Here we change the height, width and framerate of the input video before sending it to the display. ximagesink is display. By giving Framerate as 1/2, it plays 1 frame every 2\u00a0second.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*4pnEbu7kkAVg3Ai3_rcR0Q.gif\"></figure><p><strong>4. Using phone\u2019s camera as IP\u00a0cam</strong></p>\n<p>Command is\u00a0:-</p>\n<pre>sudo gst-launch-1.0 rtspsrc location=rtsp://192.168.1.7:8080/h264_ulaw.sdp latency=10 ! queue ! rtph264depay ! h264parse ! avdec_h264 ! videoconvert ! videoscale ! video/x-raw,width=640,height=480 ! ximagesink</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*maUu5_KWFbgGMo_we39B9A.gif\"></figure><p><strong>Let\u2019s break down the elements and see what each of them\u00a0means.</strong></p>\n<p>rtspsrc = Input source is RTSP. Location is property of rtspsrc element. We define the path of input source which is in this case an IP address. Latency is also a property. From docs \u201c<em>For pipelines with live sources, a latency is introduced, mostly because of the way a live source works</em>\u201d. Read more about Latency in the\u00a0<a href=\"https://gstreamer.freedesktop.org/documentation/additional/design/latency.html?gi-language=c\">docs</a>.</p>\n<p>queue = queue creates a buffer such that the input data from the source is stored and the next element would pick the data from the queue and process at it\u2019s own pace. You can read more about it\u00a0<a href=\"https://gstreamer.freedesktop.org/documentation/coreelements/queue.html?gi-language=c\">here</a>.</p>\n<p>rtph264depay = H264 video from RTP\u00a0packets</p>\n<p>h264parse = Parses h264 encoded frames in a way such that avdec_h264 can understand</p>\n<p>avdec_h264 = Decodes h264 formatted data</p>\n<p>videoconvert = Ensures compatibility between previous element and next element by converting frames from one format to\u00a0other</p>\n<p>videoscale = Resized video. video/x-raw is capability of videoscale. Height and width are properties.</p>\n<p>ximagesink = Displays output on\u00a0screen</p>\n<p><strong><em>NOTE\u00a0: Properties are added with the element by giving a space after the element and Capabilities are added like a element, seperating it with a\u00a0\u2018!\u2019.</em></strong></p>\n<h3>GStreamer &amp;\u00a0OpenCV</h3>\n<p>I\u2019ll show a sample code on how to use GStreamer pipeline as input to\u00a0OpenCV</p>\n<pre>import cv2</pre>\n<pre>gstreamer_str = \"sudo gst-launch-1.0 rtspsrc location=rtsp://192.168.1.5:8080/h264_ulaw.sdp latency=100 ! queue ! rtph264depay ! h264parse ! avdec_h264 ! videoconvert ! videoscale ! video/x-raw,width=640,height=480,format=BGR ! appsink drop=1\"</pre>\n<pre>cap = cv2.VideoCapture(gstreamer_str, cv2.CAP_GSTREAMER)</pre>\n<pre>while(cap.isOpened()):</pre>\n<pre>    ret, frame = cap.read()</pre>\n<pre>    if ret:</pre>\n<pre>        cv2.imshow(\"Input via Gstreamer\", frame)</pre>\n<pre>        if cv2.waitKey(25) &amp; 0xFF == ord('q'):</pre>\n<pre>            break</pre>\n<pre>        else:</pre>\n<pre>            break</pre>\n<pre>cap.release()</pre>\n<pre>cv2.destroyAllWindows()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*3L3L26ks6yZqRKhnpLYD7w.gif\"></figure><h3>References</h3>\n<ol>\n<li>Installation Guide\u200a\u2014\u200a<a href=\"https://medium.com/@arfanmahmud47/build-opencv-4-from-source-with-gstreamer-ubuntu-zorin-peppermint-c2cff5393ef\">Medium\u00a0Article</a>\n</li>\n<li>Understanding GStreamer for Absolute Beginners\u200a\u2014\u200a<a href=\"https://www.youtube.com/watch?v=_yU1kfcC6rY\">YouTube</a>\n</li>\n<li>Introduction To GStreamer\u200a\u2014\u200a<a href=\"https://www.youtube.com/watch?v=am_0vlQpJgE\">YouTube</a>\n</li>\n<li>GStreamer documentation\u200a\u2014\u200a<a href=\"https://gstreamer.freedesktop.org/documentation/application-development/introduction/gstreamer.html?gi-language=c\">Docs</a>\n</li>\n</ol>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=2276d9ed548e\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["deep-learning","gstreamer","python","artificial-intelligence","opencv"]},{"title":"Intel OpenVINO in brief","pubDate":"2022-04-25 05:17:15","link":"https://sahilchachra.medium.com/intel-openvino-in-brief-14a6905f5593?source=rss-f31bf6073414------2","guid":"https://medium.com/p/14a6905f5593","author":"Sahil Chachra","thumbnail":"https://cdn-images-1.medium.com/max/820/0*APM6uuwhoqthdZvU.png","description":"\n<p>This blog focuses on theoretical and practical knowledge of OpenVINO. It discusses few prominent features of OpenVINO to help you get\u00a0started.</p>\n<blockquote>My system\u2019s config\u200a\u2014\u200ai5 9th Gen 9300H, 16GB\u00a0RAM</blockquote>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/820/0*APM6uuwhoqthdZvU.png\"><figcaption>Source\u200a\u2014<a href=\"https://docs.openvino.ai/nightly/_static/images/ov_chart.png\">\u200aIntel\u2019s\u00a0site</a></figcaption></figure><h3>Contents</h3>\n<ol>\n<li>Intro to\u00a0OpenVINO</li>\n<li>Model Optimizer</li>\n<li>Inference Engine</li>\n<li>Performance Benchmarking</li>\n<li>DL Benchmark</li>\n<li>Conclusion</li>\n<li>Refereneces</li>\n</ol>\n<h3>Intro to\u00a0OpenVino</h3>\n<p>OpenVINO, stands for Open Visual inferencing and Neural Network Optimization, is a tookit to deploy vision based solutions on Intel hardwares. OpenVINO is only used for inference and not training of vision based models. You can deploy your model on the cloud or on the edge having intel hardware.</p>\n<p>OpenVINO helps deploy model on Intel CPUs such as Core i5, i7, Xeon or Integrated GPUs such as Intel HD Graphics, Intel Iris or on VPU (Visual Processing Unit) such as <a href=\"https://www.intel.in/content/www/in/en/products/details/processors/movidius-vpu.html\">Movidius</a> or on Intel\u00a0<a href=\"https://www.intel.com/content/www/us/en/products/programmable.html\">FPGAs</a>.</p>\n<p>Link to install OpenVINO\u2014 <a href=\"https://docs.openvino.ai/latest/openvino_docs_install_guides_installing_openvino_linux_header.html\">Intel\u2019s\u00a0Site</a></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/930/1*GykP2rbiDDqePzMrIMIb4Q.png\"><figcaption>OpenVINO Inference Workflow</figcaption></figure><h3>Model Optimizer</h3>\n<p>Model Optimizer abstracts the hardware for the user. Different CPUs have different instruction set and architecture. We also have different frameworks available. To help run models from different frameworks on different Intel devices, Model optimizer first converts the model to intermediate representation called IR and then use IR files (weights and architecture files) to inference on different devices be it CPU or GPU. Model optimizer is also hardware agnostic, meaning the IR files generated can run on any target Intel\u00a0device.</p>\n<p>Model Optimizer converts a model from different framework to single representation. Then it optimizes the model using technique such as layer fusion. Then it changes the format in which weights are stored such from FP32 to FP16 or\u00a0INT8.</p>\n<p><em>IR or Intermediate representation has 2 files\u200a\u2014\u200aXML file and BIN file. XML file stores the model architecture and BIN file stores the model\u2019s\u00a0weights.</em></p>\n<p>Model Optimizer supports Tensorflow, Caffe, Kaldi, MXNet and Onnx (as of publishing this blog) as model framework. NO, it doesn\u2019t supports PyTorch natively but you can convert PyTorch model to Onnx and then pass it to the Model Optimizer.</p>\n<p>For example, if you want to convert your onnx model to\u00a0IR</p>\n<pre>python3 mo_onnx.py --input_model resnet.onnx</pre>\n<p>Checkout this <a href=\"https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html\">link </a>for other arguements which you can\u00a0pass.</p>\n<p>You can also quantize your model to FP16 and INT8 depending whether or not your target device supports it. Check out the below\u00a0table.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/722/1*vTdZiIzsJSgceanQpIfyHQ.png\"><figcaption>Source\u200a\u2014\u200a<a href=\"https://www.youtube.com/watch?v=RF8ypHyiKrY&amp;list=PLg-UKERBljNxdIQir1wrirZJ50yTp4eHv&amp;index=9\">Intel\u2019s YouTube\u00a0video</a></figcaption></figure><p>Model Optimizer lets you resize the input you pass to the first layer of your model. That is, if your model takes image of size 3x224x224, while exporting you can pass the following parameter. This mean you don\u2019t need to retrain your model. By this, all the other layers\u2019 input dimension changes. (<em>Not sure if it works if I want to increase the size of Image and not decrease)</em></p>\n<pre>--input_shape [1, 3, 100, 100] </pre>\n<p>Check out the docs to see how you can cut the network or add scale/mean operations to the\u00a0network.</p>\n<h3>Inference Engine</h3>\n<p>After having optimised our model, here comes the Inference Engine. Inference Engine(IE) is an API and is common for all Intel hardwares. Implementation of a function (say multiplication) will be different for different devices but the IE has plugins for each device. The user only needs to specify the target device and IE takes care of other\u00a0things.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/563/1*ctB94cSOclfmEhyKNpCH-w.png\"><figcaption>Different APIs for different hardware\u00a0devices</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/432/1*BJDaRkFGa0DzbtAbNNpcwQ.png\"><figcaption>Inference Engine Work\u00a0flow</figcaption></figure><h4>Heterogeneous Plug-in</h4>\n<p>Above, you saw that you can run same IR on multiple devices. Now, what if I tell you that OpenVINO lets you run different layers on different hardware devices! Yes, OpenVINO supports this and is called\u200a\u2014\u200aHeterogeneous Plug-in.</p>\n<p>Why do we need this?\u200a\u2014\u200aThere might be some layers which is not supported by FPGA but is supported by CPU! If a full network cannot work on one device then we run different layers on different device!</p>\n<p>Won\u2019t it be slow?\u200a\u2014\u200aYeah it might be slow depending on data transfer speed from one device to another. Not all models would be compatible or at times it won\u2019t be suitable to use this because of throughput.</p>\n<p>OpenVINO takes care of precision when switching devices during inference.</p>\n<h3>Performance Benchmarking</h3>\n<p>Assume you have a model to deploy but the hardware is not fixed yet. Now you bring in Intel devices which are candidate deployment devices. Now, by using Performance Benchmarking, you can get the throughput of the model, latency and best data format on each of these devices, hence helping you choose the correct one as per your requirements.</p>\n<p>You can run benchmarking on synchronous and asynchronous mode, enabling sequential and parallel processing.</p>\n<p>Follow this <a href=\"https://www.youtube.com/watch?v=n7HNNR3r4WE&amp;list=PLg-UKERBljNxdIQir1wrirZJ50yTp4eHv&amp;index=28\">video</a> to see how to use Performance Benchmarking.</p>\n<p>I ran Performance Benchmarking on YoloV5s model by passing 128 images with batch size 1. Here is the\u00a0output!</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/746/1*X0jN4l4PyriEhAuzTRqMRQ.png\"></figure><h3>DL Benchmark</h3>\n<p>DL Benchmark provides UI to optimize your model and run several benchmarks. You can set the maximum accuracy drop you would want when optimizing your model! It offers same options just like the command line way but it becomes easy to\u00a0use.</p>\n<p>Watch <a href=\"https://www.youtube.com/watch?v=20ROqz5j1y8&amp;list=PLg-UKERBljNxdIQir1wrirZJ50yTp4eHv&amp;index=38\">this </a>to learn more. I would have attached screenshots here but watching this video makes more sense and gives better explanation.</p>\n<h3>Conclusion</h3>\n<p>Intel OpenVINO has shown that DL models can be run on CPUs efficiently. This will surely bring down deployment cost as CPUs are relatively cheaper than GPUs. There might some trade offs but I am sure we can find our way\u00a0out.</p>\n<p>So, this blog was just to help you get started with Intel OpenVINO. Do checkout the references for videos from where you can learn more about OpenVINO. Also, do some hands on too! It\u2019s easy to install and\u00a0use.</p>\n<p>Connect with me on <a href=\"https://www.linkedin.com/in/sahil-chachra/\">LinkedIn</a> and <a href=\"https://twitter.com/ChachraSahil\">Twitter</a>.\u00a0:)</p>\n<p>Thanks for reading\u00a0:)</p>\n<h3>References</h3>\n<p>[1] OpenVINO Toolkit Tutorial\u200a\u2014\u200a<a href=\"https://www.youtube.com/playlist?list=PLg-UKERBljNxdIQir1wrirZJ50yTp4eHv\">YouTube\u00a0Link</a></p>\n<p>[2] Intel Movidius\u200a\u2014<a href=\"https://www.intel.in/content/www/in/en/products/details/processors/movidius-vpu.html\">\u200aIntel\u2019s\u00a0site</a></p>\n<p>[3] Intel FPGAs\u200a\u2014<a href=\"https://www.intel.com/content/www/us/en/products/programmable.html\">\u200aIntel\u2019s\u00a0site</a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=14a6905f5593\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>This blog focuses on theoretical and practical knowledge of OpenVINO. It discusses few prominent features of OpenVINO to help you get\u00a0started.</p>\n<blockquote>My system\u2019s config\u200a\u2014\u200ai5 9th Gen 9300H, 16GB\u00a0RAM</blockquote>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/820/0*APM6uuwhoqthdZvU.png\"><figcaption>Source\u200a\u2014<a href=\"https://docs.openvino.ai/nightly/_static/images/ov_chart.png\">\u200aIntel\u2019s\u00a0site</a></figcaption></figure><h3>Contents</h3>\n<ol>\n<li>Intro to\u00a0OpenVINO</li>\n<li>Model Optimizer</li>\n<li>Inference Engine</li>\n<li>Performance Benchmarking</li>\n<li>DL Benchmark</li>\n<li>Conclusion</li>\n<li>Refereneces</li>\n</ol>\n<h3>Intro to\u00a0OpenVino</h3>\n<p>OpenVINO, stands for Open Visual inferencing and Neural Network Optimization, is a tookit to deploy vision based solutions on Intel hardwares. OpenVINO is only used for inference and not training of vision based models. You can deploy your model on the cloud or on the edge having intel hardware.</p>\n<p>OpenVINO helps deploy model on Intel CPUs such as Core i5, i7, Xeon or Integrated GPUs such as Intel HD Graphics, Intel Iris or on VPU (Visual Processing Unit) such as <a href=\"https://www.intel.in/content/www/in/en/products/details/processors/movidius-vpu.html\">Movidius</a> or on Intel\u00a0<a href=\"https://www.intel.com/content/www/us/en/products/programmable.html\">FPGAs</a>.</p>\n<p>Link to install OpenVINO\u2014 <a href=\"https://docs.openvino.ai/latest/openvino_docs_install_guides_installing_openvino_linux_header.html\">Intel\u2019s\u00a0Site</a></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/930/1*GykP2rbiDDqePzMrIMIb4Q.png\"><figcaption>OpenVINO Inference Workflow</figcaption></figure><h3>Model Optimizer</h3>\n<p>Model Optimizer abstracts the hardware for the user. Different CPUs have different instruction set and architecture. We also have different frameworks available. To help run models from different frameworks on different Intel devices, Model optimizer first converts the model to intermediate representation called IR and then use IR files (weights and architecture files) to inference on different devices be it CPU or GPU. Model optimizer is also hardware agnostic, meaning the IR files generated can run on any target Intel\u00a0device.</p>\n<p>Model Optimizer converts a model from different framework to single representation. Then it optimizes the model using technique such as layer fusion. Then it changes the format in which weights are stored such from FP32 to FP16 or\u00a0INT8.</p>\n<p><em>IR or Intermediate representation has 2 files\u200a\u2014\u200aXML file and BIN file. XML file stores the model architecture and BIN file stores the model\u2019s\u00a0weights.</em></p>\n<p>Model Optimizer supports Tensorflow, Caffe, Kaldi, MXNet and Onnx (as of publishing this blog) as model framework. NO, it doesn\u2019t supports PyTorch natively but you can convert PyTorch model to Onnx and then pass it to the Model Optimizer.</p>\n<p>For example, if you want to convert your onnx model to\u00a0IR</p>\n<pre>python3 mo_onnx.py --input_model resnet.onnx</pre>\n<p>Checkout this <a href=\"https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html\">link </a>for other arguements which you can\u00a0pass.</p>\n<p>You can also quantize your model to FP16 and INT8 depending whether or not your target device supports it. Check out the below\u00a0table.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/722/1*vTdZiIzsJSgceanQpIfyHQ.png\"><figcaption>Source\u200a\u2014\u200a<a href=\"https://www.youtube.com/watch?v=RF8ypHyiKrY&amp;list=PLg-UKERBljNxdIQir1wrirZJ50yTp4eHv&amp;index=9\">Intel\u2019s YouTube\u00a0video</a></figcaption></figure><p>Model Optimizer lets you resize the input you pass to the first layer of your model. That is, if your model takes image of size 3x224x224, while exporting you can pass the following parameter. This mean you don\u2019t need to retrain your model. By this, all the other layers\u2019 input dimension changes. (<em>Not sure if it works if I want to increase the size of Image and not decrease)</em></p>\n<pre>--input_shape [1, 3, 100, 100] </pre>\n<p>Check out the docs to see how you can cut the network or add scale/mean operations to the\u00a0network.</p>\n<h3>Inference Engine</h3>\n<p>After having optimised our model, here comes the Inference Engine. Inference Engine(IE) is an API and is common for all Intel hardwares. Implementation of a function (say multiplication) will be different for different devices but the IE has plugins for each device. The user only needs to specify the target device and IE takes care of other\u00a0things.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/563/1*ctB94cSOclfmEhyKNpCH-w.png\"><figcaption>Different APIs for different hardware\u00a0devices</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/432/1*BJDaRkFGa0DzbtAbNNpcwQ.png\"><figcaption>Inference Engine Work\u00a0flow</figcaption></figure><h4>Heterogeneous Plug-in</h4>\n<p>Above, you saw that you can run same IR on multiple devices. Now, what if I tell you that OpenVINO lets you run different layers on different hardware devices! Yes, OpenVINO supports this and is called\u200a\u2014\u200aHeterogeneous Plug-in.</p>\n<p>Why do we need this?\u200a\u2014\u200aThere might be some layers which is not supported by FPGA but is supported by CPU! If a full network cannot work on one device then we run different layers on different device!</p>\n<p>Won\u2019t it be slow?\u200a\u2014\u200aYeah it might be slow depending on data transfer speed from one device to another. Not all models would be compatible or at times it won\u2019t be suitable to use this because of throughput.</p>\n<p>OpenVINO takes care of precision when switching devices during inference.</p>\n<h3>Performance Benchmarking</h3>\n<p>Assume you have a model to deploy but the hardware is not fixed yet. Now you bring in Intel devices which are candidate deployment devices. Now, by using Performance Benchmarking, you can get the throughput of the model, latency and best data format on each of these devices, hence helping you choose the correct one as per your requirements.</p>\n<p>You can run benchmarking on synchronous and asynchronous mode, enabling sequential and parallel processing.</p>\n<p>Follow this <a href=\"https://www.youtube.com/watch?v=n7HNNR3r4WE&amp;list=PLg-UKERBljNxdIQir1wrirZJ50yTp4eHv&amp;index=28\">video</a> to see how to use Performance Benchmarking.</p>\n<p>I ran Performance Benchmarking on YoloV5s model by passing 128 images with batch size 1. Here is the\u00a0output!</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/746/1*X0jN4l4PyriEhAuzTRqMRQ.png\"></figure><h3>DL Benchmark</h3>\n<p>DL Benchmark provides UI to optimize your model and run several benchmarks. You can set the maximum accuracy drop you would want when optimizing your model! It offers same options just like the command line way but it becomes easy to\u00a0use.</p>\n<p>Watch <a href=\"https://www.youtube.com/watch?v=20ROqz5j1y8&amp;list=PLg-UKERBljNxdIQir1wrirZJ50yTp4eHv&amp;index=38\">this </a>to learn more. I would have attached screenshots here but watching this video makes more sense and gives better explanation.</p>\n<h3>Conclusion</h3>\n<p>Intel OpenVINO has shown that DL models can be run on CPUs efficiently. This will surely bring down deployment cost as CPUs are relatively cheaper than GPUs. There might some trade offs but I am sure we can find our way\u00a0out.</p>\n<p>So, this blog was just to help you get started with Intel OpenVINO. Do checkout the references for videos from where you can learn more about OpenVINO. Also, do some hands on too! It\u2019s easy to install and\u00a0use.</p>\n<p>Connect with me on <a href=\"https://www.linkedin.com/in/sahil-chachra/\">LinkedIn</a> and <a href=\"https://twitter.com/ChachraSahil\">Twitter</a>.\u00a0:)</p>\n<p>Thanks for reading\u00a0:)</p>\n<h3>References</h3>\n<p>[1] OpenVINO Toolkit Tutorial\u200a\u2014\u200a<a href=\"https://www.youtube.com/playlist?list=PLg-UKERBljNxdIQir1wrirZJ50yTp4eHv\">YouTube\u00a0Link</a></p>\n<p>[2] Intel Movidius\u200a\u2014<a href=\"https://www.intel.in/content/www/in/en/products/details/processors/movidius-vpu.html\">\u200aIntel\u2019s\u00a0site</a></p>\n<p>[3] Intel FPGAs\u200a\u2014<a href=\"https://www.intel.com/content/www/us/en/products/programmable.html\">\u200aIntel\u2019s\u00a0site</a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=14a6905f5593\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["deep-learning","ai","openvino","data-science","intel"]},{"title":"Video Analytics Dashboard for YoloV5 and DeepSort","pubDate":"2022-03-27 12:33:21","link":"https://sahilchachra.medium.com/video-analytics-dashboard-for-yolov5-and-deepsort-c5994461cb44?source=rss-f31bf6073414------2","guid":"https://medium.com/p/c5994461cb44","author":"Sahil Chachra","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*8tJP7q8vBADV78E0BzmAKA.png","description":"\n<p>This blog is about a small scale video analytics tool which I built recently using Yolov5, DeepSort and Streamlit. In this blog, I will walk you thourgh the features of the dashboard, show some demos and also discuss future\u00a0scope.</p>\n<h3>Motive</h3>\n<p>As AI engineers, we love data and we love to see graphs and numbers! So why not project the inference data on some platform to understand the inference better? When a model is deployed on the edge for some kind of monitoring, it takes up rigorous amount of frontend and backend developement apart from deep learning efforts\u200a\u2014\u200afrom getting the live data to displaying the correct output. <strong>So, I wanted to replicate a small scale video analytics tool and understand what all feature would be useful for such a tool and what could be the limitations?</strong></p>\n<blockquote>The best part is, we can plug in any custom trained model, be it trained to detect animals, urban city objects, chimenys in factories or different space junks! This dashboard is not domain specific and hence can also be used for testing your custom trained models! So instead of scrolling through the logs in the terminal, you use this dashboard for the same! Further on, this dashboard can be modifed to suit any given use case, be it traffic monitoring, model testing tool or monitoring of aquatic\u00a0animals!</blockquote>\n<h3>Project Components</h3>\n<ol>\n<li>Inference Configuration</li>\n<li>Inference</li>\n<li>Inference stats</li>\n<li>System stats</li>\n<li>Inference overview</li>\n</ol>\n<h4>Inference Configuration</h4>\n<p>This component deals with the entire settings of the tool, giving you several options for <strong>input source</strong>(can be local video, rtsp or webcam), <strong>class</strong> <strong>confidence value</strong>, <strong>threshold for drift detection</strong>, <strong>threhold for fps drop warning</strong>,<strong>saving output videos</strong> and <strong>saving frames on which the model is not performing</strong> well.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*8tJP7q8vBADV78E0BzmAKA.png\"><figcaption>Left sidebar is Configuration panel</figcaption></figure><h4>Inference</h4>\n<p>This inference which you see is output of YoloV5s version 6 with DeepSort to track the objects. We have label and then class confidence value. In the box we have the id for the object. For example, here we have a car with id\u00a02.</p>\n<p>Check references for\u00a0links.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/914/1*ot_IUCnRclM51QDsgGLYIQ.png\"><figcaption>Inference View</figcaption></figure><h4>Inference stats</h4>\n<p>Inference stats shows the FPS, detected objects in current frame and total objects detected so far (not tracked).</p>\n<p>These stats can help in multiple way\u200a\u2014\u200asuch\u00a0as,</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/867/1*joYT7UnaZps7ZiAIO0vTdw.png\"><figcaption>Inference Stats</figcaption></figure><p><strong>The tool will alert when FPS falls below given threshold</strong>. This can help keep an eye on the performance of system to decide whether or not to upgrade the hardware or optimise model. It may happen that with time, number of objects are slowly increasing and the model is becoming slow in detections and hence would lead to drop in FPS. It may also happen that, during certain time of the day when there is usual surge in number of objects the FPS reduces indicating optimizing the model or upgrading the hardware being\u00a0used.</p>\n<p>As as add-on, we can store number of vehicles being driven on certain road for given time duration. For eg, from 6AM to 8AM there is heavy traffic and hence requires the person monitoring the traffic to be vigilant. Then after 8AM, the traffic reduces. This data can also be sent to local body to help them schedule road\u00a0repairs.</p>\n<p>Total detected objects data can help when we are collecting data for retraining the model as well. This count can help us prevent class imbalance. For eg, we want to retrain our model due to data drift (Government has added new electric buses for which model is getting confused with small vans hence leading to change in data being infered on), so this count can help us plan our training process to prevent overfitting on one single class. Also, for eg, if during the entire day there were 100 buses on the road being driven along side the cars, then the transport body can think of providing a seperate lane for buses to help regulate\u00a0traffic.</p>\n<h4>System stats</h4>\n<p>System stats help monitor CPU, RAM and GPU usage. It helps keep an eye on the hardware if it\u2019s being under or over utilized. One can decide on upgrading the GPU if GPU utilization peaks frequently when there is a sudden surge of objects in the camera. Similarly this is applicable to memory and CPU as\u00a0well.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/815/1*I20XjFW0pnugfoXjaTUm8g.png\"><figcaption>System Stats</figcaption></figure><h4>Inference overview</h4>\n<p>In this section, we get summary of the inference, for example -&gt; classes on which the model is not performing well, number of frames for which atleast one object was detected with confidence less than threshold value, minimum and maximum\u00a0FPS.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/824/1*xc6jaUrJaXM5XkO2JegYMw.png\"><figcaption>Inference Overview</figcaption></figure><h3>Demo</h3>\n<p>The following video is a demo of the dashboard using a video from local\u00a0device.</p>\n<a href=\"https://medium.com/media/5d840c10ba30917602d77f7bf90e01d4/href\">https://medium.com/media/5d840c10ba30917602d77f7bf90e01d4/href</a><h3>Epilogue</h3>\n<p>This was a small walk through of the tool and this can be modified to something outstanding. We can add different types of graphs and stats from the device or inference. We can also use optimized model which will inturn give higher\u00a0FPS.</p>\n<p>One can do futher research to understand model and data drift. Here, I just tried to implement the definition of data drift. Drift detection in production can help in prior upgradation of the\u00a0model.</p>\n<p>Thankyou for reading my blog\u00a0:). Hope you learnt something new!<br>For more such blogs, do follow me on Medium. I post blogs every month. Lets connect on <a href=\"https://www.linkedin.com/in/sahil-chachra/\">LinkedIn</a> as well\u00a0:)</p>\n<h3>Code</h3>\n<p>Here is the code. If you like the project do give it a star on GitHub.\u00a0:).</p>\n<p>GitHub\u200a\u2014\u200a<a href=\"https://github.com/SahilChachra/Video-Analytics-Dashboard\">Link</a></p>\n<p>(Also, I am running this project locally, having 4Gb Nvidia 1650 and 16Gb ram. I\u2019ll optimize the code going forward\u00a0:)\u00a0)</p>\n<h3>References</h3>\n<p>[1] YoloV5 by Ultralytics \u2014<a href=\"https://github.com/ultralytics/yolov5\">Link</a></p>\n<p>[2] DeepSort\u200a\u2014\u200a<a href=\"https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch\">Link</a></p>\n<p>[3] Streamlit\u200a\u2014\u200a<a href=\"https://docs.streamlit.io/\">Documentation</a></p>\n<p>[4] Input video source\u2014\u00a0<a href=\"https://www.pexels.com/video/1548730/\">Video</a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c5994461cb44\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>This blog is about a small scale video analytics tool which I built recently using Yolov5, DeepSort and Streamlit. In this blog, I will walk you thourgh the features of the dashboard, show some demos and also discuss future\u00a0scope.</p>\n<h3>Motive</h3>\n<p>As AI engineers, we love data and we love to see graphs and numbers! So why not project the inference data on some platform to understand the inference better? When a model is deployed on the edge for some kind of monitoring, it takes up rigorous amount of frontend and backend developement apart from deep learning efforts\u200a\u2014\u200afrom getting the live data to displaying the correct output. <strong>So, I wanted to replicate a small scale video analytics tool and understand what all feature would be useful for such a tool and what could be the limitations?</strong></p>\n<blockquote>The best part is, we can plug in any custom trained model, be it trained to detect animals, urban city objects, chimenys in factories or different space junks! This dashboard is not domain specific and hence can also be used for testing your custom trained models! So instead of scrolling through the logs in the terminal, you use this dashboard for the same! Further on, this dashboard can be modifed to suit any given use case, be it traffic monitoring, model testing tool or monitoring of aquatic\u00a0animals!</blockquote>\n<h3>Project Components</h3>\n<ol>\n<li>Inference Configuration</li>\n<li>Inference</li>\n<li>Inference stats</li>\n<li>System stats</li>\n<li>Inference overview</li>\n</ol>\n<h4>Inference Configuration</h4>\n<p>This component deals with the entire settings of the tool, giving you several options for <strong>input source</strong>(can be local video, rtsp or webcam), <strong>class</strong> <strong>confidence value</strong>, <strong>threshold for drift detection</strong>, <strong>threhold for fps drop warning</strong>,<strong>saving output videos</strong> and <strong>saving frames on which the model is not performing</strong> well.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*8tJP7q8vBADV78E0BzmAKA.png\"><figcaption>Left sidebar is Configuration panel</figcaption></figure><h4>Inference</h4>\n<p>This inference which you see is output of YoloV5s version 6 with DeepSort to track the objects. We have label and then class confidence value. In the box we have the id for the object. For example, here we have a car with id\u00a02.</p>\n<p>Check references for\u00a0links.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/914/1*ot_IUCnRclM51QDsgGLYIQ.png\"><figcaption>Inference View</figcaption></figure><h4>Inference stats</h4>\n<p>Inference stats shows the FPS, detected objects in current frame and total objects detected so far (not tracked).</p>\n<p>These stats can help in multiple way\u200a\u2014\u200asuch\u00a0as,</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/867/1*joYT7UnaZps7ZiAIO0vTdw.png\"><figcaption>Inference Stats</figcaption></figure><p><strong>The tool will alert when FPS falls below given threshold</strong>. This can help keep an eye on the performance of system to decide whether or not to upgrade the hardware or optimise model. It may happen that with time, number of objects are slowly increasing and the model is becoming slow in detections and hence would lead to drop in FPS. It may also happen that, during certain time of the day when there is usual surge in number of objects the FPS reduces indicating optimizing the model or upgrading the hardware being\u00a0used.</p>\n<p>As as add-on, we can store number of vehicles being driven on certain road for given time duration. For eg, from 6AM to 8AM there is heavy traffic and hence requires the person monitoring the traffic to be vigilant. Then after 8AM, the traffic reduces. This data can also be sent to local body to help them schedule road\u00a0repairs.</p>\n<p>Total detected objects data can help when we are collecting data for retraining the model as well. This count can help us prevent class imbalance. For eg, we want to retrain our model due to data drift (Government has added new electric buses for which model is getting confused with small vans hence leading to change in data being infered on), so this count can help us plan our training process to prevent overfitting on one single class. Also, for eg, if during the entire day there were 100 buses on the road being driven along side the cars, then the transport body can think of providing a seperate lane for buses to help regulate\u00a0traffic.</p>\n<h4>System stats</h4>\n<p>System stats help monitor CPU, RAM and GPU usage. It helps keep an eye on the hardware if it\u2019s being under or over utilized. One can decide on upgrading the GPU if GPU utilization peaks frequently when there is a sudden surge of objects in the camera. Similarly this is applicable to memory and CPU as\u00a0well.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/815/1*I20XjFW0pnugfoXjaTUm8g.png\"><figcaption>System Stats</figcaption></figure><h4>Inference overview</h4>\n<p>In this section, we get summary of the inference, for example -&gt; classes on which the model is not performing well, number of frames for which atleast one object was detected with confidence less than threshold value, minimum and maximum\u00a0FPS.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/824/1*xc6jaUrJaXM5XkO2JegYMw.png\"><figcaption>Inference Overview</figcaption></figure><h3>Demo</h3>\n<p>The following video is a demo of the dashboard using a video from local\u00a0device.</p>\n<a href=\"https://medium.com/media/5d840c10ba30917602d77f7bf90e01d4/href\">https://medium.com/media/5d840c10ba30917602d77f7bf90e01d4/href</a><h3>Epilogue</h3>\n<p>This was a small walk through of the tool and this can be modified to something outstanding. We can add different types of graphs and stats from the device or inference. We can also use optimized model which will inturn give higher\u00a0FPS.</p>\n<p>One can do futher research to understand model and data drift. Here, I just tried to implement the definition of data drift. Drift detection in production can help in prior upgradation of the\u00a0model.</p>\n<p>Thankyou for reading my blog\u00a0:). Hope you learnt something new!<br>For more such blogs, do follow me on Medium. I post blogs every month. Lets connect on <a href=\"https://www.linkedin.com/in/sahil-chachra/\">LinkedIn</a> as well\u00a0:)</p>\n<h3>Code</h3>\n<p>Here is the code. If you like the project do give it a star on GitHub.\u00a0:).</p>\n<p>GitHub\u200a\u2014\u200a<a href=\"https://github.com/SahilChachra/Video-Analytics-Dashboard\">Link</a></p>\n<p>(Also, I am running this project locally, having 4Gb Nvidia 1650 and 16Gb ram. I\u2019ll optimize the code going forward\u00a0:)\u00a0)</p>\n<h3>References</h3>\n<p>[1] YoloV5 by Ultralytics \u2014<a href=\"https://github.com/ultralytics/yolov5\">Link</a></p>\n<p>[2] DeepSort\u200a\u2014\u200a<a href=\"https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch\">Link</a></p>\n<p>[3] Streamlit\u200a\u2014\u200a<a href=\"https://docs.streamlit.io/\">Documentation</a></p>\n<p>[4] Input video source\u2014\u00a0<a href=\"https://www.pexels.com/video/1548730/\">Video</a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c5994461cb44\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["yolov5","nvidia","object-detection","ai","streamlit"]},{"title":"Dive into basics of GPU, CUDA &amp; Accelerated programming using Numba in Python","pubDate":"2022-02-08 08:32:34","link":"https://sahilchachra.medium.com/dive-into-basics-of-gpu-cuda-accelerated-programming-using-numba-in-python-a0be21aa00b7?source=rss-f31bf6073414------2","guid":"https://medium.com/p/a0be21aa00b7","author":"Sahil Chachra","thumbnail":"https://cdn-images-1.medium.com/max/620/1*wXVTl_nL3BdfSCSxaSzsAg.png","description":"\n<p>In this blog, I will talk about basics of GPU, CUDA and Numba. I will also briefly discuss how using Numba makes a noticable difference in day-to-day code both on CPU and\u00a0GPU.</p>\n<p>This blog is not a Numba tutorial. To learn Numba follow this\u200a\u2014\u200a<a href=\"https://www.kaggle.com/general/274291\">Kaggle Notebook</a> (Even I learnt it from here). This blog will be more focused on the underlying theory.</p>\n<p>The main motive behind the blog\u200a\u2014\u200aDeep learning practioners often talk about GPU\u2019s capability and which GPU one can use given the use case and usage. Being in this field it is important to understand the components of the GPU and CUDA as we use both of them every single\u00a0day!</p>\n<p>I have attached all the resources which I have referred to, to understand the below topics. Feel free to explore those links\u00a0too!</p>\n<h3>Table of\u00a0Contents</h3>\n<ol>\n<li>Brief intro to\u00a0Numba</li>\n<li>What is CUDA\u00a0?</li>\n<li>CUDA Kernels</li>\n<li>Thread, Block, Grid and Wrap in\u00a0CUDA</li>\n<li>Streaming Multiprocessors</li>\n<li>SMIT Architecture</li>\n<li>Coalesced Memory\u00a0Access</li>\n<li>Bank conflicts</li>\n<li>Numba on CPU and GPU (with\u00a0CUDA)</li>\n</ol>\n<h3>1. Brief intro to\u00a0Numba</h3>\n<p>Numba is just-in-time compiler (just-in-time means the code is compiled during runtime instead of compiling it before hand.) for Python best suited for functions involving numpy arrays (and their calculations), lengthy mathematical operations and\u00a0loops.</p>\n<p>When using Numba, it works only with functions, i.e, it doesn\u2019t optimize the entire Python application or a part of code in the function. It is not a replacement for Python\u2019s interpreter but it helps it speed up the execution. Numba accelerates the code by specifying data type to the variable we use. In Python, we don\u2019t mention int or float. Python\u2019s interpreter does that itself while compiling which makes it slow. So Numba assigns data type such as int32 or float32 before\u00a0hand.</p>\n<p>However, Numba cannot optimize all the code we write meaning it doesn\u2019t work with certain data types. Refer to the documentation (Link in References).</p>\n<p>Alternative to Numba is pyCUDA and CUDA in C/C++. But one of the main advantages of Numba is that is accelerates code for CPU also whereas other two are specific to Nvidia\u00a0GPUs.</p>\n<h3>2. What is CUDA &amp; why use\u00a0it?</h3>\n<p>CUDA stands for Compute Unified Device Architecture. It is an API and is a parallel computing platform. It is specific to Nvidia\u2019s\u00a0GPU.</p>\n<p>Wait wait, what is parallel computing platform? It is type of architecture for computing where multiple processors simultaneously execute multiple calculations which are break down of complex large problems.</p>\n<p>GPUs, in general, are used where parallel computation is required.</p>\n<p><strong>What are CUDA cores? </strong>CUDA cores are nothing but number of cores in Nvidia\u2019s GPUs (Nvidia -&gt; CUDA). These are high-tech cores specializing in parallel computing which perform complex operations/calculations. Greater the number of cores, faster the computation!</p>\n<p><strong>Why do we use CUDA?</strong> For complex calculations, which can be broken down into smaller problems and each sub-problem is independent of other sub-problem\u2019s result, here in my friend parallel computing comes into the picture. To compute such huge number of sub-problems faster and parallelly, you bring in GPUs which have way too many cores compared to a CPU. Now each sub-problem can be assumed as a individual task being computed by each core of the GPU. Since you depend on the GPU to complete the task at lightening speed, your code should also be in a form that the it expects to be in. Now how will you code an operation in a way the GPU expects it? So, CUDA is an API &amp; programming language by Nvidia, which works on Nvidia\u2019s own GPUs to help you run your code (say written in Python or C++) (for example convolution operation) on their GPUs efficiently. To use CUDA in our code, we install CUDA\u00a0toolkit.</p>\n<h3>3. CUDA\u00a0Kernels</h3>\n<p>A function which is supposed to run on GPU is called kernel. In order to designate a function as kernel we use something called as function qualifiers. These are similar to decorators in Python. Function qualifiers are placed just above the function.</p>\n<p>Few function qualifiers are (in C/C++)\u00a0\u2014</p>\n<pre> __global__ - functions marked with this qualifier becomes kernel. This means the kernel can be called from a host and will run on device</pre>\n<pre>__device__ - functions marked with this qualifier denotes that the function/kernel can only be called by a device and will run on the device (device = GPU)</pre>\n<p>How does a simple kernel looks like with Numba? Following is an example where we multiple numbers by\u00a02</p>\n<pre>from numba import cuda<br>import numpy as np</pre>\n<pre>@cuda.jit # decorator to run function on GPU using CUDA<br>def multiply_kernel(x, out):<br>    idx = cuda.grid(1) # create 1-D grid of threads<br>    out[idx] = x[idx] * 2  # in each thread, save value of x[i]*2</pre>\n<pre>n = 4096<br>x = np.arange(n).astype(np.int32)<br>d_x = cuda.to_device(x)<br>d_out = cuda.device_array_like(d_x) # create output array</pre>\n<pre>blocks_per_grid= 32  # number blocks in each grid<br>threads_per_block = 128  # number threads in each block</pre>\n<pre>multiply_kernel[blocks_per_grid, threads_per_block](d_x, d_out)<br>cuda.synchronize() # Wait for GPU to complete the task<br>print(d_out.copy_to_host()) # copy data from GPU to CPU/Host</pre>\n<p>Output will be something like this\u00a0: [0, 2, 4, 6,\u00a0\u2026\u00a0,\u00a08192]</p>\n<h3>4. Threads, Blocks, Grid and Wrap in\u00a0CUDA</h3>\n<p>Threads\u200a\u2014\u200aThreads are single execution unit that run your\u00a0kernels.</p>\n<p>Blocks\u200a\u2014\u200aSeveral threads together form a Block. There are max 1024 threads in each\u00a0block.</p>\n<p>A GPU can run some number of blocks parallelly. A very good GPU can use 10 blocks to complete your task in 10mins and an average GPU could use only 2 blocks concurrently to complete your task in 20mins. The number of threads in each block remain the same, that is 1024. So this means that the same code can run on different GPUs without needed a single change in the code. Thanks to\u00a0Nvidia!</p>\n<p>Grid\u200a\u2014\u200aSeveral blocks forms a\u00a0Grid</p>\n<p>Warp\u200a\u2014\u200aTo perform any task, threads require resources. Streaming Multiprocessors don\u2019t directly assign resources to the threads individually. Instead they divide threads into groups of 32 (maximum, can be less also) called Wraps and then assign resources to execute any task. You can now say that a Block has many\u00a0Wraps.</p>\n<p>Half-warp\u200a\u2014\u200aOnly 16 threads will be executed on the SM at any given time hence we consider half-wrap as unit of memory accesses. (Applicable to architectures before\u00a0Fermi)</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/620/1*wXVTl_nL3BdfSCSxaSzsAg.png\"><figcaption>Diagram referred from <a href=\"https://www.researchgate.net/figure/CUDA-memory-architecture_fig3_327646625\">Research\u00a0gate</a></figcaption></figure><h3>5. Streaming Multiprocessors</h3>\n<p>Our kernels are executed by SMs or Streaming Multiprocessors. A GPU consist of several of them. These run at a lower clock rate and have small cache. It\u2019s primary task is to execute several threads blocks parallelly. When one of it\u2019s thread block has completed it\u2019s execution, SM picks the next thread block serially.</p>\n<blockquote>From Nvidia\u2019s documentation\u200a\u2014<em>\u200aWhen a CUDA program on the host CPU invokes a kernel grid, the blocks of the grid are enumerated and distributed to multiprocessors with available execution capacity. The threads of a thread block execute concurrently on one multiprocessor, and multiple thread blocks can execute concurrently on one multiprocessor. As thread blocks terminate, new blocks are launched on the vacated multiprocessors.</em>\n</blockquote>\n<p>Each SM has\u200a\u2014\u200aSeveral caches (L1 cache, Constant cache, Texture cache), Shared memory, Wrap schedulers and Execution cores for floating-point and integer operations. The shared memory on the SM is divided between thread blocks on the SM. If shared memory is not accessed properly by the threads, it leads to \u2018bank conflict\u2019 where threads queue behind each other and hence the performance drops. One SM can execute more than one block at any given\u00a0time.</p>\n<h3>6. SMIT Architecture</h3>\n<p>I found the best explanation in Nvidia\u2019s own documentation hence adding it here. <strong>As mentioned in Nvidia\u2019s CUDA documentation </strong>(See references\u200a\u2014\u200a4), <strong>(quoting from section\u00a0: Hardware Implementation)</strong></p>\n<blockquote>\u201cA multiprocessor is designed to execute hundreds of threads concurrently. To manage such a large amount of threads, it employs a unique architecture called SIMT (Single-Instruction, Multiple-Thread) that is described in SIMT Architecture. The instructions are pipelined, leveraging instruction-level parallelism within a single thread, as well as extensive thread-level parallelism through simultaneous hardware multithreading as detailed in Hardware Multithreading. Unlike CPU cores, they are issued in order and there is no branch prediction or speculative execution.\u201d</blockquote>\n<h3>7. Coalesced Memory\u00a0Access</h3>\n<p>When a given set of threads read/write simultaneously from contiguous block of global memory in a single transaction, it is known as coalesced (can pronounce as\u00a0: coalesd) memory access. <em>Coalesced memory access applies only to GPUs with CUDA compute capability of 1.x and 2. Newer GPUs use more complex ways to access the global\u00a0memory.</em></p>\n<h3>8. Bank conflicts</h3>\n<p>What are banks?\u200a\u2014\u200aThe shared memory that can be accessed in parallel are divided into modules called\u00a0Banks.</p>\n<p>In shared memory, 4 bytes is called 1 word. There are 32 banks (in modern cards) in shared memory in Nvidia GPU. Each successive word of shared memory belong to a bank. This means word 0 belongs to bank 0, word 1 belongs to bank 1 and so on. A wrap can send request to any permutation of these 32 banks. While dividing shared memory into banks, if any of the memory address occurs in 2 banks then it causes bank conflict.</p>\n<p>If all the threads in a wrap wants to read value from a single bank then only once the value will be read and it will be shared among the threads. This is called <em>broadcast.</em></p>\n<h3>9. Numba on CPU and\u00a0GPU</h3>\n<h4>On CPU</h4>\n<p>While estimating value of PI using Monte Carlo method, we pass the range as 10k. On CPU without numba it took 10.8ms (mean of 7 runs, 100 loops each) whereas with Numba it took 237\u03bcs (mean of 7 runs, 100 loops each). That is 45 times\u00a0faster!</p>\n<pre><a href=\"http://twitter.com/jit\">@jit</a>(nopython=True)<br>def monte_carlo_pi(n):<br>    acc = 0<br>    for i in range(n):<br>        x = random.random()<br>        y = random.random()<br>        if (x**2 + y**2) &lt; 1.0:<br>            acc += 1<br>    return 4.0 * acc / n</pre>\n<pre>%timeit monte_carlo_pi(10000)</pre>\n<p>This is an example code from <a href=\"https://analyticsindiamag.com/make-python-code-faster-with-numba/\">Analytics Vidhya Numba tutorial</a>. To accelerate your code with Numba, its best to refer the documentation and modify your functions accordingly such that it only performs complex calculations. Due to limited types supported by Numba, creating such small functions will help you fully utilize it\u2019s potential.</p>\n<h4>On GPU</h4>\n<pre><a href=\"http://twitter.com/cuda\">@cuda</a>.jit<br>def add_kernel(x, y, out):<br>    idx = cuda.grid(1)<br>    out[idx] = x[idx] + y[idx]</pre>\n<pre>n = 4096<br>x = np.arange(n).astype(np.int32) <br>y = np.ones_like(x)   </pre>\n<pre>d_x = cuda.to_device(x) <br>d_y = cuda.to_device(y) <br>d_out = cuda.device_array_like(d_x) </pre>\n<pre>threads_per_block = 128<br>blocks_per_grid = 32</pre>\n<p>This addition function on CUDA takes 100\u03bcs (mean of 7 runs, 10000 loops each) and a novice addition code for same input takes 880\u03bcs (mean of 7 runs, 1000 loops each). Same operation on CUDA is 8.8 times\u00a0faster!</p>\n<p>But the time taken by the CUDA function is only for the calculation it has done meaning here we do not consider time taken to move data from host memory to GPU memory and then moving back output from GPU memory to the host memory.<br>Still, I think we can use it with functions where we perform complex calculations using Numpy or basic math operations. If that function is called every single second say while inferencing a model, then it would make a significant improvement in overall performance!</p>\n<h3>Conclusion</h3>\n<p>It was fun exploring basic concepts of GPU and CUDA. Also, by getting exposed to Numba, it is clear that we can accelerate code on CPU as well. Maybe when running inference on CPU, Numba might help accelerate some part of the pipeline!</p>\n<p>Follow the Kaggle notebook shared above to explore Numba\u00a0:).<br>Thanks for\u00a0reading.</p>\n<p>Follow me for more such blogs! Also connect with me on <a href=\"https://www.linkedin.com/in/sahil-chachra/\">LinkedIn</a>.</p>\n<h3>References</h3>\n<ol>\n<li><a href=\"https://numba.readthedocs.io/en/stable/user/5minguide.html\">https://numba.readthedocs.io/en/stable/user/5minguide.html</a></li>\n<li><a href=\"https://www.techopedia.com/definition/3978/just-in-time-compiler-jit-compiler#:~:text=A%20just-in-time%20(,fly%20as%20the%20program%20executes.\">https://www.techopedia.com/definition/3978/just-in-time-compiler-jit-compiler#:~:text=A%20just%2Din%2Dtime%20(,fly%20as%20the%20program%20executes.</a></li>\n<li><a href=\"https://www.kaggle.com/harshwalia/1-introduction-to-cuda-python-with-numba\">https://www.kaggle.com/harshwalia/1-introduction-to-cuda-python-with-numba</a></li>\n<li><a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html\">https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html</a></li>\n<li><a href=\"https://www.youtube.com/playlist?list=PLKK11Ligqititws0ZOoGk3SW-TZCar4dK\">https://www.youtube.com/playlist?list=PLKK11Ligqititws0ZOoGk3SW-TZCar4dK</a></li>\n<li><a href=\"https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/\">https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/</a></li>\n<li><a href=\"https://deeplizard.com/learn/video/6stDhEA0wFQ\">https://deeplizard.com/learn/video/6stDhEA0wFQ</a></li>\n<li><a href=\"https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/\">https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/</a></li>\n<li><a href=\"http://thebeardsage.com/cuda-streaming-multiprocessors/\">http://thebeardsage.com/cuda-streaming-multiprocessors/</a></li>\n<li><a href=\"https://stackoverflow.com/questions/32226993/understanding-streaming-multiprocessors-sm-and-streaming-processors-sp\">https://stackoverflow.com/questions/32226993/understanding-streaming-multiprocessors-sm-and-streaming-processors-sp</a></li>\n<li><a href=\"https://stackoverflow.com/questions/2207171/help-me-understand-cuda/2213744#2213744\">https://stackoverflow.com/questions/2207171/help-me-understand-cuda/2213744#2213744</a></li>\n<li>SMs\u200a\u2014\u200a<a href=\"https://stackoverflow.com/questions/3519598/streaming-multiprocessors-blocks-and-threads-cuda\">https://stackoverflow.com/questions/3519598/streaming-multiprocessors-blocks-and-threads-cuda</a>\n</li>\n<li><a href=\"https://stackoverflow.com/questions/10460742/how-do-cuda-blocks-warps-threads-map-onto-cuda-cores\">https://stackoverflow.com/questions/10460742/how-do-cuda-blocks-warps-threads-map-onto-cuda-cores</a></li>\n<li><a href=\"https://cvw.cac.cornell.edu/gpu/coalesced\">https://cvw.cac.cornell.edu/gpu/coalesced</a></li>\n<li>Half wrap\u200a\u2014\u200a<a href=\"https://forums.developer.nvidia.com/t/why-only-half-warp/15915/6\">https://forums.developer.nvidia.com/t/why-only-half-warp/15915/6</a>\n</li>\n<li>Bank conflic\u200a\u2014\u200a<a href=\"https://stackoverflow.com/questions/3841877/what-is-a-bank-conflict-doing-cuda-opencl-programming#:~:text=The%20shared%20memory%20that%20can,the%20advantages%20of%20parallel%20access\">https://stackoverflow.com/questions/3841877/what-is-a-bank-conflict-doing-cuda-opencl-programming#:~:text=The%20shared%20memory%20that%20can,the%20advantages%20of%20parallel%20access</a>.</li>\n<li>Bank conflict\u200a\u2014\u200a<a href=\"https://www.youtube.com/watch?v=CZgM3DEBplE\">https://www.youtube.com/watch?v=CZgM3DEBplE</a>\n</li>\n<li>Bank conflict\u200a\u2014\u200a<a href=\"https://www.generacodice.com/en/articolo/666174/%C2%BFqu%C3%A9-es-un-conflicto-banco?-(hacer-la-programaci%C3%B3n-cuda-/-opencl)\">https://www.generacodice.com/en/articolo/666174/%C2%BFqu%C3%A9-es-un-conflicto-banco?-(hacer-la-programaci%C3%B3n-cuda-/-opencl)</a>\n</li>\n</ol>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a0be21aa00b7\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>In this blog, I will talk about basics of GPU, CUDA and Numba. I will also briefly discuss how using Numba makes a noticable difference in day-to-day code both on CPU and\u00a0GPU.</p>\n<p>This blog is not a Numba tutorial. To learn Numba follow this\u200a\u2014\u200a<a href=\"https://www.kaggle.com/general/274291\">Kaggle Notebook</a> (Even I learnt it from here). This blog will be more focused on the underlying theory.</p>\n<p>The main motive behind the blog\u200a\u2014\u200aDeep learning practioners often talk about GPU\u2019s capability and which GPU one can use given the use case and usage. Being in this field it is important to understand the components of the GPU and CUDA as we use both of them every single\u00a0day!</p>\n<p>I have attached all the resources which I have referred to, to understand the below topics. Feel free to explore those links\u00a0too!</p>\n<h3>Table of\u00a0Contents</h3>\n<ol>\n<li>Brief intro to\u00a0Numba</li>\n<li>What is CUDA\u00a0?</li>\n<li>CUDA Kernels</li>\n<li>Thread, Block, Grid and Wrap in\u00a0CUDA</li>\n<li>Streaming Multiprocessors</li>\n<li>SMIT Architecture</li>\n<li>Coalesced Memory\u00a0Access</li>\n<li>Bank conflicts</li>\n<li>Numba on CPU and GPU (with\u00a0CUDA)</li>\n</ol>\n<h3>1. Brief intro to\u00a0Numba</h3>\n<p>Numba is just-in-time compiler (just-in-time means the code is compiled during runtime instead of compiling it before hand.) for Python best suited for functions involving numpy arrays (and their calculations), lengthy mathematical operations and\u00a0loops.</p>\n<p>When using Numba, it works only with functions, i.e, it doesn\u2019t optimize the entire Python application or a part of code in the function. It is not a replacement for Python\u2019s interpreter but it helps it speed up the execution. Numba accelerates the code by specifying data type to the variable we use. In Python, we don\u2019t mention int or float. Python\u2019s interpreter does that itself while compiling which makes it slow. So Numba assigns data type such as int32 or float32 before\u00a0hand.</p>\n<p>However, Numba cannot optimize all the code we write meaning it doesn\u2019t work with certain data types. Refer to the documentation (Link in References).</p>\n<p>Alternative to Numba is pyCUDA and CUDA in C/C++. But one of the main advantages of Numba is that is accelerates code for CPU also whereas other two are specific to Nvidia\u00a0GPUs.</p>\n<h3>2. What is CUDA &amp; why use\u00a0it?</h3>\n<p>CUDA stands for Compute Unified Device Architecture. It is an API and is a parallel computing platform. It is specific to Nvidia\u2019s\u00a0GPU.</p>\n<p>Wait wait, what is parallel computing platform? It is type of architecture for computing where multiple processors simultaneously execute multiple calculations which are break down of complex large problems.</p>\n<p>GPUs, in general, are used where parallel computation is required.</p>\n<p><strong>What are CUDA cores? </strong>CUDA cores are nothing but number of cores in Nvidia\u2019s GPUs (Nvidia -&gt; CUDA). These are high-tech cores specializing in parallel computing which perform complex operations/calculations. Greater the number of cores, faster the computation!</p>\n<p><strong>Why do we use CUDA?</strong> For complex calculations, which can be broken down into smaller problems and each sub-problem is independent of other sub-problem\u2019s result, here in my friend parallel computing comes into the picture. To compute such huge number of sub-problems faster and parallelly, you bring in GPUs which have way too many cores compared to a CPU. Now each sub-problem can be assumed as a individual task being computed by each core of the GPU. Since you depend on the GPU to complete the task at lightening speed, your code should also be in a form that the it expects to be in. Now how will you code an operation in a way the GPU expects it? So, CUDA is an API &amp; programming language by Nvidia, which works on Nvidia\u2019s own GPUs to help you run your code (say written in Python or C++) (for example convolution operation) on their GPUs efficiently. To use CUDA in our code, we install CUDA\u00a0toolkit.</p>\n<h3>3. CUDA\u00a0Kernels</h3>\n<p>A function which is supposed to run on GPU is called kernel. In order to designate a function as kernel we use something called as function qualifiers. These are similar to decorators in Python. Function qualifiers are placed just above the function.</p>\n<p>Few function qualifiers are (in C/C++)\u00a0\u2014</p>\n<pre> __global__ - functions marked with this qualifier becomes kernel. This means the kernel can be called from a host and will run on device</pre>\n<pre>__device__ - functions marked with this qualifier denotes that the function/kernel can only be called by a device and will run on the device (device = GPU)</pre>\n<p>How does a simple kernel looks like with Numba? Following is an example where we multiple numbers by\u00a02</p>\n<pre>from numba import cuda<br>import numpy as np</pre>\n<pre>@cuda.jit # decorator to run function on GPU using CUDA<br>def multiply_kernel(x, out):<br>    idx = cuda.grid(1) # create 1-D grid of threads<br>    out[idx] = x[idx] * 2  # in each thread, save value of x[i]*2</pre>\n<pre>n = 4096<br>x = np.arange(n).astype(np.int32)<br>d_x = cuda.to_device(x)<br>d_out = cuda.device_array_like(d_x) # create output array</pre>\n<pre>blocks_per_grid= 32  # number blocks in each grid<br>threads_per_block = 128  # number threads in each block</pre>\n<pre>multiply_kernel[blocks_per_grid, threads_per_block](d_x, d_out)<br>cuda.synchronize() # Wait for GPU to complete the task<br>print(d_out.copy_to_host()) # copy data from GPU to CPU/Host</pre>\n<p>Output will be something like this\u00a0: [0, 2, 4, 6,\u00a0\u2026\u00a0,\u00a08192]</p>\n<h3>4. Threads, Blocks, Grid and Wrap in\u00a0CUDA</h3>\n<p>Threads\u200a\u2014\u200aThreads are single execution unit that run your\u00a0kernels.</p>\n<p>Blocks\u200a\u2014\u200aSeveral threads together form a Block. There are max 1024 threads in each\u00a0block.</p>\n<p>A GPU can run some number of blocks parallelly. A very good GPU can use 10 blocks to complete your task in 10mins and an average GPU could use only 2 blocks concurrently to complete your task in 20mins. The number of threads in each block remain the same, that is 1024. So this means that the same code can run on different GPUs without needed a single change in the code. Thanks to\u00a0Nvidia!</p>\n<p>Grid\u200a\u2014\u200aSeveral blocks forms a\u00a0Grid</p>\n<p>Warp\u200a\u2014\u200aTo perform any task, threads require resources. Streaming Multiprocessors don\u2019t directly assign resources to the threads individually. Instead they divide threads into groups of 32 (maximum, can be less also) called Wraps and then assign resources to execute any task. You can now say that a Block has many\u00a0Wraps.</p>\n<p>Half-warp\u200a\u2014\u200aOnly 16 threads will be executed on the SM at any given time hence we consider half-wrap as unit of memory accesses. (Applicable to architectures before\u00a0Fermi)</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/620/1*wXVTl_nL3BdfSCSxaSzsAg.png\"><figcaption>Diagram referred from <a href=\"https://www.researchgate.net/figure/CUDA-memory-architecture_fig3_327646625\">Research\u00a0gate</a></figcaption></figure><h3>5. Streaming Multiprocessors</h3>\n<p>Our kernels are executed by SMs or Streaming Multiprocessors. A GPU consist of several of them. These run at a lower clock rate and have small cache. It\u2019s primary task is to execute several threads blocks parallelly. When one of it\u2019s thread block has completed it\u2019s execution, SM picks the next thread block serially.</p>\n<blockquote>From Nvidia\u2019s documentation\u200a\u2014<em>\u200aWhen a CUDA program on the host CPU invokes a kernel grid, the blocks of the grid are enumerated and distributed to multiprocessors with available execution capacity. The threads of a thread block execute concurrently on one multiprocessor, and multiple thread blocks can execute concurrently on one multiprocessor. As thread blocks terminate, new blocks are launched on the vacated multiprocessors.</em>\n</blockquote>\n<p>Each SM has\u200a\u2014\u200aSeveral caches (L1 cache, Constant cache, Texture cache), Shared memory, Wrap schedulers and Execution cores for floating-point and integer operations. The shared memory on the SM is divided between thread blocks on the SM. If shared memory is not accessed properly by the threads, it leads to \u2018bank conflict\u2019 where threads queue behind each other and hence the performance drops. One SM can execute more than one block at any given\u00a0time.</p>\n<h3>6. SMIT Architecture</h3>\n<p>I found the best explanation in Nvidia\u2019s own documentation hence adding it here. <strong>As mentioned in Nvidia\u2019s CUDA documentation </strong>(See references\u200a\u2014\u200a4), <strong>(quoting from section\u00a0: Hardware Implementation)</strong></p>\n<blockquote>\u201cA multiprocessor is designed to execute hundreds of threads concurrently. To manage such a large amount of threads, it employs a unique architecture called SIMT (Single-Instruction, Multiple-Thread) that is described in SIMT Architecture. The instructions are pipelined, leveraging instruction-level parallelism within a single thread, as well as extensive thread-level parallelism through simultaneous hardware multithreading as detailed in Hardware Multithreading. Unlike CPU cores, they are issued in order and there is no branch prediction or speculative execution.\u201d</blockquote>\n<h3>7. Coalesced Memory\u00a0Access</h3>\n<p>When a given set of threads read/write simultaneously from contiguous block of global memory in a single transaction, it is known as coalesced (can pronounce as\u00a0: coalesd) memory access. <em>Coalesced memory access applies only to GPUs with CUDA compute capability of 1.x and 2. Newer GPUs use more complex ways to access the global\u00a0memory.</em></p>\n<h3>8. Bank conflicts</h3>\n<p>What are banks?\u200a\u2014\u200aThe shared memory that can be accessed in parallel are divided into modules called\u00a0Banks.</p>\n<p>In shared memory, 4 bytes is called 1 word. There are 32 banks (in modern cards) in shared memory in Nvidia GPU. Each successive word of shared memory belong to a bank. This means word 0 belongs to bank 0, word 1 belongs to bank 1 and so on. A wrap can send request to any permutation of these 32 banks. While dividing shared memory into banks, if any of the memory address occurs in 2 banks then it causes bank conflict.</p>\n<p>If all the threads in a wrap wants to read value from a single bank then only once the value will be read and it will be shared among the threads. This is called <em>broadcast.</em></p>\n<h3>9. Numba on CPU and\u00a0GPU</h3>\n<h4>On CPU</h4>\n<p>While estimating value of PI using Monte Carlo method, we pass the range as 10k. On CPU without numba it took 10.8ms (mean of 7 runs, 100 loops each) whereas with Numba it took 237\u03bcs (mean of 7 runs, 100 loops each). That is 45 times\u00a0faster!</p>\n<pre><a href=\"http://twitter.com/jit\">@jit</a>(nopython=True)<br>def monte_carlo_pi(n):<br>    acc = 0<br>    for i in range(n):<br>        x = random.random()<br>        y = random.random()<br>        if (x**2 + y**2) &lt; 1.0:<br>            acc += 1<br>    return 4.0 * acc / n</pre>\n<pre>%timeit monte_carlo_pi(10000)</pre>\n<p>This is an example code from <a href=\"https://analyticsindiamag.com/make-python-code-faster-with-numba/\">Analytics Vidhya Numba tutorial</a>. To accelerate your code with Numba, its best to refer the documentation and modify your functions accordingly such that it only performs complex calculations. Due to limited types supported by Numba, creating such small functions will help you fully utilize it\u2019s potential.</p>\n<h4>On GPU</h4>\n<pre><a href=\"http://twitter.com/cuda\">@cuda</a>.jit<br>def add_kernel(x, y, out):<br>    idx = cuda.grid(1)<br>    out[idx] = x[idx] + y[idx]</pre>\n<pre>n = 4096<br>x = np.arange(n).astype(np.int32) <br>y = np.ones_like(x)   </pre>\n<pre>d_x = cuda.to_device(x) <br>d_y = cuda.to_device(y) <br>d_out = cuda.device_array_like(d_x) </pre>\n<pre>threads_per_block = 128<br>blocks_per_grid = 32</pre>\n<p>This addition function on CUDA takes 100\u03bcs (mean of 7 runs, 10000 loops each) and a novice addition code for same input takes 880\u03bcs (mean of 7 runs, 1000 loops each). Same operation on CUDA is 8.8 times\u00a0faster!</p>\n<p>But the time taken by the CUDA function is only for the calculation it has done meaning here we do not consider time taken to move data from host memory to GPU memory and then moving back output from GPU memory to the host memory.<br>Still, I think we can use it with functions where we perform complex calculations using Numpy or basic math operations. If that function is called every single second say while inferencing a model, then it would make a significant improvement in overall performance!</p>\n<h3>Conclusion</h3>\n<p>It was fun exploring basic concepts of GPU and CUDA. Also, by getting exposed to Numba, it is clear that we can accelerate code on CPU as well. Maybe when running inference on CPU, Numba might help accelerate some part of the pipeline!</p>\n<p>Follow the Kaggle notebook shared above to explore Numba\u00a0:).<br>Thanks for\u00a0reading.</p>\n<p>Follow me for more such blogs! Also connect with me on <a href=\"https://www.linkedin.com/in/sahil-chachra/\">LinkedIn</a>.</p>\n<h3>References</h3>\n<ol>\n<li><a href=\"https://numba.readthedocs.io/en/stable/user/5minguide.html\">https://numba.readthedocs.io/en/stable/user/5minguide.html</a></li>\n<li><a href=\"https://www.techopedia.com/definition/3978/just-in-time-compiler-jit-compiler#:~:text=A%20just-in-time%20(,fly%20as%20the%20program%20executes.\">https://www.techopedia.com/definition/3978/just-in-time-compiler-jit-compiler#:~:text=A%20just%2Din%2Dtime%20(,fly%20as%20the%20program%20executes.</a></li>\n<li><a href=\"https://www.kaggle.com/harshwalia/1-introduction-to-cuda-python-with-numba\">https://www.kaggle.com/harshwalia/1-introduction-to-cuda-python-with-numba</a></li>\n<li><a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html\">https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html</a></li>\n<li><a href=\"https://www.youtube.com/playlist?list=PLKK11Ligqititws0ZOoGk3SW-TZCar4dK\">https://www.youtube.com/playlist?list=PLKK11Ligqititws0ZOoGk3SW-TZCar4dK</a></li>\n<li><a href=\"https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/\">https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/</a></li>\n<li><a href=\"https://deeplizard.com/learn/video/6stDhEA0wFQ\">https://deeplizard.com/learn/video/6stDhEA0wFQ</a></li>\n<li><a href=\"https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/\">https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/</a></li>\n<li><a href=\"http://thebeardsage.com/cuda-streaming-multiprocessors/\">http://thebeardsage.com/cuda-streaming-multiprocessors/</a></li>\n<li><a href=\"https://stackoverflow.com/questions/32226993/understanding-streaming-multiprocessors-sm-and-streaming-processors-sp\">https://stackoverflow.com/questions/32226993/understanding-streaming-multiprocessors-sm-and-streaming-processors-sp</a></li>\n<li><a href=\"https://stackoverflow.com/questions/2207171/help-me-understand-cuda/2213744#2213744\">https://stackoverflow.com/questions/2207171/help-me-understand-cuda/2213744#2213744</a></li>\n<li>SMs\u200a\u2014\u200a<a href=\"https://stackoverflow.com/questions/3519598/streaming-multiprocessors-blocks-and-threads-cuda\">https://stackoverflow.com/questions/3519598/streaming-multiprocessors-blocks-and-threads-cuda</a>\n</li>\n<li><a href=\"https://stackoverflow.com/questions/10460742/how-do-cuda-blocks-warps-threads-map-onto-cuda-cores\">https://stackoverflow.com/questions/10460742/how-do-cuda-blocks-warps-threads-map-onto-cuda-cores</a></li>\n<li><a href=\"https://cvw.cac.cornell.edu/gpu/coalesced\">https://cvw.cac.cornell.edu/gpu/coalesced</a></li>\n<li>Half wrap\u200a\u2014\u200a<a href=\"https://forums.developer.nvidia.com/t/why-only-half-warp/15915/6\">https://forums.developer.nvidia.com/t/why-only-half-warp/15915/6</a>\n</li>\n<li>Bank conflic\u200a\u2014\u200a<a href=\"https://stackoverflow.com/questions/3841877/what-is-a-bank-conflict-doing-cuda-opencl-programming#:~:text=The%20shared%20memory%20that%20can,the%20advantages%20of%20parallel%20access\">https://stackoverflow.com/questions/3841877/what-is-a-bank-conflict-doing-cuda-opencl-programming#:~:text=The%20shared%20memory%20that%20can,the%20advantages%20of%20parallel%20access</a>.</li>\n<li>Bank conflict\u200a\u2014\u200a<a href=\"https://www.youtube.com/watch?v=CZgM3DEBplE\">https://www.youtube.com/watch?v=CZgM3DEBplE</a>\n</li>\n<li>Bank conflict\u200a\u2014\u200a<a href=\"https://www.generacodice.com/en/articolo/666174/%C2%BFqu%C3%A9-es-un-conflicto-banco?-(hacer-la-programaci%C3%B3n-cuda-/-opencl)\">https://www.generacodice.com/en/articolo/666174/%C2%BFqu%C3%A9-es-un-conflicto-banco?-(hacer-la-programaci%C3%B3n-cuda-/-opencl)</a>\n</li>\n</ol>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a0be21aa00b7\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["deep-learning","numba","cuda","gpu","python"]},{"title":"Paper Summary\u200a\u2014\u200aMetaFormer is Actually What You Need for Vision","pubDate":"2022-01-07 15:34:38","link":"https://sahilchachra.medium.com/paper-summary-metaformer-is-actually-what-you-need-for-vision-b6f172482604?source=rss-f31bf6073414------2","guid":"https://medium.com/p/b6f172482604","author":"Sahil Chachra","thumbnail":"https://cdn-images-1.medium.com/max/702/1*p13M1xSf2W36K4jSm6TADA.png","description":"\n<h3>Paper Summary\u200a\u2014\u200aMetaFormer is Actually What You Need for\u00a0Vision</h3>\n<p>In recent times we have seen that Transformers (for vision) have performed very well, i.e., at par or at times surpassing the previously claimed SOTA for vision. The heart of the Transformer is the attention based token mixer which was applauded for the results the transformers gave in other fields as\u00a0well.</p>\n<p>Now, what if I tell you, you replace that entire Attention based token mixer (which takes in key, value and query and does wonders), and replace that with just a Pooling layer and the result you will get will be at par with many SOTA architectures!</p>\n<p>Can you believe? Just simple Pooling operations, (MaxPool, AvgPool, etc in vision) which has no parameters, perform at par with Attention Mechanism!? Even I did not believe until I read the paper\u200a\u2014<strong><em>\u200aMetaFormer is Actually What You Need for Vision\u00a0(</em></strong><a href=\"https://arxiv.org/abs/2111.11418\"><strong><em>link</em></strong></a><strong><em>).</em></strong></p>\n<p><strong>Note\u200a\u2014\u200aIf I quote anything from the paper it\u2019ll be italic and inside quotes. For example\u200a\u2014\u200a\u201c</strong><em>this is an\u00a0example\u201d</em></p>\n<h3>Abstract</h3>\n<p>It was believed that the attention module of the transformer contributed most to it\u2019s amazing performance but later when it was replaced by spatial MLPs, it still gave good results. So, as per the authors, it means that, apart from the attention module, the general architecture of the transformer contributes to its performance.<br>To verify this, the authors replaced the attention module with a non-parametric pooling layer (they called the model as PoolFormer) and the results were still good! This result encourages the authors to move ahead and propose the concept of \u201cMetaFormer\u201d where the general architecture of the transformer is used without any specific token\u00a0mixer.</p>\n<h3>Introduction</h3>\n<p>The attention module for mixing information among tokens in the transformer\u2019s encoder is termed as \u201ctoken mixer\u201d. Believing the attention mechanism to be the reason behind transformer\u2019s performance, many works have been proposed surrounding this\u00a0concept.</p>\n<p>But some of the recent works, replaced the attention module with a Fourier Transform and still performed very well, attaining 97% of the accuracy of Vanilla transformer. So this shows that, if we use MetaFormer (the general architecture of the transformer having any computation in the place of attention module) as the general architecture, good results are attained.</p>\n<p>PoolFormer, proposed by the authors, outperforms fine-tuned transformers and MLP like\u00a0models.</p>\n<p>Contributions of the paper are\u00a0:-</p>\n<ol>\n<li>\u201c<em>Abstract transformers into a general architecture MetaFormer\u201d</em>\n</li>\n<li>Authors evaluate PoolFormer in multiple tasks such as Classification, object detection, semantic segmentation and instance segmentation and it \u201c<em>achieves competitive performance compared with the SOTA models using sophistic design of token\u00a0mixers.\u201d</em>\n</li>\n</ol>\n<h3>Method</h3>\n<h4>MetaFormer</h4>\n<p><em>\u201cInput I is first processed by input embedding such as patch embedding in\u00a0VITs.\u201d</em></p>\n<p><em>X = </em>InputEmb(<em>I</em>)</p>\n<p>Where X is a matrix of real number of dimension NxC, N is for squence length and C is embedding dimension. \u201c<em>Then embedding tokens are fed to repeated MetaFormer blocks, each of which includes two residual sub-blocks. The first sub-block mainly contains a token mixer to communicate information among tokens\u201d and its represented as</em></p>\n<p><em>Y = </em>TokenMixer(Norm(X)) + X\u00a0, where Norm is Layer Normalization or Batch Normalization and TokenMixer is a module for mixing information like attention, Spatial MLP,\u00a0etc</p>\n<p><em>\u201cThe second sub-block consists of two-layered MLP with non-linear activation\u201d</em></p>\n<p>Z = NonLinearActivationFunction(Normalization(Y) * W1) * W2 +\u00a0Y</p>\n<p>where W1 is matrix of real numbers of dimension C x rC and W2 is matrix of real numbers of dimension rC x C. W1 &amp; W2 both are learnable parameters with MLP expansion ratio\u00a0r.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/702/1*p13M1xSf2W36K4jSm6TADA.png\"><figcaption>Figure 1\u200a\u2014\u200aSource\u00a0: From paper\u00a0itself</figcaption></figure><h4>PoolFormer</h4>\n<p>The authors use Pooling Operation as Token Mixer in the general architecture proposed above that is MetaFormer. This operation has no parameters which can be learnt and \u201c<em>it just makes each token averagely aggregate its nearby token features\u201d. </em>Spatial MLP and Self attention have quadratic computational complexity where as pooling has linear computational complexity.</p>\n<p>\u201c<em>PoolFormer has 4 stages H/4 * W/4, H/8 * W/8, H/16 * W/16 and H/32 * W/32. There are two groups having different embedding size\u00a0:-</em></p>\n<ol>\n<li><em>small-sized models with embedding dimensions of 64, 128, 320, and 512 responding to the four\u00a0stages;</em></li>\n<li><em>2) medium-sized models with embedding dimensions 96, 192, 384, and\u00a0768.</em></li>\n</ol>\n<p><em>Assuming there are L PoolFormer blocks in total, stages 1, 2, 3, and 4 will contain L/6, L/6, L/2, and L/6 PoolFormer blocks respectively. The MLP expansion ratio is set as 4. According to the above simple model scaling rule, we obtain 5 different model sizes of PoolFormer and their hyperparameters are shown</em>\u201d\u00a0below.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/754/1*ZxnVKXf19LArPOeZ_rhxXQ.png\"><figcaption>Figure 2\u200a\u2014\u200aSource\u00a0: paper\u00a0iteself</figcaption></figure><h3>Results</h3>\n<p>For ImageNet, PoolFormer-S24 has top-1 accuracy greater than 80% with only 21M parameters and 2.6G MACs whereas DeiT-s achieves 79.8 but requires 4.6G MACs. ResMLP-S24 needs 30M parameters andd 6G MACs with accuracy of\u00a079.4.</p>\n<h3>Conclusion</h3>\n<p>Recently we have seen how simple architectures or simple practices (such as MLP Mixer, ResNet Strikes back and Patches are all you need) are overcoming complex SOTA architectures. Even in this paper we saw how simple Pooling operation performs better/at-par with Attention based module in Transformer for\u00a0Vision.</p>\n<p>\u201c<em>This finding conveys that the general architecture MetaFormer is actually what we need when designing vision models. By adopting MetaFormer, it is guaranteed that the derived models would have the potential to achieve reasonable performance.</em>\u201d</p>\n<p>For experiments, detailed results and other details, I highly recommend you to read the paper as you will get in-depth insights about all these\u00a0topics.</p>\n<p>This summary was short as the idea being conveyed was simple and precise in the\u00a0paper.</p>\n<p>Thanks for reading\u00a0:). Do follow me on Medium to get paper summaries every month.<br>Connect with me on <a href=\"https://www.linkedin.com/in/sahil-chachra/\">LinkedIn</a> too\u00a0:D.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b6f172482604\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>Paper Summary\u200a\u2014\u200aMetaFormer is Actually What You Need for\u00a0Vision</h3>\n<p>In recent times we have seen that Transformers (for vision) have performed very well, i.e., at par or at times surpassing the previously claimed SOTA for vision. The heart of the Transformer is the attention based token mixer which was applauded for the results the transformers gave in other fields as\u00a0well.</p>\n<p>Now, what if I tell you, you replace that entire Attention based token mixer (which takes in key, value and query and does wonders), and replace that with just a Pooling layer and the result you will get will be at par with many SOTA architectures!</p>\n<p>Can you believe? Just simple Pooling operations, (MaxPool, AvgPool, etc in vision) which has no parameters, perform at par with Attention Mechanism!? Even I did not believe until I read the paper\u200a\u2014<strong><em>\u200aMetaFormer is Actually What You Need for Vision\u00a0(</em></strong><a href=\"https://arxiv.org/abs/2111.11418\"><strong><em>link</em></strong></a><strong><em>).</em></strong></p>\n<p><strong>Note\u200a\u2014\u200aIf I quote anything from the paper it\u2019ll be italic and inside quotes. For example\u200a\u2014\u200a\u201c</strong><em>this is an\u00a0example\u201d</em></p>\n<h3>Abstract</h3>\n<p>It was believed that the attention module of the transformer contributed most to it\u2019s amazing performance but later when it was replaced by spatial MLPs, it still gave good results. So, as per the authors, it means that, apart from the attention module, the general architecture of the transformer contributes to its performance.<br>To verify this, the authors replaced the attention module with a non-parametric pooling layer (they called the model as PoolFormer) and the results were still good! This result encourages the authors to move ahead and propose the concept of \u201cMetaFormer\u201d where the general architecture of the transformer is used without any specific token\u00a0mixer.</p>\n<h3>Introduction</h3>\n<p>The attention module for mixing information among tokens in the transformer\u2019s encoder is termed as \u201ctoken mixer\u201d. Believing the attention mechanism to be the reason behind transformer\u2019s performance, many works have been proposed surrounding this\u00a0concept.</p>\n<p>But some of the recent works, replaced the attention module with a Fourier Transform and still performed very well, attaining 97% of the accuracy of Vanilla transformer. So this shows that, if we use MetaFormer (the general architecture of the transformer having any computation in the place of attention module) as the general architecture, good results are attained.</p>\n<p>PoolFormer, proposed by the authors, outperforms fine-tuned transformers and MLP like\u00a0models.</p>\n<p>Contributions of the paper are\u00a0:-</p>\n<ol>\n<li>\u201c<em>Abstract transformers into a general architecture MetaFormer\u201d</em>\n</li>\n<li>Authors evaluate PoolFormer in multiple tasks such as Classification, object detection, semantic segmentation and instance segmentation and it \u201c<em>achieves competitive performance compared with the SOTA models using sophistic design of token\u00a0mixers.\u201d</em>\n</li>\n</ol>\n<h3>Method</h3>\n<h4>MetaFormer</h4>\n<p><em>\u201cInput I is first processed by input embedding such as patch embedding in\u00a0VITs.\u201d</em></p>\n<p><em>X = </em>InputEmb(<em>I</em>)</p>\n<p>Where X is a matrix of real number of dimension NxC, N is for squence length and C is embedding dimension. \u201c<em>Then embedding tokens are fed to repeated MetaFormer blocks, each of which includes two residual sub-blocks. The first sub-block mainly contains a token mixer to communicate information among tokens\u201d and its represented as</em></p>\n<p><em>Y = </em>TokenMixer(Norm(X)) + X\u00a0, where Norm is Layer Normalization or Batch Normalization and TokenMixer is a module for mixing information like attention, Spatial MLP,\u00a0etc</p>\n<p><em>\u201cThe second sub-block consists of two-layered MLP with non-linear activation\u201d</em></p>\n<p>Z = NonLinearActivationFunction(Normalization(Y) * W1) * W2 +\u00a0Y</p>\n<p>where W1 is matrix of real numbers of dimension C x rC and W2 is matrix of real numbers of dimension rC x C. W1 &amp; W2 both are learnable parameters with MLP expansion ratio\u00a0r.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/702/1*p13M1xSf2W36K4jSm6TADA.png\"><figcaption>Figure 1\u200a\u2014\u200aSource\u00a0: From paper\u00a0itself</figcaption></figure><h4>PoolFormer</h4>\n<p>The authors use Pooling Operation as Token Mixer in the general architecture proposed above that is MetaFormer. This operation has no parameters which can be learnt and \u201c<em>it just makes each token averagely aggregate its nearby token features\u201d. </em>Spatial MLP and Self attention have quadratic computational complexity where as pooling has linear computational complexity.</p>\n<p>\u201c<em>PoolFormer has 4 stages H/4 * W/4, H/8 * W/8, H/16 * W/16 and H/32 * W/32. There are two groups having different embedding size\u00a0:-</em></p>\n<ol>\n<li><em>small-sized models with embedding dimensions of 64, 128, 320, and 512 responding to the four\u00a0stages;</em></li>\n<li><em>2) medium-sized models with embedding dimensions 96, 192, 384, and\u00a0768.</em></li>\n</ol>\n<p><em>Assuming there are L PoolFormer blocks in total, stages 1, 2, 3, and 4 will contain L/6, L/6, L/2, and L/6 PoolFormer blocks respectively. The MLP expansion ratio is set as 4. According to the above simple model scaling rule, we obtain 5 different model sizes of PoolFormer and their hyperparameters are shown</em>\u201d\u00a0below.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/754/1*ZxnVKXf19LArPOeZ_rhxXQ.png\"><figcaption>Figure 2\u200a\u2014\u200aSource\u00a0: paper\u00a0iteself</figcaption></figure><h3>Results</h3>\n<p>For ImageNet, PoolFormer-S24 has top-1 accuracy greater than 80% with only 21M parameters and 2.6G MACs whereas DeiT-s achieves 79.8 but requires 4.6G MACs. ResMLP-S24 needs 30M parameters andd 6G MACs with accuracy of\u00a079.4.</p>\n<h3>Conclusion</h3>\n<p>Recently we have seen how simple architectures or simple practices (such as MLP Mixer, ResNet Strikes back and Patches are all you need) are overcoming complex SOTA architectures. Even in this paper we saw how simple Pooling operation performs better/at-par with Attention based module in Transformer for\u00a0Vision.</p>\n<p>\u201c<em>This finding conveys that the general architecture MetaFormer is actually what we need when designing vision models. By adopting MetaFormer, it is guaranteed that the derived models would have the potential to achieve reasonable performance.</em>\u201d</p>\n<p>For experiments, detailed results and other details, I highly recommend you to read the paper as you will get in-depth insights about all these\u00a0topics.</p>\n<p>This summary was short as the idea being conveyed was simple and precise in the\u00a0paper.</p>\n<p>Thanks for reading\u00a0:). Do follow me on Medium to get paper summaries every month.<br>Connect with me on <a href=\"https://www.linkedin.com/in/sahil-chachra/\">LinkedIn</a> too\u00a0:D.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b6f172482604\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["deep-learning","computer-vision","data-science","ai"]},{"title":"Paper Summary\u200a\u2014\u200atorch.manual_seed(3407) is all you need","pubDate":"2021-12-11 20:00:33","link":"https://sahilchachra.medium.com/paper-summary-torch-manual-seed-3407-is-all-you-need-9ef0f7aa7d78?source=rss-f31bf6073414------2","guid":"https://medium.com/p/9ef0f7aa7d78","author":"Sahil Chachra","thumbnail":"https://cdn-images-1.medium.com/max/573/1*w8sr36Sn5rqLD1G70N9KIg.png","description":"\n<h3>Paper Summary\u200a\u2014\u200atorch.manual_seed(3407) is all you\u00a0need</h3>\n<p>Whenever we train a neural network from scratch, it\u2019s weights are initialized with random values. So, if you re-run the same training job again and again, the values used to initialized the weights will keep on changing as they would be randomly generated.</p>\n<p>Now just imagine, metric of a State of the Art architecture for a given task is 80. You propose a new architecture for the same task and train your model from scratch. After you run it once (assuming all hyper-parameters were just perfect), you get 79.8 metric value. For some reason, you then just re-run the experiment keeping everything unchanged. You get 80.2 metric value and your architecture has now surpassed previous SOTA\u2019s performance. Apart from weights of the neural network being randomly initialized, every other value/hyper-parameter was left unchanged during the second trial! So what do you think the reason might\u00a0be?</p>\n<p>Let\u2019s look at the summary of the paper\u200a\u2014<strong>\u200a<em>torch.manual_seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision</em> </strong>by<strong> <em>David Picard</em>. </strong>Link of the paper\u00a0: <a href=\"https://arxiv.org/pdf/2109.08203.pdf\">Paper\u00a0Link</a>.</p>\n<p><strong>Note\u200a\u2014\u200aIf I quote anything from the paper it\u2019ll be italic and inside quotes. For example\u200a\u2014\u200a\u201c</strong><em>this is an\u00a0example\u201d</em></p>\n<h3>Abstract</h3>\n<p>The author investigates the \u201c<em>effect of random seed selection on the accuracy\u201d</em> of a deep learning architecture for perception models. He has done so by scanning up to 10\u2074 seeds on datasets such as CIFAR-10 and on the ImageNet dataset by using pre-trained models. \u201c<strong><em>The conclusions are that even if the variance is not very large, it is surprisingly easy to find an outlier that performs much better or much worse than the average.</em></strong>\u201d</p>\n<h3>Introduction</h3>\n<p>The main aim of the experiment is\u00a0:-</p>\n<ol>\n<li>\u201c<em>What is the distribution of scores with respect to the choice of seed?</em>\u00a0\u201d</li>\n<li>\u201c<em>Are there black swans, i.e., seeds that produce radically different results?</em>\u201d</li>\n<li>\u201c<em>Does pretraining on larger datasets mitigate variability induced by the choice of\u00a0seed?</em>\u201d</li>\n</ol>\n<p>These questions are important to discuss/ask because it\u2019s a common practice in domains relying on machine learning because the papers report only single run of the experiment performed. Like the table in the papers (papers say who claim the new SOTA and prove it with experimental result) just show Architecture-small = X metric value, Architecture-medium = Y metric value, etc. They don\u2019t mention Architecture-small = X metric value (1st run), Architecture-small = X metric value (2nd run), etc. As per the author this trend is there because of limited computing resources and having one at least one result is better than no result at\u00a0all!</p>\n<p>Like physical experiments have noise in the measurement the same way we have random initialization, data split in train and test, etc. The author then tells that if the effect of setting random seeds for different experiments is negligible then things are fine but if not then the publications should include \u201c<em>detailed analysis of the contribution of each random factor to the variation in observed performances\u201d</em></p>\n<h3>Experimental Setup</h3>\n<p>For CIFAR 10\u200a\u2014\u200asome 10,000 seeds were explored where each seed experiment took 30 seconds to train on evaluate. GPU used by the author was V100. The model used was custom ResNet with 9\u00a0layers.</p>\n<p>For ImageNet, since it was nearly impossible train neural networks from scratch with so many seeds, the author used pretrained models \u201c<em>where only the classification layer is initialized from scratch.</em>\u201d The author used three different models\u200a\u2014\u200aSupervised ResNet 50, SSL ResNet 50 and SSL\u00a0ViT.</p>\n<p><strong>(Please refer the paper to know more about the training\u00a0setup)</strong></p>\n<h3>Limitations</h3>\n<p>The author informs that this experiments has many limitations which affects the final conclusion of the experiment.</p>\n<p>The accuracy from the experiments the author ran is not at par with SOTA results because of budget constraints. CIFAR 10\u2019s accuracy achieved is some how comparable to ResNet\u2019s result from 2016. \u201c<strong><em>This means papers from that era may have been subject to the sensitivity to the seed\u201d </em></strong>that the author observes.</p>\n<p>For ResNet50 model trained on ImageNet dataset, in DINO paper, the authors say that they get improved accuracy by tweaking the hyperparameters. But still the author believes that experiments done with ImageNet \u201c<em>underestimate the variability because they all start from the same pretrained model, which means the effect of the seed is limited to initialization of the classification layer and the optimization process.</em></p>\n<p>The author also says that these limitations would have overcome if 10 times more computation budget was available on CIFAR dataset and 50 to 100 times more computation budget was available for ImageNet\u00a0models.</p>\n<h3>Findings</h3>\n<h4>Convergence instability</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/573/1*w8sr36Sn5rqLD1G70N9KIg.png\"><figcaption>Source\u200a\u2014\u200afrom\u00a0Paper</figcaption></figure><p>This histograms shows density plot of final validation accuracy on CIFAR10 dataset on 500 seeds. Each das line is one\u00a0run.</p>\n<p>We can see a lot of dash lines at accuracy 90.50% and 91.00%. \u201c<em>Hence a 0.5% difference in accuracy could be entirely explained by just a seed difference, without having chosen a particularly bad or good\u00a0seed.</em>\u201d</p>\n<p>\u201c<strong><em>The answer to first question is thus that the distribution of scores with respect to the seeds is fairly concentrated and sort of pointy. This is a reassuring result because it means that scores are likely to be representative of what the model and the training setup can do, expect when one is actively searching (knowingly or unknowingly) for a good/bad\u00a0seed.</em></strong>\u201d</p>\n<h4>Searching for Black\u00a0Swans</h4>\n<p>Short training setup was used to scan 10\u2074 seeds. The minimum and maximum values are 89.01% and 90.83% respectively. This difference of 1.82% is termed as significant difference in the domain! This difference means whether or not the paper on the new SOTA can be published or\u00a0not!</p>\n<p>\u201c<strong><em>The results of this test allow me to answer positively to the second question: there are indeed seeds that produce scores sufficiently good (respectively bad) to be considered as a significant improvement (respectively downgrade) by the computer vision community. This is a worrying result as the community is currently very much score driven, and yet these can just be artifacts of randomness.</em></strong>\u201d</p>\n<h4>Large Scale\u00a0datasets</h4>\n<p>Author uses pretrained models which are fine tuned and evaluated in ImageNet to see if by using larger training set with a pretrained model still shows randomness in scores with respect to seed (50 seeds were used for ImageNet experiment).</p>\n<p>As per the accuracies reported, there is standard deviation of 0.1% and the difference between minimum and maximum values is about 0.5%. This difference is less than that seen in CIFAR10 experiments\u200a\u2014\u200athis is still surprising as all the experiments were ran using same initial pretrained weights apart from last layer. Only image batches varies as per the seed set. A difference of 0.5% in ImageNet is very significant value which will determine whether to publish the work or\u00a0not.</p>\n<p><strong>The answer to the third question is mixed\u200a\u2014\u200ausing pretrained models reduce the variation induced by the seed we choose. But the variation is still has to be considered as even 0.5% is still considered as significant improvement in vision community.</strong></p>\n<p>For large dataset such as ImageNet, it will be good if more than 50 seeds is scanned to study if choice of seed affects the accuracy even for pretrained models on large scale datasets.</p>\n<h3>Discussion</h3>\n<ol>\n<li>\u201c<em>What is the distribution of scores with respect to the choice of seed?</em> \u201d\u200a\u2014\u200a\u201c<em>Once the model converged, this distribution is relatively stable which means that some seed are intrinsically better than\u00a0others.</em>\u201d</li>\n<li>\u201c<em>Are there black swans, i.e., seeds that produce radically different results?</em>\u201d\u200a\u2014\u200a\u201c<em>Yes. On a scanning of 104 seeds, we obtained a difference between the maximum and minimum accuracy close to 2% which is above the threshold commonly used by the computer vision community of what is considered significant.</em>\u201d</li>\n<li>\u201c<em>Does pretraining on larger datasets mitigate variability induced by the choice of seed?</em>\u201d\u200a\u2014\u200a\u201c<em>It certainly reduces the variations due to using different seeds, but it does not mitigate it. On Imagenet, we found a difference between the maximum and the minimum accuracy of around 0.5%, which is commonly accepted as significant by the community for this dataset.</em>\u201d</li>\n</ol>\n<p>In the end the author suggests the researchers to study the randomness of different seeds on their experiments and asks them to report average, minimum, maximum and standard deviation scores.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9ef0f7aa7d78\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>Paper Summary\u200a\u2014\u200atorch.manual_seed(3407) is all you\u00a0need</h3>\n<p>Whenever we train a neural network from scratch, it\u2019s weights are initialized with random values. So, if you re-run the same training job again and again, the values used to initialized the weights will keep on changing as they would be randomly generated.</p>\n<p>Now just imagine, metric of a State of the Art architecture for a given task is 80. You propose a new architecture for the same task and train your model from scratch. After you run it once (assuming all hyper-parameters were just perfect), you get 79.8 metric value. For some reason, you then just re-run the experiment keeping everything unchanged. You get 80.2 metric value and your architecture has now surpassed previous SOTA\u2019s performance. Apart from weights of the neural network being randomly initialized, every other value/hyper-parameter was left unchanged during the second trial! So what do you think the reason might\u00a0be?</p>\n<p>Let\u2019s look at the summary of the paper\u200a\u2014<strong>\u200a<em>torch.manual_seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision</em> </strong>by<strong> <em>David Picard</em>. </strong>Link of the paper\u00a0: <a href=\"https://arxiv.org/pdf/2109.08203.pdf\">Paper\u00a0Link</a>.</p>\n<p><strong>Note\u200a\u2014\u200aIf I quote anything from the paper it\u2019ll be italic and inside quotes. For example\u200a\u2014\u200a\u201c</strong><em>this is an\u00a0example\u201d</em></p>\n<h3>Abstract</h3>\n<p>The author investigates the \u201c<em>effect of random seed selection on the accuracy\u201d</em> of a deep learning architecture for perception models. He has done so by scanning up to 10\u2074 seeds on datasets such as CIFAR-10 and on the ImageNet dataset by using pre-trained models. \u201c<strong><em>The conclusions are that even if the variance is not very large, it is surprisingly easy to find an outlier that performs much better or much worse than the average.</em></strong>\u201d</p>\n<h3>Introduction</h3>\n<p>The main aim of the experiment is\u00a0:-</p>\n<ol>\n<li>\u201c<em>What is the distribution of scores with respect to the choice of seed?</em>\u00a0\u201d</li>\n<li>\u201c<em>Are there black swans, i.e., seeds that produce radically different results?</em>\u201d</li>\n<li>\u201c<em>Does pretraining on larger datasets mitigate variability induced by the choice of\u00a0seed?</em>\u201d</li>\n</ol>\n<p>These questions are important to discuss/ask because it\u2019s a common practice in domains relying on machine learning because the papers report only single run of the experiment performed. Like the table in the papers (papers say who claim the new SOTA and prove it with experimental result) just show Architecture-small = X metric value, Architecture-medium = Y metric value, etc. They don\u2019t mention Architecture-small = X metric value (1st run), Architecture-small = X metric value (2nd run), etc. As per the author this trend is there because of limited computing resources and having one at least one result is better than no result at\u00a0all!</p>\n<p>Like physical experiments have noise in the measurement the same way we have random initialization, data split in train and test, etc. The author then tells that if the effect of setting random seeds for different experiments is negligible then things are fine but if not then the publications should include \u201c<em>detailed analysis of the contribution of each random factor to the variation in observed performances\u201d</em></p>\n<h3>Experimental Setup</h3>\n<p>For CIFAR 10\u200a\u2014\u200asome 10,000 seeds were explored where each seed experiment took 30 seconds to train on evaluate. GPU used by the author was V100. The model used was custom ResNet with 9\u00a0layers.</p>\n<p>For ImageNet, since it was nearly impossible train neural networks from scratch with so many seeds, the author used pretrained models \u201c<em>where only the classification layer is initialized from scratch.</em>\u201d The author used three different models\u200a\u2014\u200aSupervised ResNet 50, SSL ResNet 50 and SSL\u00a0ViT.</p>\n<p><strong>(Please refer the paper to know more about the training\u00a0setup)</strong></p>\n<h3>Limitations</h3>\n<p>The author informs that this experiments has many limitations which affects the final conclusion of the experiment.</p>\n<p>The accuracy from the experiments the author ran is not at par with SOTA results because of budget constraints. CIFAR 10\u2019s accuracy achieved is some how comparable to ResNet\u2019s result from 2016. \u201c<strong><em>This means papers from that era may have been subject to the sensitivity to the seed\u201d </em></strong>that the author observes.</p>\n<p>For ResNet50 model trained on ImageNet dataset, in DINO paper, the authors say that they get improved accuracy by tweaking the hyperparameters. But still the author believes that experiments done with ImageNet \u201c<em>underestimate the variability because they all start from the same pretrained model, which means the effect of the seed is limited to initialization of the classification layer and the optimization process.</em></p>\n<p>The author also says that these limitations would have overcome if 10 times more computation budget was available on CIFAR dataset and 50 to 100 times more computation budget was available for ImageNet\u00a0models.</p>\n<h3>Findings</h3>\n<h4>Convergence instability</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/573/1*w8sr36Sn5rqLD1G70N9KIg.png\"><figcaption>Source\u200a\u2014\u200afrom\u00a0Paper</figcaption></figure><p>This histograms shows density plot of final validation accuracy on CIFAR10 dataset on 500 seeds. Each das line is one\u00a0run.</p>\n<p>We can see a lot of dash lines at accuracy 90.50% and 91.00%. \u201c<em>Hence a 0.5% difference in accuracy could be entirely explained by just a seed difference, without having chosen a particularly bad or good\u00a0seed.</em>\u201d</p>\n<p>\u201c<strong><em>The answer to first question is thus that the distribution of scores with respect to the seeds is fairly concentrated and sort of pointy. This is a reassuring result because it means that scores are likely to be representative of what the model and the training setup can do, expect when one is actively searching (knowingly or unknowingly) for a good/bad\u00a0seed.</em></strong>\u201d</p>\n<h4>Searching for Black\u00a0Swans</h4>\n<p>Short training setup was used to scan 10\u2074 seeds. The minimum and maximum values are 89.01% and 90.83% respectively. This difference of 1.82% is termed as significant difference in the domain! This difference means whether or not the paper on the new SOTA can be published or\u00a0not!</p>\n<p>\u201c<strong><em>The results of this test allow me to answer positively to the second question: there are indeed seeds that produce scores sufficiently good (respectively bad) to be considered as a significant improvement (respectively downgrade) by the computer vision community. This is a worrying result as the community is currently very much score driven, and yet these can just be artifacts of randomness.</em></strong>\u201d</p>\n<h4>Large Scale\u00a0datasets</h4>\n<p>Author uses pretrained models which are fine tuned and evaluated in ImageNet to see if by using larger training set with a pretrained model still shows randomness in scores with respect to seed (50 seeds were used for ImageNet experiment).</p>\n<p>As per the accuracies reported, there is standard deviation of 0.1% and the difference between minimum and maximum values is about 0.5%. This difference is less than that seen in CIFAR10 experiments\u200a\u2014\u200athis is still surprising as all the experiments were ran using same initial pretrained weights apart from last layer. Only image batches varies as per the seed set. A difference of 0.5% in ImageNet is very significant value which will determine whether to publish the work or\u00a0not.</p>\n<p><strong>The answer to the third question is mixed\u200a\u2014\u200ausing pretrained models reduce the variation induced by the seed we choose. But the variation is still has to be considered as even 0.5% is still considered as significant improvement in vision community.</strong></p>\n<p>For large dataset such as ImageNet, it will be good if more than 50 seeds is scanned to study if choice of seed affects the accuracy even for pretrained models on large scale datasets.</p>\n<h3>Discussion</h3>\n<ol>\n<li>\u201c<em>What is the distribution of scores with respect to the choice of seed?</em> \u201d\u200a\u2014\u200a\u201c<em>Once the model converged, this distribution is relatively stable which means that some seed are intrinsically better than\u00a0others.</em>\u201d</li>\n<li>\u201c<em>Are there black swans, i.e., seeds that produce radically different results?</em>\u201d\u200a\u2014\u200a\u201c<em>Yes. On a scanning of 104 seeds, we obtained a difference between the maximum and minimum accuracy close to 2% which is above the threshold commonly used by the computer vision community of what is considered significant.</em>\u201d</li>\n<li>\u201c<em>Does pretraining on larger datasets mitigate variability induced by the choice of seed?</em>\u201d\u200a\u2014\u200a\u201c<em>It certainly reduces the variations due to using different seeds, but it does not mitigate it. On Imagenet, we found a difference between the maximum and the minimum accuracy of around 0.5%, which is commonly accepted as significant by the community for this dataset.</em>\u201d</li>\n</ol>\n<p>In the end the author suggests the researchers to study the randomness of different seeds on their experiments and asks them to report average, minimum, maximum and standard deviation scores.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9ef0f7aa7d78\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["deep-learning","artificial-intelligence","data-science","research","experiment"]},{"title":"Paper Summary\u200a\u2014\u200aWhat is being transferred in transfer learning?","pubDate":"2021-11-21 10:29:03","link":"https://sahilchachra.medium.com/paper-summary-what-is-being-transferred-in-transfer-learning-250dc7a9d127?source=rss-f31bf6073414------2","guid":"https://medium.com/p/250dc7a9d127","author":"Sahil Chachra","thumbnail":"https://cdn-images-1.medium.com/max/768/1*zaVM0VQ0OxDeYlpd4VPHlQ@2x.jpeg","description":"\n<h3>Paper Summary\u200a\u2014\u200aWhat is being transferred in transfer learning?</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/768/1*zaVM0VQ0OxDeYlpd4VPHlQ@2x.jpeg\"></figure><p>Now-a-days we widely use the transfer learning to either achieve good results or to get save time on training the model from scratch. But we don\u2019t know what is actually being transferred during this\u00a0process.</p>\n<p>If you use a ImageNet pretrained classification model and train it on some medical domain (say chest X-ray images for some disease) you will see it\u2019s performing better than training that same model from scratch (considering we have small train set)! How even the objects present in the ImageNet dataset are similar to Human chest X-ray\u00a0images!?</p>\n<p>Few days ago, I came across this paper\u200a\u2014\u200a<strong>What is being transferred in transfer learning</strong> (<a href=\"https://arxiv.org/abs/2008.11687\">link</a>). It\u2019s a paper from <strong>Google Brain</strong>. This paper helps you understand what is happening when pre-trained models are trained for some different task. This blog is a summary of the paper.<strong> I will try to cover most of the topics but read the paper too.<em> Its worth the\u00a0time</em>!</strong></p>\n<p>Coming to the\u00a0paper\u2026</p>\n<p><strong>Note\u200a\u2014\u200aIf I quote anything from the paper it\u2019ll be italic and inside quotes. For example\u200a\u2014\u200a\u201c</strong><em>this is an example.\u201d</em></p>\n<p><em>I have not discussed about Performance barrier and loss landscape because I believe it is better to read this part from the paper directly as summarizing this would be copying stuff from the paper and there is no point just\u00a0copying.</em></p>\n<h3>Abstract</h3>\n<p>In the abstract the authors say that although we are using transfer learning so much yet we do not understand \u201c<em>which part of the neural network is responsible\u201d </em>for this. In this paper, the authors<em> \u201cprovide new tools and analyses to address these fundamental questions\u201d.</em> This paper focuses on results of several experiments which were done to understand what is being transferred in transfer learning. In short, one experiment was on \u201c<em>block-shuffled images\u201d, </em>then they separated<em> </em>the <em>\u201ceffect of feature reuse from learning low-level statistics of data</em>\u201d, they also show that when we use pretrained model weights, the <em>\u201cmodel stays in the same basin in the loss landscape and different instances of such model are similar in feature space and close in parameter space.\u201d</em></p>\n<h3>Introduction</h3>\n<p>We use transfer learning to transfer knowledge from one domain(source domain) to other domain(target domain) when we have less data or we need to produce results in lesser time. In total three datasets were used to carry out experiments\u200a\u2014\u200aImageNet, DomainNet (has images such as real images, sketches and clip arts to study transfer learning) and CheXpert (chest X-ray\u00a0images)</p>\n<h3>Problem Setup</h3>\n<p>The authors use ImageNet(Source domain) pre-trained weights as starting weights for transfer learning and consider CheXpert and datasets from DomainNet as target domain (or downstream task). In other words, they take ImageNet pretrained weights and train the model on CheXpert and DomainNet dataset to study transfer learning.</p>\n<p>The authors \u201c<em>analyze networks in four different cases\u00a0: pre-trained network, the network at random initialization, the network that is fine-tuned on target domain after pre-training on source domain and the model that is trained on target domain from random initialization.\u201d </em>All these four cases will be referred as following in the paper\u00a0: \u201c<em>RI (random initialization), P (pre trained model), RI-T (model trained on target domain from random initialization), P-T (model trained/fine-tuned on target domain starting from pre-trained weights).\u201d</em></p>\n<h3>Role of Feature\u00a0reuse</h3>\n<p>We usually use transfer learning when we have a very small dataset for target domain. But still we don\u2019t know how this works even when source domain (dataset of pre-trained weights) is visually very different from the target domain (dataset for the current\u00a0task).</p>\n<p>To study the <strong>role of feature reuse</strong>, ImageNet pre-trained model was used and target domains were CheXpert and DomainNet.</p>\n<p>When RI-T (<em>model trained on target domain from random initialization) was compared to P-T(model trained/fine-tuned on target domain starting from pre-trained weights), </em>a very huge performance boost was seen on the DomainNet\u2019s Real dataset. \u201c<em>This confirms the intuition that feature reuse plays an important role in transfer learning\u201d. </em>Now, when ImageNet pretrained weights were used to fine-tune the model on CheXpert dataset, it was observed that P-T converges faster than RI-T in all the cases. \u201c<em>This suggests additional benefits of pre-trained weights that are not directly coming from feature\u00a0reuse\u201d.</em></p>\n<p>To further verify this hypothesis, \u201c<em>a series of modified downstream task\u201d </em>were created which were very different from each other visually. To be precise, image was partitioned into equal sized blocks and those blocks were shuffled across the image randomly. By doing this, the image was changing visually but the low level statistics remained the same. (Block size 1 means\u200a\u2014\u200apixels of size 1 were taken and shuffled across the image). Now the target domain looked visually far distinct from the source\u00a0domain.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/482/1*c-_KWjUgNpyCtDSZVRQQXg.png\"><figcaption>Middle Image\u200a\u2014\u200aPixel of block size 8 shuffled. Right image\u200a\u2014\u200aPixel of block size 1 shuffled. (Source\u200a\u2014\u200aPaper\u00a0itself)</figcaption></figure><p>After performing this shuffling, it was observed that\u200a\u2014\u200a1) final performance for RI-T and P-T drops as blocks size becomes smaller in size indicating that task is becoming more difficult to learn. 2) There is a decrease in relative accuracy difference with decrease in block size on real and clipart dataset from DomainNet \u201c<em>showing consistency with the intuition that decreasing feature reuse leads to diminishing benefits\u201d. </em>3) On q1uick draw dataset from DomainNet, there not much decrease. The dataset was already very dissimilar to the ImageNet dataset, indicating that some other factors of the pre-trained weights are helping the model in the downstream task.</p>\n<p><strong>The authors conclude that\u200a\u2014\u200a\u201c</strong><em>feature reuse plays a very important role in transfer learning, especially when the downstream task shares similar visual features with the pre-training domain.\u201d </em>They also tell that there are low level statistics which don\u2019t have any visual information play a role in transfer learning as even after shuffling the image pixels completely and shuffling the input channels, they did not see significant decreasing trend of metrics for quickdraw dataset.</p>\n<h3>Opening up the\u00a0model</h3>\n<p>In the second experiment, the authors <strong>investigate the mistakes made by the model</strong>, that is, they see the \u201c<em>common mistakes\u201d </em>made by both the models if they classify the same data point incorrectly. Then they also see \u201c<em>uncommon mistakes\u201d</em> where in one model classifies a data point correctly and other one incorrectly. They first \u201c<em>compare the ratio of common and uncommon mistakes between two P-Ts, a P-T and a RI-T and two RI-Ts\u201d. T</em>hey see that good number of uncommon mistakes are between O-T and RI-T where as two P-Ts have very few uncommon mistakes. This trend is visible for CheXpert and DomainNet dataset.</p>\n<p><strong>Feature similarity</strong>\u00a0: The authors use Centered Kernel alignment technique to measure similarity between \u201c<em>two output features in a layer of network architecture given two instances of such network\u201d.</em> It was observed that \u201c<em> two instances of P-T are highly similar across different layers\u201d. </em>It was noted that the feature similarity is stronger towards the last layers of the network in case of P-T and RI-T. <strong>Important point to be noted here\u200a\u2014\u200a\u201c</strong><em>These experiments show that the initialization point, whether pre-trained or random, drastically impacts feature similarity, and although both networks are showing high accuracy, they are not that similar in the feature space. This emphasizes on role of feature reuse and that two P-T are reusing the same features.\u201d</em></p>\n<h3>Module Criticality</h3>\n<p>In this section, the authors discuss how to calculate which module of the architecture is more critical from the rest of the\u00a0modules.</p>\n<p>The most important outcome was that\u200a\u2014\u200ait was seen that lower layers of the model are responsible for more general features while the higher layers have features which are specific to the target domain/dataset the model is being trained\u00a0on.</p>\n<h3>Conclusion</h3>\n<p>Do checkout the paper once to gain more insights about the experiments performed. This paper tells us that\u00a0:-</p>\n<ol>\n<li>We can use (or at least consider trying) transfer learning even if the target domain is visually very different from the source domain (dataset used in pre-trained weights)</li>\n<li>Features in the lower layers and some low level statistics which we cannot see visually are mostly responsible for this amazing performance of transfer learning.</li>\n<li>Since upper layers are more domain specific, we should freeze lower layers and train the upper layers for better performance of the model. But from which layers to start unfreezing would require extensive experiments and resources.</li>\n</ol>\n<p>Thanks for reading the blog. Hope you find it helpful!<br>Connect with me on <a href=\"https://www.linkedin.com/in/sahil-chachra/\">LinkedIn</a>\u00a0:).</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=250dc7a9d127\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>Paper Summary\u200a\u2014\u200aWhat is being transferred in transfer learning?</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/768/1*zaVM0VQ0OxDeYlpd4VPHlQ@2x.jpeg\"></figure><p>Now-a-days we widely use the transfer learning to either achieve good results or to get save time on training the model from scratch. But we don\u2019t know what is actually being transferred during this\u00a0process.</p>\n<p>If you use a ImageNet pretrained classification model and train it on some medical domain (say chest X-ray images for some disease) you will see it\u2019s performing better than training that same model from scratch (considering we have small train set)! How even the objects present in the ImageNet dataset are similar to Human chest X-ray\u00a0images!?</p>\n<p>Few days ago, I came across this paper\u200a\u2014\u200a<strong>What is being transferred in transfer learning</strong> (<a href=\"https://arxiv.org/abs/2008.11687\">link</a>). It\u2019s a paper from <strong>Google Brain</strong>. This paper helps you understand what is happening when pre-trained models are trained for some different task. This blog is a summary of the paper.<strong> I will try to cover most of the topics but read the paper too.<em> Its worth the\u00a0time</em>!</strong></p>\n<p>Coming to the\u00a0paper\u2026</p>\n<p><strong>Note\u200a\u2014\u200aIf I quote anything from the paper it\u2019ll be italic and inside quotes. For example\u200a\u2014\u200a\u201c</strong><em>this is an example.\u201d</em></p>\n<p><em>I have not discussed about Performance barrier and loss landscape because I believe it is better to read this part from the paper directly as summarizing this would be copying stuff from the paper and there is no point just\u00a0copying.</em></p>\n<h3>Abstract</h3>\n<p>In the abstract the authors say that although we are using transfer learning so much yet we do not understand \u201c<em>which part of the neural network is responsible\u201d </em>for this. In this paper, the authors<em> \u201cprovide new tools and analyses to address these fundamental questions\u201d.</em> This paper focuses on results of several experiments which were done to understand what is being transferred in transfer learning. In short, one experiment was on \u201c<em>block-shuffled images\u201d, </em>then they separated<em> </em>the <em>\u201ceffect of feature reuse from learning low-level statistics of data</em>\u201d, they also show that when we use pretrained model weights, the <em>\u201cmodel stays in the same basin in the loss landscape and different instances of such model are similar in feature space and close in parameter space.\u201d</em></p>\n<h3>Introduction</h3>\n<p>We use transfer learning to transfer knowledge from one domain(source domain) to other domain(target domain) when we have less data or we need to produce results in lesser time. In total three datasets were used to carry out experiments\u200a\u2014\u200aImageNet, DomainNet (has images such as real images, sketches and clip arts to study transfer learning) and CheXpert (chest X-ray\u00a0images)</p>\n<h3>Problem Setup</h3>\n<p>The authors use ImageNet(Source domain) pre-trained weights as starting weights for transfer learning and consider CheXpert and datasets from DomainNet as target domain (or downstream task). In other words, they take ImageNet pretrained weights and train the model on CheXpert and DomainNet dataset to study transfer learning.</p>\n<p>The authors \u201c<em>analyze networks in four different cases\u00a0: pre-trained network, the network at random initialization, the network that is fine-tuned on target domain after pre-training on source domain and the model that is trained on target domain from random initialization.\u201d </em>All these four cases will be referred as following in the paper\u00a0: \u201c<em>RI (random initialization), P (pre trained model), RI-T (model trained on target domain from random initialization), P-T (model trained/fine-tuned on target domain starting from pre-trained weights).\u201d</em></p>\n<h3>Role of Feature\u00a0reuse</h3>\n<p>We usually use transfer learning when we have a very small dataset for target domain. But still we don\u2019t know how this works even when source domain (dataset of pre-trained weights) is visually very different from the target domain (dataset for the current\u00a0task).</p>\n<p>To study the <strong>role of feature reuse</strong>, ImageNet pre-trained model was used and target domains were CheXpert and DomainNet.</p>\n<p>When RI-T (<em>model trained on target domain from random initialization) was compared to P-T(model trained/fine-tuned on target domain starting from pre-trained weights), </em>a very huge performance boost was seen on the DomainNet\u2019s Real dataset. \u201c<em>This confirms the intuition that feature reuse plays an important role in transfer learning\u201d. </em>Now, when ImageNet pretrained weights were used to fine-tune the model on CheXpert dataset, it was observed that P-T converges faster than RI-T in all the cases. \u201c<em>This suggests additional benefits of pre-trained weights that are not directly coming from feature\u00a0reuse\u201d.</em></p>\n<p>To further verify this hypothesis, \u201c<em>a series of modified downstream task\u201d </em>were created which were very different from each other visually. To be precise, image was partitioned into equal sized blocks and those blocks were shuffled across the image randomly. By doing this, the image was changing visually but the low level statistics remained the same. (Block size 1 means\u200a\u2014\u200apixels of size 1 were taken and shuffled across the image). Now the target domain looked visually far distinct from the source\u00a0domain.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/482/1*c-_KWjUgNpyCtDSZVRQQXg.png\"><figcaption>Middle Image\u200a\u2014\u200aPixel of block size 8 shuffled. Right image\u200a\u2014\u200aPixel of block size 1 shuffled. (Source\u200a\u2014\u200aPaper\u00a0itself)</figcaption></figure><p>After performing this shuffling, it was observed that\u200a\u2014\u200a1) final performance for RI-T and P-T drops as blocks size becomes smaller in size indicating that task is becoming more difficult to learn. 2) There is a decrease in relative accuracy difference with decrease in block size on real and clipart dataset from DomainNet \u201c<em>showing consistency with the intuition that decreasing feature reuse leads to diminishing benefits\u201d. </em>3) On q1uick draw dataset from DomainNet, there not much decrease. The dataset was already very dissimilar to the ImageNet dataset, indicating that some other factors of the pre-trained weights are helping the model in the downstream task.</p>\n<p><strong>The authors conclude that\u200a\u2014\u200a\u201c</strong><em>feature reuse plays a very important role in transfer learning, especially when the downstream task shares similar visual features with the pre-training domain.\u201d </em>They also tell that there are low level statistics which don\u2019t have any visual information play a role in transfer learning as even after shuffling the image pixels completely and shuffling the input channels, they did not see significant decreasing trend of metrics for quickdraw dataset.</p>\n<h3>Opening up the\u00a0model</h3>\n<p>In the second experiment, the authors <strong>investigate the mistakes made by the model</strong>, that is, they see the \u201c<em>common mistakes\u201d </em>made by both the models if they classify the same data point incorrectly. Then they also see \u201c<em>uncommon mistakes\u201d</em> where in one model classifies a data point correctly and other one incorrectly. They first \u201c<em>compare the ratio of common and uncommon mistakes between two P-Ts, a P-T and a RI-T and two RI-Ts\u201d. T</em>hey see that good number of uncommon mistakes are between O-T and RI-T where as two P-Ts have very few uncommon mistakes. This trend is visible for CheXpert and DomainNet dataset.</p>\n<p><strong>Feature similarity</strong>\u00a0: The authors use Centered Kernel alignment technique to measure similarity between \u201c<em>two output features in a layer of network architecture given two instances of such network\u201d.</em> It was observed that \u201c<em> two instances of P-T are highly similar across different layers\u201d. </em>It was noted that the feature similarity is stronger towards the last layers of the network in case of P-T and RI-T. <strong>Important point to be noted here\u200a\u2014\u200a\u201c</strong><em>These experiments show that the initialization point, whether pre-trained or random, drastically impacts feature similarity, and although both networks are showing high accuracy, they are not that similar in the feature space. This emphasizes on role of feature reuse and that two P-T are reusing the same features.\u201d</em></p>\n<h3>Module Criticality</h3>\n<p>In this section, the authors discuss how to calculate which module of the architecture is more critical from the rest of the\u00a0modules.</p>\n<p>The most important outcome was that\u200a\u2014\u200ait was seen that lower layers of the model are responsible for more general features while the higher layers have features which are specific to the target domain/dataset the model is being trained\u00a0on.</p>\n<h3>Conclusion</h3>\n<p>Do checkout the paper once to gain more insights about the experiments performed. This paper tells us that\u00a0:-</p>\n<ol>\n<li>We can use (or at least consider trying) transfer learning even if the target domain is visually very different from the source domain (dataset used in pre-trained weights)</li>\n<li>Features in the lower layers and some low level statistics which we cannot see visually are mostly responsible for this amazing performance of transfer learning.</li>\n<li>Since upper layers are more domain specific, we should freeze lower layers and train the upper layers for better performance of the model. But from which layers to start unfreezing would require extensive experiments and resources.</li>\n</ol>\n<p>Thanks for reading the blog. Hope you find it helpful!<br>Connect with me on <a href=\"https://www.linkedin.com/in/sahil-chachra/\">LinkedIn</a>\u00a0:).</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=250dc7a9d127\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["deep-learning","transfer-learning","data-science","artificial-intelligence"]},{"title":"Paper summary\u200a\u2014\u200aDecoupled Weight Decay Regularization","pubDate":"2021-10-29 14:30:19","link":"https://sahilchachra.medium.com/paper-summary-decoupled-weight-decay-regularization-1583cbc855bd?source=rss-f31bf6073414------2","guid":"https://medium.com/p/1583cbc855bd","author":"Sahil Chachra","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*dVPcugWY3wyxmSGYdUyDYg.png","description":"\n<h3>Paper summary\u200a\u2014\u200aDecoupled Weight Decay Regularization</h3>\n<p>If you search for - what is difference between L2 Regularization and Weight decay regularization, the most frequent answer would be that both are somewhat same. Yes both take you to same result in Stochastic Gradient Descent with Momentum but not when it comes to Adaptive gradient optimizers. The concept and working of weight decay in these adaptive optimizer (ex. Adam) is different.</p>\n<p>I highly recommend you to read the paper Decoupled Weight Decay Regularization by Ilya Loshchilov &amp; Frank Hutter\u200a\u2014\u200a<a href=\"https://arxiv.org/abs/1711.05101\">link</a>.</p>\n<p>Note\u200a\u2014\u200aAll the content in this summary/blog was written by referring to the above paper. At times I would quote from the paper because not every sentence can be reframed and still retain the meaning of it. <strong>Wherever I will quote from the paper, I will make it italic and put that in\u00a0quotes.</strong></p>\n<h3>Abstract</h3>\n<p>In the abstract, the authors convey that L2 regularization and Weight decay regularization are same for standard SGD but it\u2019s not the same case with Adaptive algorithms such as Adam. So in this paper they suggest a small modification to recover the idea of weight decay by <em>\u201cdecoupling the weight decay from the optimization steps taken w.r.t the loss function\u201d. </em>Moving forward in the paper, authors have provided experiments/evidence to prove that the modifications done are effective.</p>\n<h3>Introduction</h3>\n<p>L2 regularization and weight decay are not the same for adaptive algorithms but is equivalent in the case of SGD. When using L2 regularization with Adam it is seen that the older/historic gradients are being regularized less as compared to that while using weight\u00a0decay.</p>\n<p>\u201c<em>L2 regularization is not effective in Adam\u201d mostly because of deep learning libraries implementing only L2 regularization and \u201cnot the original weight\u00a0decay\u201d.</em></p>\n<p>\u201c<em>Weight decay is equally effective in both SGD and\u00a0Adam</em>\u201d</p>\n<p>Performance of weight decay depends on the batch size. Larger the batch size, smaller it the favorable weight\u00a0decay.</p>\n<p>It is advised to use learning rate multiplier/scheduler while using\u00a0Adam.</p>\n<p><strong>\u201c<em>Main contribution of this paper is to improve regularization in Adam by decoupling the weight decay from the gradient-based update.\u201d</em></strong></p>\n<h3>Decoupling the Weight Decay from gradient based weight\u00a0update</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*dVPcugWY3wyxmSGYdUyDYg.png\"><figcaption>Source of image\u200a\u2014\u200aFrom original paper itself\u200a\u2014\u200a<a href=\"https://arxiv.org/abs/1711.05101\">Link</a></figcaption></figure><p>In the paper, the author propose to decay the weights while updating weight in current iteration for SGD (line 9). In this way, \u03bb<strong> </strong>and \u03b1 can be decoupled (independent of each other)(As before line 6 had \u03bb and line 8 had \u03b1\u200a\u2014\u200amaking each other dependent).</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Q9W_oSHOK3c8K-ZliSWYhQ.png\"><figcaption>Source of image\u200a\u2014\u200aFrom original paper itself\u200a\u2014\u200a<a href=\"https://arxiv.org/abs/1711.05101\">Link</a></figcaption></figure><p>When Adam is run with L2 regularization, it was seen that \u201c<em>weights that tend to have large gradients in f do not get regularized as much as they would with decoupled weight decay since the gradient of the regularizer gets scaled along with the gradient of f\u00a0</em>\u201d</p>\n<p><strong>How does L2 regularization and Weight decay differ in terms of Adaptive gradient algorithms?\u200a\u2014\u200a</strong>\u201c<em>In L2 regularization, the sum of the gradient of the loss function and the gradient of the regularizer are adapted, whereas with decoupled weight decay, only the gradient of the loss function are adapted. With L2, both types of gradients are normalized by their typical magnitudes and there weights x with large typical gradient magnitude s are regularized by a smaller relative amount than other\u00a0weights.</em></p>\n<p><em>In contrast, de-coupled weight decay regularizes all weights with the same rate \u03bb, effectively regularizing weights x with large s more than standard L2 regularization does.</em>\u201d</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/987/1*tX-L5ZjBlGlJyadWtXt9rA.png\"><figcaption>Figure 1\u200a\u2014\u200aSource of image\u200a\u2014\u200aFrom original paper itself\u200a\u2014\u200a<a href=\"https://arxiv.org/abs/1711.05101\">Link</a></figcaption></figure><p>From the above image we can see AdamW (Adam with Weight decay) with cosine annealing learning rate scheduler gives the best performance and hence the authors suggest to use learning rate schedulers with adaptive gradient algorithms as\u00a0well.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/923/1*2lAAgTKmMxZRn36M0PAK-w.png\"><figcaption>Figure 2\u200a\u2014\u200aSource of image\u200a\u2014\u200aFrom original paper itself\u200a\u2014\u200a<a href=\"https://arxiv.org/abs/1711.05101\">Link</a></figcaption></figure><p>From Figure 2, Top left graph we can see that in SGD, L2 regularization is not decoupled from learning rate as the best hyperparameter basin is diagonal meaning initial learning rate and L2 regularization are interdependent on each other. This means, if we change only one of them we might get worse results. So to get best results we will have to change both simultaneously\u200a\u2014\u200ainitial learning rate and L2 regularization, giving large hyperparameter space.</p>\n<p>On the other hand (referring to Figure 2\u200a\u2014\u200agraph on top right), SGD with Weight decay or SGDW shows that initial learning rate and L2 regularization are decoupled. This shows that even if learning rate is not well tuned, having it fixed at some value and only changing weight decay factor would yield good value. This is also shown by the graph not diagonal.</p>\n<p>Coming to Adam with L2 regularization (Figure 2\u200a\u2014\u200abottom left graph), we see that it performs even worse than\u00a0SGD.</p>\n<p>Adam with weight decay or AdamW(Figure 2\u200a\u2014\u200abottom right graph) shows that it largely decouples learning rate and weight decay\u200a\u2014\u200akeep one parameter constant and try to optimize the other. Performance of this was better than SGD, Adam and\u00a0SGDW!</p>\n<p>Do refer the paper for more insights on experiments, performance and mathematical proofs!</p>\n<p>Thanks for reading the summary.<br>Connect with me on <a href=\"https://www.linkedin.com/in/sahil-chachra/\">LinkedIn</a>.\u00a0:)</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=1583cbc855bd\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>Paper summary\u200a\u2014\u200aDecoupled Weight Decay Regularization</h3>\n<p>If you search for - what is difference between L2 Regularization and Weight decay regularization, the most frequent answer would be that both are somewhat same. Yes both take you to same result in Stochastic Gradient Descent with Momentum but not when it comes to Adaptive gradient optimizers. The concept and working of weight decay in these adaptive optimizer (ex. Adam) is different.</p>\n<p>I highly recommend you to read the paper Decoupled Weight Decay Regularization by Ilya Loshchilov &amp; Frank Hutter\u200a\u2014\u200a<a href=\"https://arxiv.org/abs/1711.05101\">link</a>.</p>\n<p>Note\u200a\u2014\u200aAll the content in this summary/blog was written by referring to the above paper. At times I would quote from the paper because not every sentence can be reframed and still retain the meaning of it. <strong>Wherever I will quote from the paper, I will make it italic and put that in\u00a0quotes.</strong></p>\n<h3>Abstract</h3>\n<p>In the abstract, the authors convey that L2 regularization and Weight decay regularization are same for standard SGD but it\u2019s not the same case with Adaptive algorithms such as Adam. So in this paper they suggest a small modification to recover the idea of weight decay by <em>\u201cdecoupling the weight decay from the optimization steps taken w.r.t the loss function\u201d. </em>Moving forward in the paper, authors have provided experiments/evidence to prove that the modifications done are effective.</p>\n<h3>Introduction</h3>\n<p>L2 regularization and weight decay are not the same for adaptive algorithms but is equivalent in the case of SGD. When using L2 regularization with Adam it is seen that the older/historic gradients are being regularized less as compared to that while using weight\u00a0decay.</p>\n<p>\u201c<em>L2 regularization is not effective in Adam\u201d mostly because of deep learning libraries implementing only L2 regularization and \u201cnot the original weight\u00a0decay\u201d.</em></p>\n<p>\u201c<em>Weight decay is equally effective in both SGD and\u00a0Adam</em>\u201d</p>\n<p>Performance of weight decay depends on the batch size. Larger the batch size, smaller it the favorable weight\u00a0decay.</p>\n<p>It is advised to use learning rate multiplier/scheduler while using\u00a0Adam.</p>\n<p><strong>\u201c<em>Main contribution of this paper is to improve regularization in Adam by decoupling the weight decay from the gradient-based update.\u201d</em></strong></p>\n<h3>Decoupling the Weight Decay from gradient based weight\u00a0update</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*dVPcugWY3wyxmSGYdUyDYg.png\"><figcaption>Source of image\u200a\u2014\u200aFrom original paper itself\u200a\u2014\u200a<a href=\"https://arxiv.org/abs/1711.05101\">Link</a></figcaption></figure><p>In the paper, the author propose to decay the weights while updating weight in current iteration for SGD (line 9). In this way, \u03bb<strong> </strong>and \u03b1 can be decoupled (independent of each other)(As before line 6 had \u03bb and line 8 had \u03b1\u200a\u2014\u200amaking each other dependent).</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Q9W_oSHOK3c8K-ZliSWYhQ.png\"><figcaption>Source of image\u200a\u2014\u200aFrom original paper itself\u200a\u2014\u200a<a href=\"https://arxiv.org/abs/1711.05101\">Link</a></figcaption></figure><p>When Adam is run with L2 regularization, it was seen that \u201c<em>weights that tend to have large gradients in f do not get regularized as much as they would with decoupled weight decay since the gradient of the regularizer gets scaled along with the gradient of f\u00a0</em>\u201d</p>\n<p><strong>How does L2 regularization and Weight decay differ in terms of Adaptive gradient algorithms?\u200a\u2014\u200a</strong>\u201c<em>In L2 regularization, the sum of the gradient of the loss function and the gradient of the regularizer are adapted, whereas with decoupled weight decay, only the gradient of the loss function are adapted. With L2, both types of gradients are normalized by their typical magnitudes and there weights x with large typical gradient magnitude s are regularized by a smaller relative amount than other\u00a0weights.</em></p>\n<p><em>In contrast, de-coupled weight decay regularizes all weights with the same rate \u03bb, effectively regularizing weights x with large s more than standard L2 regularization does.</em>\u201d</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/987/1*tX-L5ZjBlGlJyadWtXt9rA.png\"><figcaption>Figure 1\u200a\u2014\u200aSource of image\u200a\u2014\u200aFrom original paper itself\u200a\u2014\u200a<a href=\"https://arxiv.org/abs/1711.05101\">Link</a></figcaption></figure><p>From the above image we can see AdamW (Adam with Weight decay) with cosine annealing learning rate scheduler gives the best performance and hence the authors suggest to use learning rate schedulers with adaptive gradient algorithms as\u00a0well.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/923/1*2lAAgTKmMxZRn36M0PAK-w.png\"><figcaption>Figure 2\u200a\u2014\u200aSource of image\u200a\u2014\u200aFrom original paper itself\u200a\u2014\u200a<a href=\"https://arxiv.org/abs/1711.05101\">Link</a></figcaption></figure><p>From Figure 2, Top left graph we can see that in SGD, L2 regularization is not decoupled from learning rate as the best hyperparameter basin is diagonal meaning initial learning rate and L2 regularization are interdependent on each other. This means, if we change only one of them we might get worse results. So to get best results we will have to change both simultaneously\u200a\u2014\u200ainitial learning rate and L2 regularization, giving large hyperparameter space.</p>\n<p>On the other hand (referring to Figure 2\u200a\u2014\u200agraph on top right), SGD with Weight decay or SGDW shows that initial learning rate and L2 regularization are decoupled. This shows that even if learning rate is not well tuned, having it fixed at some value and only changing weight decay factor would yield good value. This is also shown by the graph not diagonal.</p>\n<p>Coming to Adam with L2 regularization (Figure 2\u200a\u2014\u200abottom left graph), we see that it performs even worse than\u00a0SGD.</p>\n<p>Adam with weight decay or AdamW(Figure 2\u200a\u2014\u200abottom right graph) shows that it largely decouples learning rate and weight decay\u200a\u2014\u200akeep one parameter constant and try to optimize the other. Performance of this was better than SGD, Adam and\u00a0SGDW!</p>\n<p>Do refer the paper for more insights on experiments, performance and mathematical proofs!</p>\n<p>Thanks for reading the summary.<br>Connect with me on <a href=\"https://www.linkedin.com/in/sahil-chachra/\">LinkedIn</a>.\u00a0:)</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=1583cbc855bd\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["data-science","hyperparameter-tuning","optimization","deep-learning","ai"]},{"title":"Run YoloV5s with TensorRT and DeepStream on Nvidia Jetson Nano","pubDate":"2021-09-30 12:38:56","link":"https://sahilchachra.medium.com/run-yolov5s-with-tensorrt-and-deepstream-on-nvidia-jetson-nano-8c888a2f0eae?source=rss-f31bf6073414------2","guid":"https://medium.com/p/8c888a2f0eae","author":"Sahil Chachra","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*963XnKmJ7b62z13b0Q3FKA.jpeg","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*963XnKmJ7b62z13b0Q3FKA.jpeg\"></figure><p>This article will help you to run your YoloV5s model with TensorRT and DeepStream.</p>\n<p><em>Note\u00a0: This article has GitHub repo links which will help you to run your model on TensorRT and DeepStream. I will just guide you/walk you through the steps following which you will be able to run COCO pretrained model. For using custom trained model, the GitHub repos have the steps to\u00a0follow.</em></p>\n<h3>Getting Started\u2026</h3>\n<p>In my previous article (<a href=\"https://sahilchachra.medium.com/setting-up-nvidias-jetson-nano-from-jetpack-to-yolov5-60a004bf48bc\">link</a>), I focused on how to setup your Jetson nano and run inference on Yolov5s model. For this article, I used docker image from Hello AI course by Nvidia (<a href=\"https://www.youtube.com/watch?v=QXIwdsyK7Rw\">YouTube link</a>) and ran inference on YoloV5s with TensorRT optimization. Further on, I installed DeepStream on Nano and ran inference on YoloV5s with\u00a0it.</p>\n<p>Assuming you are using <a href=\"https://github.com/ultralytics/yolov5\">official repo</a> to train/run your YoloV5s model and the folder is in the home directory,</p>\n<ol><li>Run this command to know your JetPack/L4t version</li></ol>\n<pre>$sudo apt-cache show nvidia-jetpack</pre>\n<p>2. Clone <a href=\"https://github.com/dusty-nv/jetson-inference\">this </a>repo and pull the docker image from <a href=\"https://github.com/dusty-nv/jetson-inference/blob/master/docs/aux-docker.md\">here</a> as per your Jetpack version. This is official repo for Hello AI course by Nvidia. The docker has everything pre-installed\u200a\u2014\u200aPyTorch, TensorRT, etc. (Follow the initial steps in the repo on how to clone the repo and pull the docker container)</p>\n<pre>$git clone --recursive <a href=\"https://github.com/dusty-nv/jetson-inference\">https://github.com/dusty-nv/jetson-inference</a></pre>\n<p>3. Now move into jetson-inference folder (created by cloning the repo) and run the docker which you just downloaded</p>\n<pre>$cd jetson-inference<br>$docker/run.sh</pre>\n<blockquote>If you get an error like\u00a0: Error response from daemon: unauthorized: authentication required. See \u2018docker run\u200a\u2014\u200ahelp\u2019.</blockquote>\n<blockquote>Then run\u00a0:</blockquote>\n<pre>$docker/run.sh -c dustynv/jetson-inference:r32.x.x</pre>\n<blockquote>r32.x.x -&gt; put the version number of the docker container pulled</blockquote>\n<p>4. Check if you are able to use the docker container by typing python. After confirming, exit and come back to the home directory.</p>\n<p>5. After you have trained your model (or if you want to run inference on the COCO pretrained model), convert the model from\u00a0.pt to\u00a0.wts format and build TensorRT engine. Follow this repo\u200a\u2014\u200a<a href=\"https://github.com/wang-xinyu/tensorrtx/tree/master/yolov5\">https://github.com/wang-xinyu/tensorrtx/tree/master/yolov5</a></p>\n<p>6. Once you have followed the steps in the above repo (assuming you will have a folder named tensorrtx from above repo), to run the tensorrt engine, mount the folder while starting the\u00a0docker</p>\n<pre>$docker/run.sh -c dustynv/jetson-inference:r32.5.0 --volume ~/tensorrtx/:/tensorrtx/</pre>\n<p>Now, your docker container can access the tensorrtx folder stored in home directory.</p>\n<p>7. Now run this command to test your tensorrt\u00a0engine</p>\n<pre>cd /tensorrtx/yolo<br>python yolov5_trt.py</pre>\n<p>8. Now, install DeepStream SDK in your Nano from <a href=\"https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_Quickstart.html#jetson-setup\">here</a>(Nvidia\u2019s site). Exit from your docker. The docker container we used doesn\u2019t have DeepStream installed. To download DeepStream SDK use this <a href=\"https://developer.nvidia.com/deepstream-getting-started\">link</a>(Nvidia\u2019s site)</p>\n<p>9. After setting up DeepStream, to run your YoloV5s TensorRT engine with DeepStream, follow this\u00a0<a href=\"https://github.com/marcoslucianops/DeepStream-Yolo/\">repo</a>.</p>\n<p>10. Assuming you are in home directory after setting up DeepStream, to run your YoloV5s tensorrt engine with DeepStream\u00a0:-</p>\n<pre>$cd /opt/nvidia/deepstream/deepstream-5.1/sources/yolo<br>$deepstream-app -c deepstream_app_config.txt</pre>\n<p><em>Thanks for reading my blog. Hope this would have helped you to run YoloV5s TensortRT engine with DeepStream. If you find any issues or any better resource, do mention it in the comments.</em></p>\n<p><em>Thanks\u00a0:).</em></p>\n<p><em>Do connect with me on </em><a href=\"https://www.linkedin.com/in/sahil-chachra/\"><em>LinkedIn\u00a0</em></a><em>:)</em></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8c888a2f0eae\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*963XnKmJ7b62z13b0Q3FKA.jpeg\"></figure><p>This article will help you to run your YoloV5s model with TensorRT and DeepStream.</p>\n<p><em>Note\u00a0: This article has GitHub repo links which will help you to run your model on TensorRT and DeepStream. I will just guide you/walk you through the steps following which you will be able to run COCO pretrained model. For using custom trained model, the GitHub repos have the steps to\u00a0follow.</em></p>\n<h3>Getting Started\u2026</h3>\n<p>In my previous article (<a href=\"https://sahilchachra.medium.com/setting-up-nvidias-jetson-nano-from-jetpack-to-yolov5-60a004bf48bc\">link</a>), I focused on how to setup your Jetson nano and run inference on Yolov5s model. For this article, I used docker image from Hello AI course by Nvidia (<a href=\"https://www.youtube.com/watch?v=QXIwdsyK7Rw\">YouTube link</a>) and ran inference on YoloV5s with TensorRT optimization. Further on, I installed DeepStream on Nano and ran inference on YoloV5s with\u00a0it.</p>\n<p>Assuming you are using <a href=\"https://github.com/ultralytics/yolov5\">official repo</a> to train/run your YoloV5s model and the folder is in the home directory,</p>\n<ol><li>Run this command to know your JetPack/L4t version</li></ol>\n<pre>$sudo apt-cache show nvidia-jetpack</pre>\n<p>2. Clone <a href=\"https://github.com/dusty-nv/jetson-inference\">this </a>repo and pull the docker image from <a href=\"https://github.com/dusty-nv/jetson-inference/blob/master/docs/aux-docker.md\">here</a> as per your Jetpack version. This is official repo for Hello AI course by Nvidia. The docker has everything pre-installed\u200a\u2014\u200aPyTorch, TensorRT, etc. (Follow the initial steps in the repo on how to clone the repo and pull the docker container)</p>\n<pre>$git clone --recursive <a href=\"https://github.com/dusty-nv/jetson-inference\">https://github.com/dusty-nv/jetson-inference</a></pre>\n<p>3. Now move into jetson-inference folder (created by cloning the repo) and run the docker which you just downloaded</p>\n<pre>$cd jetson-inference<br>$docker/run.sh</pre>\n<blockquote>If you get an error like\u00a0: Error response from daemon: unauthorized: authentication required. See \u2018docker run\u200a\u2014\u200ahelp\u2019.</blockquote>\n<blockquote>Then run\u00a0:</blockquote>\n<pre>$docker/run.sh -c dustynv/jetson-inference:r32.x.x</pre>\n<blockquote>r32.x.x -&gt; put the version number of the docker container pulled</blockquote>\n<p>4. Check if you are able to use the docker container by typing python. After confirming, exit and come back to the home directory.</p>\n<p>5. After you have trained your model (or if you want to run inference on the COCO pretrained model), convert the model from\u00a0.pt to\u00a0.wts format and build TensorRT engine. Follow this repo\u200a\u2014\u200a<a href=\"https://github.com/wang-xinyu/tensorrtx/tree/master/yolov5\">https://github.com/wang-xinyu/tensorrtx/tree/master/yolov5</a></p>\n<p>6. Once you have followed the steps in the above repo (assuming you will have a folder named tensorrtx from above repo), to run the tensorrt engine, mount the folder while starting the\u00a0docker</p>\n<pre>$docker/run.sh -c dustynv/jetson-inference:r32.5.0 --volume ~/tensorrtx/:/tensorrtx/</pre>\n<p>Now, your docker container can access the tensorrtx folder stored in home directory.</p>\n<p>7. Now run this command to test your tensorrt\u00a0engine</p>\n<pre>cd /tensorrtx/yolo<br>python yolov5_trt.py</pre>\n<p>8. Now, install DeepStream SDK in your Nano from <a href=\"https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_Quickstart.html#jetson-setup\">here</a>(Nvidia\u2019s site). Exit from your docker. The docker container we used doesn\u2019t have DeepStream installed. To download DeepStream SDK use this <a href=\"https://developer.nvidia.com/deepstream-getting-started\">link</a>(Nvidia\u2019s site)</p>\n<p>9. After setting up DeepStream, to run your YoloV5s TensorRT engine with DeepStream, follow this\u00a0<a href=\"https://github.com/marcoslucianops/DeepStream-Yolo/\">repo</a>.</p>\n<p>10. Assuming you are in home directory after setting up DeepStream, to run your YoloV5s tensorrt engine with DeepStream\u00a0:-</p>\n<pre>$cd /opt/nvidia/deepstream/deepstream-5.1/sources/yolo<br>$deepstream-app -c deepstream_app_config.txt</pre>\n<p><em>Thanks for reading my blog. Hope this would have helped you to run YoloV5s TensortRT engine with DeepStream. If you find any issues or any better resource, do mention it in the comments.</em></p>\n<p><em>Thanks\u00a0:).</em></p>\n<p><em>Do connect with me on </em><a href=\"https://www.linkedin.com/in/sahil-chachra/\"><em>LinkedIn\u00a0</em></a><em>:)</em></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8c888a2f0eae\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["deepstream","jetson-nano","tensorrt","ai","deep-learning"]},{"title":"Setting up Nvidia\u2019s Jetson Nano\u00a0: From Jetpack to YoloV5","pubDate":"2021-08-21 13:29:58","link":"https://sahilchachra.medium.com/setting-up-nvidias-jetson-nano-from-jetpack-to-yolov5-60a004bf48bc?source=rss-f31bf6073414------2","guid":"https://medium.com/p/60a004bf48bc","author":"Sahil Chachra","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*Rn1XfeaR13cbkpa-snT7lw.jpeg","description":"\n<h3>Setting up Nvidia\u2019s Jetson Nano\u00a0: From Jetpack to\u00a0YoloV5</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Rn1XfeaR13cbkpa-snT7lw.jpeg\"></figure><p>In this blog, I will take you through several resources to step up your Jetson such as to install remote desktop, Pytorch, Tensorflow, etc. Also, I will talk about how to install other required libraries to run your deep learning tasks some\u00a0tips.</p>\n<p><em>Note\u00a0: All the steps/procedures discussed/shared were used/tested by me on Nvidia Jetson Nano(B01, 4GB variant). The following packages were installed/set up on Jetpack 4.5 (R32.5.0). While writing this blog, Jetpack 4.6 has been released.</em></p>\n<h3>A brief on my Jetson\u2019s\u00a0setup</h3>\n<p>I am using Jetson Nano(B01) variant which has 4GB ram. I use 4A power adapter to power it. I have also installed a small fan on the heatsink to maintain the temperature(don\u2019t know yet how effective it is). In terms of storage, I am using 64GB SD card from SanDisk. To work on jetson, I actually plug in a monitor, mouse and keyboard and use it like a separate machine for now. I tried using it via SSH, but then while learning you cannot entirely work through terminal/cmd. To access internet, I am using a ethernet cable which connects the jetson directly to my router. To capture live feed, I am using Logitech C270(HD)\u00a0webcam.</p>\n<p>To access internet, why am I not using Intel\u2019s wifi/bluetooth adapter or USB wifi receiver? I am not using Intel\u2019s adapter because right now I don\u2019t see a need to use jetson wirelessly. I am using it with my monitor and other components plugged in. And the reason to not use USB wifi receiver is because each jetpack is based on some linux kernel version and those receivers work with few specific linux kernel\u2019s versions only. So considering it as not so future proof, I did not consider using\u00a0it.</p>\n<h3>Initial setup</h3>\n<ol>\n<li>Head over to Nvidia\u2019s official site to download Jetpack and follow the instructions to install it. You will need to download SD card formatter tool and jetpack image flashing tool. Everything will be mentioned in the steps. (Link for Jetpack 4.5 and steps\u200a\u2014\u200a<a href=\"https://developer.nvidia.com/jetpack-sdk-45-archive\">https://developer.nvidia.com/jetpack-sdk-45-archive</a>)</li>\n<li>After flashing jetpack to SD card, plug it in and also attach monitor, keyboard and\u00a0mouse.</li>\n<li>After booting it, increase the swap size to 4GB. ( If you don\u2019t see the steps in the site then refer this video\u200a\u2014\u200a<a href=\"https://www.youtube.com/watch?v=uvU8AXY1170\">https://www.youtube.com/watch?v=uvU8AXY1170</a>)</li>\n<li>Now connect your jetson to internet via ethernet cable or Intel\u2019s wifi/bluetooth module for\u00a0jetson.</li>\n<li>Now run\u00a0\u2014</li>\n</ol>\n<pre>sudo apt-get update<br>sudo apt-get upgrade</pre>\n<p>6. After this install nano text editor\u00a0\u2014</p>\n<pre>sudo apt install nano</pre>\n<p>7. If you want to access jetson remotely with UI, setup VNC server. Link\u200a\u2014\u200a<a href=\"https://medium.com/@bharathsudharsan023/jetson-nano-remote-vnc-access-d1e71c82492b\">https://medium.com/@bharathsudharsan023/jetson-nano-remote-vnc-access-d1e71c82492b</a>\u00a0. If you are on Windows, then follow the steps till installing and setting up VNC server. Then to access jetson via VNC, install VNC viewer. Open VNC viewer, go to File -&gt; New connection -&gt; in VNC server enter your jetson\u2019s IP address and in name field give any name you\u00a0want.</p>\n<p>8. If you want to access jetson remotely without UI, go to cmd/terminal and type -&gt; ssh your_jetson_username@jetson_IP_address. It will prompt for password. (You can also connect jetson to your system using mirco-usb if you don\u2019t have access to internet. You connect by going to terminal and typing -&gt; ssh your_jetson_username@192.168.0.55\u00a0. This should be the IP most probably)</p>\n<p>Congratulations, your Jetson Nano is now\u00a0ready!</p>\n<h3>Should I follow Jetson AI Fundamentals course\u00a0now?</h3>\n<p>I spent few days with the course. The course helped me to setup my jetson and get some hands on. Do spend some time with this course and get familiar to jetson\u2019s speed and UI. This course is very beginner friendly. They use NGC container to run a simple project which has required libraries pre-installed (but those libraries are accessible inside the container only). This course will teach you how to use jetson with headless mode(without monitor, mouse and keyboard).</p>\n<h3>Installing required libraries</h3>\n<p>Remember to use pip3 while installing these libraries(you have python 2 and python 3 both installed). Anaconda is not supported by Jetson as per today. If you want to create virtual environments you can use python\u2019s venv or Archiconda.</p>\n<ol><li>Install numpy version 1.19.4 only. Any version above this can cause problems while using Pytorch or Tensorflow (Faced issues myself\u200a\u2014\u200atensorflow threw core dumped error). To install numpy 1.19.4. Also install opencv\u00a0:-</li></ol>\n<pre>sudo apt-get install python3-pip<br>pip3 install cython<br>pip3 install numpy==1.19.4<br>pip3 install opencv-python</pre>\n<p>2. Download few dependencies before installing Pytorch:-</p>\n<pre>sudo apt-get install python3-pip libjpeg-dev libopenblas-dev libopenmpi-dev libomp-dev<br>sudo apt install libfreetype6-dev python3-dev zlib1g-dev</pre>\n<p>3. Head over to <a href=\"https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-9-0-now-available/72048\">https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-9-0-now-available/72048</a> or search \u201cdownload pytorch jetson\u201d and open Nvidia\u2019s website. Now, click on arrow and download Pytorch 1.8. It will download a\u00a0.whl file. Install it\u00a0using:-</p>\n<pre>pip3 install path_to_pytorch18.whl</pre>\n<p>4. Pytorch also requires torchvision library. To install it, (in the below command, vision and torchvision are separated by space\u200a\u2014\u200acopy the entire clone command till the word torchvision and paste it in the terminal)</p>\n<pre>git clone \u2014 branch v0.9.1 <a href=\"https://github.com/pytorch/vision\">https://github.com/pytorch/vision</a> torchvision<br>cd torchvision/<br>sudo python3 setup.py install</pre>\n<p>5. Now come out of the torchvision folder by using command <strong>cd\u00a0..</strong> and verify installation of Pytorch and torchvision by importing them. If you see this error \u201cCannot find Python.h\u201d while installing torchvision, kindly reinstall python3-dev package.</p>\n<p>6. Now install Tensorflow 2.4.1 by following this article\u200a\u2014\u200a<a href=\"https://qengineering.eu/install-tensorflow-2.4.0-on-jetson-nano.html\">https://qengineering.eu/install-tensorflow-2.4.0-on-jetson-nano.html</a>\u00a0. <em>If gdown command(used to download tf2.4.1.whl file) doesn\u2019t work, then simply copy the drive link from the steps and download the file yourself.</em></p>\n<p>7. Verify the installation of tensorflow by importing it and printing the\u00a0version.</p>\n<h3>Running Yolov5 on your\u00a0Jetson</h3>\n<ol><li>Clone the repo and move inside the cloned\u00a0folder</li></ol>\n<pre>git clone <a href=\"https://github.com/ultralytics/yolov5.git\">https://github.com/ultralytics/yolov5.git</a><br>cd yolov5<br>export OPENBLAS_CORETYPE=ARMV8</pre>\n<p>2. Now type nano requirements.txt in terminal and comment out these packages as these are already installed -&gt; numpy, tensorflow, torch, torchvision and\u00a0opencv</p>\n<p>3. Now install the remaining libraries using the following command. This process may take time as many of these libraries are downloaded and built locally on the\u00a0jetson.</p>\n<pre>pip3 install -r requirements.txt</pre>\n<p>4. After installing, download Yolov5s model from the repository, place it in the yolov5 directory and run the following. If the following commands throws error, kindly look for missing dependencies.</p>\n<pre>python3 detect.py --weights yolov5s.pt --source 0 --device 0</pre>\n<p>Source 0 means you are using Webcam. (change the source to path of image or video if you don\u2019t want to use live feed). Device 0 means that the model should be loaded in\u00a0GPU.</p>\n<p>5. Do checkout detect.py for more options while running inference on YoloV5s model (Yolov5s is a light weight\u00a0model)</p>\n<h3>Few Tips</h3>\n<ol>\n<li>Keep System monitor application on and always keep an eye on it. Jetson tends to lag when you have several tabs open on Chromium and parallelly you are running some inference.</li>\n<li>Restart the jetson if you see high ram usage even with no application open.</li>\n<li>If you are new to jetson, I highly recommend to use it with display, mouse and keyboard rather than via\u00a0SSH.</li>\n<li>You may use VNC but at times it is very slow. You may use SSH if you know you won\u2019t require UI for any\u00a0task.</li>\n<li>Try to use jetson in a cool place. High temperature may damage it and also the SD card as it just sits below the heatsink.</li>\n<li>Try to use NGC containers as they provide many options, such as\u200a\u2014\u200aproviding entire DL libraries installed in a container. Just pull and run! (I couldn\u2019t use it as the files/libraries I downloaded after pulling it were all gone when I restarted the\u00a0jetson).</li>\n<li>Every time you run yolov5\u2019s detect.py, the output is saved. Remember to delete the output if not needed else it will eat up\u00a0space.</li>\n</ol>\n<h3>Conclusion</h3>\n<p>Hope these tips and steps might have helped you in setting up your jetson nano. The main goal of this blog was to guide you through the right steps to install these libraries instead of you keep on searching for these. Also, if you come across any community where people discuss/work on jetson, do let me know in the comments section of this blog. I would love to join to learn and contribute! Thanks for reading my blog.\u00a0:-)</p>\n<p>Connect with me on <a href=\"https://www.linkedin.com/in/sahil-chachra/\">LinkedIn</a>.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=60a004bf48bc\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>Setting up Nvidia\u2019s Jetson Nano\u00a0: From Jetpack to\u00a0YoloV5</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Rn1XfeaR13cbkpa-snT7lw.jpeg\"></figure><p>In this blog, I will take you through several resources to step up your Jetson such as to install remote desktop, Pytorch, Tensorflow, etc. Also, I will talk about how to install other required libraries to run your deep learning tasks some\u00a0tips.</p>\n<p><em>Note\u00a0: All the steps/procedures discussed/shared were used/tested by me on Nvidia Jetson Nano(B01, 4GB variant). The following packages were installed/set up on Jetpack 4.5 (R32.5.0). While writing this blog, Jetpack 4.6 has been released.</em></p>\n<h3>A brief on my Jetson\u2019s\u00a0setup</h3>\n<p>I am using Jetson Nano(B01) variant which has 4GB ram. I use 4A power adapter to power it. I have also installed a small fan on the heatsink to maintain the temperature(don\u2019t know yet how effective it is). In terms of storage, I am using 64GB SD card from SanDisk. To work on jetson, I actually plug in a monitor, mouse and keyboard and use it like a separate machine for now. I tried using it via SSH, but then while learning you cannot entirely work through terminal/cmd. To access internet, I am using a ethernet cable which connects the jetson directly to my router. To capture live feed, I am using Logitech C270(HD)\u00a0webcam.</p>\n<p>To access internet, why am I not using Intel\u2019s wifi/bluetooth adapter or USB wifi receiver? I am not using Intel\u2019s adapter because right now I don\u2019t see a need to use jetson wirelessly. I am using it with my monitor and other components plugged in. And the reason to not use USB wifi receiver is because each jetpack is based on some linux kernel version and those receivers work with few specific linux kernel\u2019s versions only. So considering it as not so future proof, I did not consider using\u00a0it.</p>\n<h3>Initial setup</h3>\n<ol>\n<li>Head over to Nvidia\u2019s official site to download Jetpack and follow the instructions to install it. You will need to download SD card formatter tool and jetpack image flashing tool. Everything will be mentioned in the steps. (Link for Jetpack 4.5 and steps\u200a\u2014\u200a<a href=\"https://developer.nvidia.com/jetpack-sdk-45-archive\">https://developer.nvidia.com/jetpack-sdk-45-archive</a>)</li>\n<li>After flashing jetpack to SD card, plug it in and also attach monitor, keyboard and\u00a0mouse.</li>\n<li>After booting it, increase the swap size to 4GB. ( If you don\u2019t see the steps in the site then refer this video\u200a\u2014\u200a<a href=\"https://www.youtube.com/watch?v=uvU8AXY1170\">https://www.youtube.com/watch?v=uvU8AXY1170</a>)</li>\n<li>Now connect your jetson to internet via ethernet cable or Intel\u2019s wifi/bluetooth module for\u00a0jetson.</li>\n<li>Now run\u00a0\u2014</li>\n</ol>\n<pre>sudo apt-get update<br>sudo apt-get upgrade</pre>\n<p>6. After this install nano text editor\u00a0\u2014</p>\n<pre>sudo apt install nano</pre>\n<p>7. If you want to access jetson remotely with UI, setup VNC server. Link\u200a\u2014\u200a<a href=\"https://medium.com/@bharathsudharsan023/jetson-nano-remote-vnc-access-d1e71c82492b\">https://medium.com/@bharathsudharsan023/jetson-nano-remote-vnc-access-d1e71c82492b</a>\u00a0. If you are on Windows, then follow the steps till installing and setting up VNC server. Then to access jetson via VNC, install VNC viewer. Open VNC viewer, go to File -&gt; New connection -&gt; in VNC server enter your jetson\u2019s IP address and in name field give any name you\u00a0want.</p>\n<p>8. If you want to access jetson remotely without UI, go to cmd/terminal and type -&gt; ssh your_jetson_username@jetson_IP_address. It will prompt for password. (You can also connect jetson to your system using mirco-usb if you don\u2019t have access to internet. You connect by going to terminal and typing -&gt; ssh your_jetson_username@192.168.0.55\u00a0. This should be the IP most probably)</p>\n<p>Congratulations, your Jetson Nano is now\u00a0ready!</p>\n<h3>Should I follow Jetson AI Fundamentals course\u00a0now?</h3>\n<p>I spent few days with the course. The course helped me to setup my jetson and get some hands on. Do spend some time with this course and get familiar to jetson\u2019s speed and UI. This course is very beginner friendly. They use NGC container to run a simple project which has required libraries pre-installed (but those libraries are accessible inside the container only). This course will teach you how to use jetson with headless mode(without monitor, mouse and keyboard).</p>\n<h3>Installing required libraries</h3>\n<p>Remember to use pip3 while installing these libraries(you have python 2 and python 3 both installed). Anaconda is not supported by Jetson as per today. If you want to create virtual environments you can use python\u2019s venv or Archiconda.</p>\n<ol><li>Install numpy version 1.19.4 only. Any version above this can cause problems while using Pytorch or Tensorflow (Faced issues myself\u200a\u2014\u200atensorflow threw core dumped error). To install numpy 1.19.4. Also install opencv\u00a0:-</li></ol>\n<pre>sudo apt-get install python3-pip<br>pip3 install cython<br>pip3 install numpy==1.19.4<br>pip3 install opencv-python</pre>\n<p>2. Download few dependencies before installing Pytorch:-</p>\n<pre>sudo apt-get install python3-pip libjpeg-dev libopenblas-dev libopenmpi-dev libomp-dev<br>sudo apt install libfreetype6-dev python3-dev zlib1g-dev</pre>\n<p>3. Head over to <a href=\"https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-9-0-now-available/72048\">https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-9-0-now-available/72048</a> or search \u201cdownload pytorch jetson\u201d and open Nvidia\u2019s website. Now, click on arrow and download Pytorch 1.8. It will download a\u00a0.whl file. Install it\u00a0using:-</p>\n<pre>pip3 install path_to_pytorch18.whl</pre>\n<p>4. Pytorch also requires torchvision library. To install it, (in the below command, vision and torchvision are separated by space\u200a\u2014\u200acopy the entire clone command till the word torchvision and paste it in the terminal)</p>\n<pre>git clone \u2014 branch v0.9.1 <a href=\"https://github.com/pytorch/vision\">https://github.com/pytorch/vision</a> torchvision<br>cd torchvision/<br>sudo python3 setup.py install</pre>\n<p>5. Now come out of the torchvision folder by using command <strong>cd\u00a0..</strong> and verify installation of Pytorch and torchvision by importing them. If you see this error \u201cCannot find Python.h\u201d while installing torchvision, kindly reinstall python3-dev package.</p>\n<p>6. Now install Tensorflow 2.4.1 by following this article\u200a\u2014\u200a<a href=\"https://qengineering.eu/install-tensorflow-2.4.0-on-jetson-nano.html\">https://qengineering.eu/install-tensorflow-2.4.0-on-jetson-nano.html</a>\u00a0. <em>If gdown command(used to download tf2.4.1.whl file) doesn\u2019t work, then simply copy the drive link from the steps and download the file yourself.</em></p>\n<p>7. Verify the installation of tensorflow by importing it and printing the\u00a0version.</p>\n<h3>Running Yolov5 on your\u00a0Jetson</h3>\n<ol><li>Clone the repo and move inside the cloned\u00a0folder</li></ol>\n<pre>git clone <a href=\"https://github.com/ultralytics/yolov5.git\">https://github.com/ultralytics/yolov5.git</a><br>cd yolov5<br>export OPENBLAS_CORETYPE=ARMV8</pre>\n<p>2. Now type nano requirements.txt in terminal and comment out these packages as these are already installed -&gt; numpy, tensorflow, torch, torchvision and\u00a0opencv</p>\n<p>3. Now install the remaining libraries using the following command. This process may take time as many of these libraries are downloaded and built locally on the\u00a0jetson.</p>\n<pre>pip3 install -r requirements.txt</pre>\n<p>4. After installing, download Yolov5s model from the repository, place it in the yolov5 directory and run the following. If the following commands throws error, kindly look for missing dependencies.</p>\n<pre>python3 detect.py --weights yolov5s.pt --source 0 --device 0</pre>\n<p>Source 0 means you are using Webcam. (change the source to path of image or video if you don\u2019t want to use live feed). Device 0 means that the model should be loaded in\u00a0GPU.</p>\n<p>5. Do checkout detect.py for more options while running inference on YoloV5s model (Yolov5s is a light weight\u00a0model)</p>\n<h3>Few Tips</h3>\n<ol>\n<li>Keep System monitor application on and always keep an eye on it. Jetson tends to lag when you have several tabs open on Chromium and parallelly you are running some inference.</li>\n<li>Restart the jetson if you see high ram usage even with no application open.</li>\n<li>If you are new to jetson, I highly recommend to use it with display, mouse and keyboard rather than via\u00a0SSH.</li>\n<li>You may use VNC but at times it is very slow. You may use SSH if you know you won\u2019t require UI for any\u00a0task.</li>\n<li>Try to use jetson in a cool place. High temperature may damage it and also the SD card as it just sits below the heatsink.</li>\n<li>Try to use NGC containers as they provide many options, such as\u200a\u2014\u200aproviding entire DL libraries installed in a container. Just pull and run! (I couldn\u2019t use it as the files/libraries I downloaded after pulling it were all gone when I restarted the\u00a0jetson).</li>\n<li>Every time you run yolov5\u2019s detect.py, the output is saved. Remember to delete the output if not needed else it will eat up\u00a0space.</li>\n</ol>\n<h3>Conclusion</h3>\n<p>Hope these tips and steps might have helped you in setting up your jetson nano. The main goal of this blog was to guide you through the right steps to install these libraries instead of you keep on searching for these. Also, if you come across any community where people discuss/work on jetson, do let me know in the comments section of this blog. I would love to join to learn and contribute! Thanks for reading my blog.\u00a0:-)</p>\n<p>Connect with me on <a href=\"https://www.linkedin.com/in/sahil-chachra/\">LinkedIn</a>.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=60a004bf48bc\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["deep-learning","yolov5","inference","jetson-nano","jetsons"]}]}